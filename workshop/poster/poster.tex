%!TeX program = lualatex

\documentclass[final,12pt]{beamer}

% ====================
% Packages
% ====================

% Packages
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=a1,orientation=portrait,scale=1.4]{beamerposter}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{mathtools}
\usetheme{aalto}
\usecolortheme{aalto}
\usepackage{graphicx}
%\usepackage[square]{natbib}
%\setlength{\bibsep}{4pt plus 0.3ex}  % Squeeze line spacing in references
\usepackage{booktabs}
% Tikz
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations,backgrounds,arrows.meta,calc}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{pgfplots}
\usepackage{subcaption}
\usetikzlibrary{}
\usepackage{chronosys}
\newcommand{\blue}[1]{\textcolor{aaltoblue}{#1}}
\newcommand{\black}[1]{\textcolor{black}{#1}}
%\usepackage{color}
\usepackage{xcolor}         % colors


% Array/table packages
\usepackage{tabularx}
\usepackage{array,multirow}
\usepackage{colortbl}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newlength{\tblw}

% Our method
\newcommand{\our}{SFR}
	
% Custom error formatting
%\newcommand{\val}[2]{ $#1$\textcolor{gray}{\tiny ${\pm}#2$}} 



% Packages for bold math
\usepackage{bm}
\newcommand{\mathbold}[1]{\bm{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\renewcommand{\mid}{\,|\,}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}
\newcommand{\numDataNew}{\ensuremath{N^{\text{new}}}}
\newcommand{\numDataOld}{\ensuremath{N^{\text{old}}}}
\newcommand{\numInducing}{\ensuremath{M}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\inputDomain}{\ensuremath{\mathbb{R}^{D}}}
\newcommand{\outputDomain}{\ensuremath{\mathbb{R}^{C}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}


% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
% \newcommand{\weights}{\ensuremath{\bm\phi}}
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transitionFnWithParams}{\ensuremath{\transitionFn_{\weights}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}

% Math Macros
\newcommand{\MB}{\mbf{B}}
\newcommand{\MC}{\mbf{C}}
\newcommand{\MZ}{\mbf{Z}}
\newcommand{\MV}{\mbf{V}}
\newcommand{\MX}{\mbf{X}}
\newcommand{\MA}{\mbf{A}}
\newcommand{\MK}{\mbf{K}}
\newcommand{\MI}{\mbf{I}}
\newcommand{\MH}{\mbf{H}}
\newcommand{\T}{\top}
\newcommand{\vzeros}{\mbf{0}}
\newcommand{\vtheta}[0]{\mathbold{\theta}}
\newcommand{\valpha}[0]{\mathbold{\alpha}}
\newcommand{\vkappa}[0]{\mathbold{\kappa}}
\newcommand{\vbeta}[0]{\mathbold{\beta}}
\newcommand{\MBeta}[0]{\mathbold{B}}
\newcommand{\vlambda}[0]{\mathbold{\lambda}}
\newcommand{\diag}{\text{{diag}}}

\newcommand{\vm}{\mbf{m}}
\newcommand{\vz}{\mbf{z}}
\newcommand{\vf}{\mbf{f}}
\newcommand{\vu}{\mbf{u}}
\newcommand{\vx}{\mbf{x}}
\newcommand{\vy}{\mbf{y}}
\newcommand{\vw}{\mbf{w}}
\newcommand{\va}{\mbf{a}}

\newcommand{\Jac}[2]{\mathcal{J}_{#1}(#2)}
\newcommand{\JacT}[2]{\mathcal{J}_{#1}^\top(#2)}


\newcommand{\GP}{\mathcal{GP}}
\newcommand{\KL}[2]{\mathrm{D}_\textrm{KL} \dbar*{#1}{#2}}
\newcommand{\MKzz}{\mbf{K}_{\mbf{z}\mbf{z}}}
\newcommand{\MKzzc}{\mbf{K}_{\mbf{z}\mbf{z}, c}}
\newcommand{\MKxx}{\mbf{K}_{\mbf{x}\mbf{x}}}
\newcommand{\MKzx}{\mbf{K}_{\mbf{z}\mbf{x}}}
\newcommand{\MKxz}{\mbf{K}_{\mbf{x}\mbf{z}}}
\newcommand{\vkzi}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vkzic}{\mbf{k}_{\mbf{z}i,c}}
\newcommand{\vkzs}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vk}{\mbf{k}}
\newcommand{\MLambda}[0]{\mathbold{\Lambda}}
\newcommand{\MSigma}[0]{\mathbold{\Sigma}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\myexpect}{\mathbb{E}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\Norm}{\mathcal{N}}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% Math and miscellaneous
\renewcommand{\mid}[0]{\,|\,}


% Config for Arno's awesome TikZ plotting stuff
\newlength{\figurewidth}
\newlength{\figureheight}


% ====================
% Title
% ====================

\title{Sparse Function-space Representation \\ of Neural Networks}

\author{%
  Aidan Scannell\textsuperscript{\star}\,\inst{1}\,\inst{2} \quad
  Riccardo Mereu\textsuperscript{\star}\,\inst{1} \quad
  Paul Chang\,\inst{1} \\
  Ella Tamir\,\inst{1}\quad
  Joni Pajarinen\,\inst{1}\quad
  Arno Solin\,\inst{1}
}

%\author{Your Name\inst{1} ~~ Collaborator Name\inst{2} ~~ Arno Solin\inst{1}}

\institute[shortinst]{ \inst{1}Aalto University \qquad \inst{2}Finnish Center for Artificial Intelligence}


% ====================
% Footer (optional)
% ====================

\footercontent{
  International Conference in Machine Learning (ICML 2023) --- Hawaii, US \hfill
  \href{mailto:aidan.scannell@aalto.fi}{aidan.scannell@aalto.fi}}

% ====================
% Body
% ====================

\begin{document}
\begin{frame}[t]
\begin{columns}[t]
\begin{column}{2.05\colwidth}
\begin{alertblock}{Summary}
Deep neural networks have limitations in \textit{estimating uncertainty}, \textit{incorporating new data}, and \textit{avoiding catastrophic forgetting}. To overcome these issues, we introduce a method that converts neural networks from weight-space to a \alert{\bf low-rank function-space} representation using \alert{\bf dual parameters}. Unlike previous methods, our approach, named \alert{\bf Sparse Function Representation (SFR)}, captures the \alert{\bf full joint distribution} of the entire data set, not just a subset. This allows for a concise and reliable way of capturing uncertainty and facilitates the inclusion of new data without the need for retraining. We provide a proof-of-concept quantifying uncertainty for supervised learning tasks on UCI benchmark data sets.
%{\bf \alert{TODO: Rewrite}}
%  Deep neural networks are known to lack uncertainty estimates, struggle to incorporate new data, and suffer from catastrophic forgetting. We present a method that mitigates these issues by converting neural networks from weight-space to a low-rank function-space representation, via the so-called dual parameters. In contrast to previous work, our sparse representation captures the joint distribution over the entire data set, rather than only over a subset. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We provide proof-of-concept demonstrations with the proposed approach for quantifying uncertainty in supervised learning on UCI benchmark tasks.
  \end{alertblock}\end{column}
\end{columns}
\begin{columns}[t]


\separatorcolumn

\begin{column}{\colwidth}
  \begin{minipage}{\textwidth}
    \vspace*{-26pt}
  	%\begin{figure}[t]
    \pgfplotsset{axis on top,scale only axis,width=\figurewidth,height=\figureheight, ylabel near ticks,ylabel style={yshift=0pt},xlabel style={yshift=-4pt},y tick label style={rotate=90},legend style={nodes={scale=.8, transform shape}},tick label style={font=\footnotesize},xtick={0,.4,.8,1.2,1.6,2.}}
  \pgfplotsset{xlabel={Input, $x$},axis line style={rounded corners=2pt}}
  %
  % Hack pgfplots addplot options
  \pgfplotsset{every axis plot/.append style={mark size=5,line width=1.2pt}}
  %
  % Set figure 
  \setlength{\figurewidth}{.4\textwidth}
  \setlength{\figureheight}{.31\textwidth}
  %
  \def\inducing{\large Sparse inducing points}
  %
  \begin{minipage}[c]{.45\textwidth}
    \raggedleft
    \pgfplotsset{ylabel={Output, $y$}}
    \input{./fig/regression-nn.tex}%
  \end{minipage}
  \hfill  
  \begin{minipage}[c]{.03\textwidth}
    \centering
    \tikz[overlay,remember picture]\node(p0){};
  \end{minipage}  
  \hfill
  \begin{minipage}[c]{.45\textwidth}
    \raggedleft
    \pgfplotsset{yticklabels={,,},ytick={\empty}}
    \input{./fig/regression-nn2svgp.tex}%
  \end{minipage}
%  \hfill  
%  \begin{subfigure}[c]{.01\textwidth}
%    \centering
%    \tikz[overlay,remember picture]\node(p1){};
%  \end{subfigure}  
%  \hfill
%  \begin{subfigure}[c]{.28\textwidth}
%    \raggedleft
%    \pgfplotsset{yticklabels={,,},ytick={\empty}}        
%    \input{./fig/regression-update.tex}%
%  \end{subfigure}
  % 
  \begin{tikzpicture}[remember picture,overlay]
      % Arrow style
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=3em, minimum width=3pt, single arrow head extend=6pt, fill=black!80, anchor=center, rotate=0, inner sep=10pt, rounded corners=1pt]
    % Arrows
    \node[myarrow] (p0-arr) at ($(p0) + (1.2em,1.5em)$) {};
    %\node[myarrow] (p1-arr) at ($(p1) + (1em,1.5em)$) {};
    % Arrow labels
    \node[font=\scriptsize\sc,color=white,scale=.8] at (p0-arr) {\our};
    %\node[font=\scriptsize\sc,color=white] at (p1-arr) {new data};   
  \end{tikzpicture}
  %\end{figure}
    \alert{\bf Regression w/ 2-layers MLP.} 
    Prediction from a trained neural network's predictions {\it (left)}, our approach using inducing points to summarize the training data {\it (middle)}. SFR captures the predictive mean and uncertainty, and can incorporate new data without retraining the model {\it (right)}.
   \end{minipage}\\[2cm]

  \begin{block}{Background: NN's Function-space Representation}
  \alert{\bf Inputs} in a {\it supervised setting} for NNs $f_\mathbf{w}: \inputDomain \to \outputDomain$:\
  \begin{itemize}
  \item $\dataset = \{(\vx_{i} , \vy_{i})\}_{i=1}^{N}$, a data set w/ input $\vx_i \in \inputDomain$ and output $\vy_i \in \outputDomain$;
  \item $\weights \in \R^{P}$, the initial weights of the neural network.
  \end{itemize}
\alert{\bf Goal: }
  minimize the empirical (regularized) risk loss function:
\begin{align} 
  \weights^{*} =  \arg \min_{\weights} \mathcal{L}(\dataset,\weights) \nonumber
     = \arg \min_{\weights} \textstyle\sum_{i=1}^{N} \ell(f_\weights(\mathbf{x}_{i}), y_i) + \delta \mathcal{R}(\weights).
 \end{align}
\alert{\bf Output:} $\weights^*$, the Maximum A-Posteriori (MAP) weights of the NN.\\[0.5cm]

\alert{\bf How to capture distribution over NN model functions?}	\\
Use their first two moments, obtaining a Gaussian process with a \alert{mean function $\mu(\cdot)$} and a \alert{covariance function $\kappa(\cdot,\cdot)$} (or kernel). \\
{\it For GPs}, linear approximations in weight space lead to function-space equivalent approximations:
\begin{equation*}
	f_\weights(\vx) \approx 
\phi^\top\!(\vx) \, \vw \quad\implies\quad \mu(\vx) = 0 \quad \text{and} \quad \kappa(\vx, \vx') = \frac{1}{\delta} \, \phi^\T\!(\vx) \, \phi(\vx')
\end{equation*}\\
{\it For NNs}, we can use the \alert{Laplace-GGN approximation} to get a linear model of the neural network at the MAP as:
\begin{equation*}
	f_{\weights^*}(\vx) \approx \Jac{\weights_*}{\vx} \, \weights \implies   \mu(\vx) =  0 \quad \text{and} \quad
  \kappa(\vx, \vx')
  = \frac{1}{\delta} \, \Jac{\weights^*}{\vx} \, \JacT{\weights^*}{\vx'}, 
\end{equation*}
where $\Jac{\weights}{\vx} \coloneqq \left[ \nabla_\weights f_\weights(\vx)\right]^\top \in \R^{C \times P}$ is the Jacobian at $\weights^*$.
%\begin{itemize}
%	\item Maximum a-posteriori 
%\end{itemize}
\end{block}
\end{column}

\separatorcolumn

\begin{column}{\colwidth}
\begin{minipage}{\textwidth}
  % Colours
  \definecolor{C0}{HTML}{DF6679}
  \definecolor{C1}{HTML}{69A9CE}
\begin{figure}[t]
  \centering
  % Set figure size
  \setlength{\figurewidth}{.31\textwidth}
  \setlength{\figureheight}{\figurewidth}
  %
  \begin{tikzpicture}[outer sep=0,inner sep=0]
% This command wasn't working 
%    \newcommand{\addfig}[2]{
%    \begin{scope}
%      \clip[rounded corners=3pt] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
%      \node (#2) at (#1) {\includegraphics[width=1.05\figurewidth]{./fig/#2}};
%    \end{scope}
%    %\draw[rounded corners=3pt,line width=1.2pt,black!60] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
%    }
%
%    % The neural network
%    \addfig{0,0}{banana-nn}
    \begin{scope}
      \clip[rounded corners=3pt] ($(0,0)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (banana-nn) at (0,0) {\includegraphics[width=1.05\figurewidth]{./fig/banana-nn}};
    \end{scope}
    
    % The nn2svgp
    %\addfig{1.1\figurewidth,0}{banana-nn2svgp}
    \begin{scope}
      \clip[rounded corners=3pt] ($(1.1\figurewidth,0)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (banana-nn2svgp) at (1.1\figurewidth,0) {\includegraphics[width=1.05\figurewidth]{./fig/banana-nn2svgp}};
    \end{scope}
    

    % The update
    %\addfig{2.2\figurewidth,0}{banana-hmc}
    \begin{scope}
      \clip[rounded corners=3pt] ($(2.2\figurewidth,0)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (banana-hmc) at (2.2\figurewidth,0) {\includegraphics[width=1.05\figurewidth]{./fig/banana-hmc}};
    \end{scope}
	% The arrow
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=3em, minimum width=3pt, single arrow head extend=6pt, fill=black!80, anchor=center, rotate=0, inner sep=10pt, rounded corners=1pt]
    \tikzstyle{myblock} = [draw=black!80, minimum height=8mm, minimum width=7mm, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    \node[myarrow] (first-arr) at ($(banana-nn)!0.5!(banana-nn2svgp)$) {};
    \node[myblock] (second-arr) at ($(banana-nn2svgp)!0.5!(banana-hmc)$) {};

    % Arrow labels
    \node[font=\scriptsize\sc,color=white,scale=.8] at (first-arr) {\our};
    \node[font=\scriptsize,color=white] at (second-arr) {\normalsize$\bm\approx$};
         
    % Labels
    \node[anchor=north, font=\footnotesize] at ($(banana-nn) + (0,-.55\figureheight)$) {Neural network prediction};
    \node[anchor=north, font=\footnotesize] at ($(banana-nn2svgp) + (0,-.55\figureheight)$) {Sparse function-space representation};
    \node[anchor=north, font=\footnotesize] at ($(banana-hmc) + (0,-.55\figureheight)$) {HMC result as baseline};      
    
  \end{tikzpicture}

\end{figure}
  \newcommand{\mycircle}{\protect\tikz[baseline=-.6ex]\protect\node[circle,inner sep=5pt,draw=black,fill=C0,opacity=.5]{};}
  \newcommand{\mysquare}{\protect\tikz[baseline=-.6ex]\protect\node[inner sep=8pt,draw=black,fill=C1,opacity=.5]{};}
  \newcommand{\myinducing}{\protect\tikz[baseline=-.7ex]\protect\node[circle,inner sep=5pt,draw=black,fill=black]{};}
\alert{\bf Uncertainty quantification for classification (\,\mysquare~vs.~\mycircle).} We convert the trained neural network {\it (left)} to a sparse GP model with a set of inducing points~\myinducing\ {\it(middle)}. Results show a similar behaviour as running full Hamiltonian Monte Carlo (HMC) on the original NN model weights {\it (right)}. Marginal uncertainty depicted by colour intensity.
\end{minipage}\\[1.5cm]

 \begin{block}{SFR: Sparse Function Representation}
\alert{TODO: Leave or remove these???}
  \begin{align*}
  \alpha_i &\coloneqq \myexpect_{p(\vw \mid \vy)}[\nabla_{f}\log p(y_i \mid f) |_{f=f_i}]
  \quad \text{and} \\
  \beta_i &\coloneqq - \myexpect_{p(\vw \mid \vy)}[\nabla^2_{f f}\log p(y_i \mid f_i) |_{f=f_i}]
\end{align*}

%\heading{Dual parameterization}
\alert{\bf Dual parameterization.} The first two moments of the resultant posterior process using the Laplace can be approximated via the dual parameters $\hat{\valpha}, \hat{\vbeta} \in \R^{N}$: 
\begin{equation*}
  \hat{\alpha}_i \coloneqq \nabla_{f}\log p(y_i \mid f) |_{f=f_i}
  \quad \text{and} \quad
  \hat{\beta}_i \coloneqq - \nabla^2_{ff}\log p(y_i \mid f) |_{f=f_i}.
\end{equation*}
But it scales w/ $\mathcal{O}(N^3)$ $\rightarrow$ Sparsify the resulting GP model to get $\mathcal{O}(M^3)$
%\heading{Sparse Neural Network GP}
\alert{\bf Sparsifying the NN's GP.}
\begin{equation*}
\valpha_{\vu}  =  \sum_{i=1}^N  \vkzi \, \hat{\alpha}_{i} \in \R^M
\; \text{and} \;
  \MBeta_{\vu} =  \sum_{i=1}^N \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T} \in \R^{M \times M}
\end{equation*}
Using this sparse definition of the dual variables, our sparse GP posterior takes the following form:
\begin{align*}   
\myexpect_{q_{\vu}(\vf)}[f_i] &= \vkzs^{\T} \MKzz^{-1} \valpha_{\vu}
   \quad \text{and} \\ 
\textrm{Var}_{q_{\vu}(\vf)}[f_i] &= k_{ii} - \vkzs^\top [\MKzz^{-1} - (\MKzz + \MBeta_{\vu})^{-1} ]\vkzs
\end{align*}



\end{block}
\end{column}

\separatorcolumn
\end{columns}

\begin{columns}
	\begin{column}{2.05\colwidth}
		  \begin{block}{Results and Discussion}

%\begin{minipage}{\textwidth}
%\begin{table}
%  \centering\scriptsize
%  %\caption{Comparisons and ablations on UCI data with negative log predictive density (NLPD\textcolor{gray}{\footnotesize$\pm$std}, lower better). Our sparse \our ($M=256$) is on par with full models (left) and outperforms the GP subset approach of \citet{immer2021improving} (right). Results for methods marked with * as reported in the original benchmark~\citep{immer2021improving}.} %See \cref{app:uci} for additional tables with comparisons.}
%	
%	% Control table spacing
%	\renewcommand{\arraystretch}{1.}
%	\setlength{\tabcolsep}{1.2pt}
%	\setlength{\tblw}{0.083\textwidth}  
%
%    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
%	\input{tables/uci.tex}
%\end{table}
\begin{table}[t!]
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{llllllll}
\toprule
Model & BNN & GLM & GP Subset (GP) & GP Subset (NN) & NN MAP & SFR (GP) & SFR (NN) \\
Data set &  &  &  &  &  &  &  \\
\midrule
Glass & 0.9562 $\pm$ 0.2649 & 0.8966 $\pm$ 0.1424 & 1.5108 $\pm$ 0.0367 & 1.1420 $\pm$ 0.0738 & 1.8350 $\pm$ 0.6147 & 1.3709 $\pm$ 0.0375 & 1.0922 $\pm$ 0.1363 \\
Waveform & 0.2693 $\pm$ 0.0310 & 0.2676 $\pm$ 0.0227 & 0.2702 $\pm$ 0.0192 & 0.2670 $\pm$ 0.0162 & 0.2670 $\pm$ 0.0162 & 0.2657 $\pm$ 0.0163 & 0.2670 $\pm$ 0.0162 \\
\bottomrule
\end{tabular}
}
\end{table}
\alert{\bf UCI Results.} For now a nice placeholder table that should contain the comparisons and ablations on UCI data with negative log predictive density. %(NLPD\textcolor{gray}{\footnotesize$\pm$std}, lower better). Our sparse our ($M=256$) is on par with full models (left) and outperforms the GP subset approach of \citet{immer2021improving} (right). Results for methods marked with * as reported in the original benchmark~\citep{immer2021improving}. 
%\end{minipage}
  \begin{itemize}
     \item Item here
     \item Another item
  \end{itemize}

  \end{block}


%  \begin{block}{Discussion}
%
%  \begin{itemize}
%     \item Item here
%     \item Another item
%  \end{itemize}
%
%  \end{block}

  \vspace*{1em}

  \nocite{*} % <-- This lists all references that are in the bib file

  \begin{block}{References}
    \vspace*{-.25em}
    \footnotesize{\bibliographystyle{ieeetr}\bibliography{bibliography}}
  \end{block}

	\end{column}
\end{columns}
\end{frame}

\end{document}
