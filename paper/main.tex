% !TeX spellcheck = en_GB
\documentclass{article}

% Pass options to natbib
\PassOptionsToPackage{numbers, compress}{natbib}

% NeurIPS packages
\usepackage[]{neurips_2023}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Redefine paragraph to be tighter
\renewcommand{\paragraph}[1]{{\bf #1}~~}

% Array/table packages
\usepackage{tabularx}
\usepackage{array,multirow}
\usepackage{colortbl}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newlength{\tblw}

% Latin
\usepackage{xspace}
\newcommand{\eg}{\textit{e.g.\@}\xspace}
\newcommand{\ie}{\textit{i.e.\@}\xspace}
\newcommand{\cf}{\textit{cf.\@}\xspace}
\newcommand{\etc}{\textit{etc.\@}\xspace}
\newcommand{\etal}{\textit{et~al.\@}\xspace}

% Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations,backgrounds,arrows.meta,calc}
\usetikzlibrary{shapes,arrows,positioning}

% Appendix/supplement title
\newcommand{\nipstitle}[1]{{%
    % rules for title box at top and bottom
    \def\toptitlebar{\hrule height4pt \vskip .25in \vskip -\parskip} 
    \def\bottomtitlebar{\vskip .29in \vskip -\parskip \hrule height1pt \vskip .09in} 
    \phantomsection\hsize\textwidth\linewidth\hsize%
    \vskip 0.1in%
    \toptitlebar%
    \begin{minipage}{\textwidth}%
        \centering{\LARGE\bf #1\par}%
    \end{minipage}%
    \bottomtitlebar%
    \addcontentsline{toc}{section}{#1}%
}}

% Bibliography
%\usepackage[maxcitenames=1, maxbibnames=4, doi=false, isbn=false, eprint=true, backend=bibtex, hyperref=true, url=false, style=authoryear-comp]{biblatex}
%\addbibresource{zotero-library.bib}
% \addbibresource{paper/zotero-library.bib}

% Let's use good old bibtex instead

% Figure customization: Tight legend box
\pgfplotsset{every axis/.append style={
		legend style={inner xsep=1pt, inner ysep=0.5pt, nodes={inner sep=1pt, text depth=0.1em},draw=none,fill=none}
}}

% Our packages
\usepackage{todonotes}
\usepackage[colorlinks=true,linkcolor=blue,allcolors=blue]{hyperref}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{derivative}

\usepackage{tikz,pgfplots}
\usepackage{subcaption}
\usetikzlibrary{}

\input{aidans-utils.tex}

% Short section names etc
% This must be imported last!
%\usepackage{cleveref}
\usepackage[capitalise,nameinlink]{cleveref}
\crefname{section}{Sec.}{Secs.}
\crefname{algorithm}{Alg.}{Algs.}
\crefname{appendix}{App.}{Apps.}
\crefname{definition}{Def.}{Defs.}
\crefname{table}{Tab.}{Tabs}

% Config for Arno's awesome TikZ plotting stuff
\newlength{\figurewidth}
\newlength{\figureheight}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\action}{\ensuremath{\mathbf{a}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}
\newcommand{\numDataNew}{\ensuremath{N^{\text{new}}}}
\newcommand{\numDataOld}{\ensuremath{N^{\text{old}}}}
\newcommand{\numInducing}{\ensuremath{M}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\inputDomain}{\ensuremath{\mathbb{R}^{D}}}
\newcommand{\outputDomain}{\ensuremath{\mathbb{R}^{C}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}

% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
% \newcommand{\weights}{\ensuremath{\bm\phi}}
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transitionFnWithParams}{\ensuremath{\transitionFn_{\weights}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}


% Packages for bold math
\usepackage{bm}
\newcommand{\mathbold}[1]{\bm{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}



% Math Macros
\newcommand{\MB}{\mbf{B}}
\newcommand{\MC}{\mbf{C}}
\newcommand{\MZ}{\mbf{Z}}
\newcommand{\MV}{\mbf{V}}
\newcommand{\MX}{\mbf{X}}
\newcommand{\MA}{\mbf{A}}
\newcommand{\MK}{\mbf{K}}
\newcommand{\MI}{\mbf{I}}
\newcommand{\MH}{\mbf{H}}
\newcommand{\T}{\top}
\newcommand{\vzeros}{\mbf{0}}
\newcommand{\vtheta}[0]{\mathbold{\theta}}
\newcommand{\valpha}[0]{\mathbold{\alpha}}
\newcommand{\vkappa}[0]{\mathbold{\kappa}}
\newcommand{\vbeta}[0]{\mathbold{\beta}}
\newcommand{\vlambda}[0]{\mathbold{\lambda}}
\newcommand{\diag}{\text{{diag}}}

\newcommand{\vm}{\mbf{m}}
\newcommand{\vz}{\mbf{z}}
\newcommand{\vf}{\mbf{f}}
\newcommand{\vu}{\mbf{u}}
\newcommand{\vx}{\mbf{x}}
\newcommand{\vy}{\mbf{y}}
\newcommand{\vw}{\mbf{w}}

\newcommand{\Jac}[2]{\mathcal{J}_{#1}(#2)}
\newcommand{\JacT}[2]{\mathcal{J}_{#1}^\top(#2)}


\newcommand{\GP}{\mathcal{GP}}
\newcommand{\KL}[2]{\mathrm{D}_\textrm{KL} \dbar*{#1}{#2}}
\newcommand{\MKzz}{\mbf{K}_{\mbf{z}\mbf{z}}}
\newcommand{\MKxx}{\mbf{K}_{\mbf{x}\mbf{x}}}
\newcommand{\MKzx}{\mbf{K}_{\mbf{z}\mbf{x}}}
\newcommand{\MKxz}{\mbf{K}_{\mbf{x}\mbf{z}}}
\newcommand{\vkzi}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vkzs}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vk}{\mbf{k}}
\newcommand{\MLambda}[0]{\mathbold{\Lambda}}
\newcommand{\MSigma}[0]{\mathbold{\Sigma}}
\definecolor{matplotlib-blue}{HTML}{1f77b4}
\newcommand{\N}{\mathrm{N}}
%\newcommand{\R}{\mathrm{R}}
\newcommand{\myexpect}{\mathbb{E}}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\Norm}{\mathcal{N}}


%\title{Investigatin Uncertainty Quantification in Model-based Reinforcement Learning}
% \title{Model-based Reinforcement Learning with Fast Posterior Updates}
%\title{Sequential Decision-Making under Uncertainty with Big Data}
% \title{Neural Network to Vatiational Sparse Gaussian Process: For Adaptive Exploration}
% \title{Neural Network to Sparse Variational Gaussian Process: For Updates in Sequential Decision Making}
% \title{Adapting Neural Networks to New Data For Updates in Sequential Decision Making via Gaussian Processes}
% \title{Converting Neural Networks to Gaussian Processes for Sequential Decision-Making Under Uncertainty}
%\title{Sparse Function Space Representation of Neural Networks for Exploration and Retention}
%\title{Sparse Function-space Neural Networks}
\title{Sparse Function-space Representation \\ of Neural Networks}% for Adaptation and Retention}
\author{%
  Aidan Scannell\textsuperscript{\star} \\
  Aalto University \\
  Finnish Center for Artificial Intelligence \\
  \texttt{aidan.scannell@aalto.fi}
  \And
  Riccardo Mereu\textsuperscript{\star} \\
  Aalto University\\
  \texttt{riccardo.mereu@aalto.fi}
  \And
  Paul Chang \\
  Aalto University\\
  \texttt{paul.chang@aalto.fi}
  \And
  Ella Tamir \\
  Aalto University\\
  \texttt{ella.tamir@aalto.fi}
  \And
  Joni Pajarinen \\
  Aalto University\\
  \texttt{joni.pajarinen@aalto.fi}
  \And
  Arno Solin \\
  Aalto University\\
  \texttt{arno.solin@aalto.fi}
}


\begin{document}

\maketitle

\begin{abstract}
% OLDER VERSION
%Sequential learning paradigms such as Continual Learning (CL) or Reinforcement Learning (RL) pose a challenge for gradient-based deep learning techniques as they struggle to incorporate new data and retain previous knowledge. Existing methods for converting neural networks from weight to function space allow a probabilistic treatment of the distribution over the function learned by the neural networks but are computationally expensive. We propose a method that converts a neural network to a low-rank functional representation as a sparse Gaussian process. With this approach, we can build a compact representation of the function encoded by the neural network that can replace previous data in continual settings and be used for fast adaptation in RL, avoiding full retraining of the model. 
%
% Rewrite on 2023-05-10
%Deep neural networks are known to lack uncertainty estimates, struggle to incorporate new data, and fail to retain previous knowledge. We present a method that mitigates these issues by transforming a weight-space neural network to a low-rank function-space representation, via the so-called dual parameters. In contrast to previous work, we model the joint distribution across the entire data set rather than a subset. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We demonstrate the proposed approach for quantifying uncertainty in supervised learning and maintaining a compact representation in sequential learning.\looseness-1

Deep neural networks are known to lack uncertainty estimates, struggle to incorporate new data, and suffer from catastrophic forgetting. We present a method that mitigates these issues by converting neural networks from weight-space to a low-rank function-space representation, via the so-called dual parameters. In contrast to previous work, our sparse representation captures the joint distribution over the entire data set, rather than only over a subset. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We demonstrate the proposed approach for quantifying uncertainty in supervised learning and maintaining an expressive functional representation for sequential learning.\looseness-1

\end{abstract}

%, maintaining a summary representation in continual learning,

\section{Introduction}
\label{sec:intro}
%
Deep learning has become the cornerstone of contemporary artificial intelligence, proving remarkably effective in tackling supervised and unsupervised learning tasks in the {\em large data}, {\em offline}, and {\em gradient-based training} regime. Despite its success, gradient-based learning techniques exhibit limitations. Firstly, how to incorporate uncertainty quantification over predictions which is needed in many applications. Secondly, in the sequential data regime data arrives not in batches 

Secondly, how to retain information from previous tasks whilst learning new tasks where the tasks aresuch as Continual Learning (CL, \cite{de2021continual})

Thirdly and differently from the CL problem is that given some data from the same distribution 

when applied to sequential learning paradigms, such as Continual Learning (CL, \cite{mccloskey1989catastrophic}) and Reinforcement Learning (RL, \cite{sutton2018reinforcement}). Recently there has been some success in tackling uncertainty quantification

In CL, the challenge is retaining a compact representation of the problem and avoid forgetting over the life-long learning horizon (see, \eg, \cite{add}). Similarly in RL, the model would need to adapt to observations of the environment through exploration, but this can be inefficient and would require constant retraining (\eg, \cite{add, add}).

%Current state of affairs 
Recent work on techniques (\eg, \cite{list}) for converting trained neural networks from weight space to function space allow a principled mathematical approach for analyzing the behaviour \cite{add}, performing probabilistic inference \cite{add}, and quantifying uncertainty in neural networks \cite{add}. The methods rely on linearizing the neural network and capturing the first two moments in function space in terms of a mean function and covariance function (or kernel) --- defining a Gaussian process (GP, \cite{rasmussen2006gaussian}). GPs provide a widely-used probabilistic toolkit with principled uncertainty estimates, and they are a standard surrogate model for Bayesian optimization \citep{garnett_bayesoptbook_2022} and are effective in model-based reinforcement learning \citep{deisenroth2011pilco} with theoretical guarantees on regret bounds \citep{srinivas2009gaussian}. 
%Yet many problems lie in high dimensional input space; for example, images are where GPs cannot learn representations. In such scenarios such as in many reinforcement learning environments neural networks are used as the surrogate model. However, uncertainty is still essential to ensure effective exploration for sequential algorithms. Successful approaches have attempted to blend neural networks with uncertainty estimates around predictions, allowing for sophisticated exploration strategies. However, there has been limited use of hybrid models that possess the feature representation ability of neural networks but also attractive the properties of GPs, such a hybrid method we propose in this paper.

%Need for adaptive learning methods + failures with current methods
In this paper, we show that we can take any neural network and represent the model in terms of the so-called `dual' parameters of a Gaussian process \cite{csato2002sparse, adam2021dual, chang2020fast}. Crucially, the resultant GP now has predictions in the same space as the initial trained neural network, a property not of previous approaches. Given the dual parameter formulation, we can convert a full GP to a variational sparse version. Using the sparse GP we can then take advantage of the property of dual parameters that allows for fast conditioning recently shown in \cite{chang2022fantasizing}. Thus we are able to avoid retraining and condition new data in to our model. The resultant DNN2SGP method can be used in any sequential decision-making scenario where retraining is not available.%and we show its effectiveness in a planning stage of a model based reinforcement algorithim.

\todo{a pragraph on how this can be used for improving CL and RL}

%Need uncertainty and adaptive methods
%Dual formulation in GPs space solves this
%Talk about planning and exploration in RL.


We introduce a sparse representation of the neural network in the function space. 
%
{\em (i)}~principled uncertainty quantification.
{\em (ii)}~A compact representation for continual learning
{\em (iii)}~Fast updates for adaptive exploration.
%
We demonstrate points {\em(i)--(iii)} in the experiments.

%List the contributions.
%The contributions of the paper our is as follows:
%\begin{itemize}
%\item We show how to take a trained neural network and convert it to a dual sparse GP. We are able to do this without retraining a variational objective for the Sparse GP. Our sparse GP uses the variational formulation, and thus gives better uncertainty estimates than other no variational sparse approaches used previously.
%\item The sparse GP now gives us a compact representation of our parameters in the function space. We can therefore take advantage of the dual parameters formulation for fast conditioning of new data in to our posterior avoiding retraining of the neural network. Crucially this allows for fast adaptation of models that our used in sequential decision making. We show how this is effective in the planning stage of model-based reinforcement exploration.
%\end{itemize}




%
%
%\begin{itemize}
%  \item Many real-world problems require learning-based systems that can adapt to new data.
%  \begin{itemize}
%    % \item For example, in domains such as robotics and healthcare,
%    \item For example, when controlling robots in non-stationary environments it is important for the robot to adapt to the changing dynamics.
%    \item However neural networks rely upon gradient-based optimisation.
%    \item Uncertainty can be used to improve sample efficiency via targeted exploration.
%    \item Uncertainty can be used to handle risk in decision making.
%  \end{itemize}
%
%  \item Gaussian processes can easily adapt to new data and they offer well-calibrated uncertainty estimates.
%  \begin{itemize}
%    \item However, they don't scale to high-dimensional and large data sets.
%  \end{itemize}
%  \item
%  \begin{itemize}
%    \item
%  \end{itemize}
%\end{itemize}




\begin{figure}[t!]
  \centering\scriptsize
  % Figure options
  \pgfplotsset{axis on top,scale only axis,width=\figurewidth,height=\figureheight, ylabel near ticks,ylabel style={yshift=-2pt},y tick label style={rotate=90},legend style={nodes={scale=1., transform shape}},tick label style={font=\tiny,scale=1}}
  \pgfplotsset{xlabel={Input, $x$},axis line style={rounded corners=2pt}}
  % Set figure 
  \setlength{\figurewidth}{.28\textwidth}
  \setlength{\figureheight}{\figurewidth}
  %
  \def\inducing{\normalsize Sparse inducing points}
  %
  \begin{subfigure}[c]{.34\textwidth}
    \raggedleft
    \pgfplotsset{ylabel={Output, $y$}}
    \input{./fig/regression-nn.tex}%
  \end{subfigure}
  \hfill  
  \begin{subfigure}[c]{.01\textwidth}
    \centering
    \tikz[overlay,remember picture]\node(p0){};
  \end{subfigure}  
  \hfill
  \begin{subfigure}[c]{.28\textwidth}
    \raggedleft
    \pgfplotsset{yticklabels={,,},ytick={\empty}}
    \input{./fig/regression-nn2svgp.tex}%
  \end{subfigure}
  \hfill  
  \begin{subfigure}[c]{.01\textwidth}
    \centering
    \tikz[overlay,remember picture]\node(p1){};
  \end{subfigure}  
  \hfill
  \begin{subfigure}[c]{.28\textwidth}
    \raggedleft
    \pgfplotsset{yticklabels={,,},ytick={\empty}}        
    \input{./fig/regression-update.tex}%
  \end{subfigure}
  \caption{\textbf{Regression example on an MLP with two hidden layers.} Left:~Predictions from the trained neural network. Center:~Our approach summarizes the...}
  \label{fig:teaser} 
  % 
  \begin{tikzpicture}[remember picture,overlay]
    % Arrow style
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=14mm, minimum width=2pt, single arrow head extend=4pt, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    % Arrows
    \node[myarrow] (p0-arr) at ($(p0) + (1em,1.5em)$) {};
    \node[myarrow] (p1-arr) at ($(p1) + (1em,1.5em)$) {};
    % Arrow labels
    \node[font=\scriptsize\sc,color=white] at (p0-arr) {nn2svgp};
    \node[font=\scriptsize\sc,color=white] at (p1-arr) {new data};   
  \end{tikzpicture}
\end{figure}




\subsection{Related work}
\label{sec:related}
%
% General Bayesian deep learning
Bayesian deep learning~\cite{Wilson:ensembles,neal1995bayesian} has recently gained increasing attention in the machine learning community as a means for uncertainty quantification (\eg, \cite{kendall2017what,wilson2020bayes}) and model selection (\eg,~\cite{immer2021marginal,antoran2022marginal}) with advancements in prior specification (\eg, \cite{meronen2020stationary,meronen2021periodic,fortuin2021bayesian,nalisnick2018do}) and efficient approximate inference under the specified model.
Calculating the posterior distribution of a Bayesian neural network is usually intractable, and approximate inference techniques need to be used, such as variational inference \cite{blei2017variational}, deep ensembles \cite{lakshminarayanan2017simple}, MC dropout \cite{gal2016dropout}, or Laplace approximation \cite{ritter2018kfac,kristiadi2020being,immer2021improving}---each having strengths and weaknesses. % point out strngths and weaknesses for each

% Introduce the laplace GGN + shortcomings
One common approach for uncertainty with neural networks is a Laplace-GGN approximation \citep{daxberger2021laplace}, which takes a trained neural network and linearises it around the optimal weights. The resulting linear model can be used to obtain uncertainty estimates and refine the neural network predictions \citep{immerScalable2021}, and is linear with respect to the weights. Therefore, as shown in \citep{immerScalable2021, khan2019approximate}, the linear model can be converted to a Gaussian process (GP), which is then used to obtain the uncertainty estimates.
However, the GP introduces a computational scaling that is cubic in the number of data points ($\mathcal{O}(n^3)$). Many approaches apply crude approximations to alleviate the computational costs, \eg, considering only a subset of the training datas, but are still obtain good uncertainty estimates \citep{immerScalable2021}\todo{other methods?}. 

The Hessian can be efficiently approximated using the generalised Gauss-Newton algorithm \cite{botev2017practical} or by Kronecker factorisation \cite{martens2015optimizing,ritter2018kfac}.


%% CL section 
%%
%% EWC, SI, VCL -> weight regularization 
% 1) what is the central problem of CL? 
% 		Training from a non-stationary distribution the models overwrites the previously learnt parameters and then forgets the previous learnt function.
% 2) What others have done?
% 		starting with EWC and SI, that are the most known methods in term of weight regularization. And others took a different approach again in the weight regularization real with VCL (to check)
% 3) People started to try experimenting with the functional regularization -> 
% In order of complexity -> DER, FROMP, S-FSVI
% In order of pubblication -> DER/FROMP, K-Priors (?), S-FSVI
EWC \cite{kirkpatrick2017overcoming}
SI \cite{zenke17a}, 
VCL \cite{nguyen-tuongModel2009},  

%% DER, FROMP, S-FSVI
DER \cite{buzzega2020dark}
FROMP \cite{pan2020continual}
S-FSVI \cite{rudner2022continual}


%Why we need uncertainty and adaptiveness
%How to adapt to new information is a limitation of current machine learning models. The problem is specifically relevant when we combine a model with a decision-making process. In such a sequential setting, an agent typically makes decisions in an environment and obtains new information and would ideally incorporate the information into their parameters. Not only do we need models that adapt to new informatoion but all express uncertainty over the models predictions. The reason being twofold: in many applications, \eg\ healthcare, autonomous driving is an essential requirement, and many advanced exploration techniques require uncertainty to determine where to query next.



\begin{figure}[t!]
  \centering
  % Set figure size
  \setlength{\figurewidth}{.31\textwidth}
  \setlength{\figureheight}{\figurewidth}
  %
  % Colours
  \definecolor{C0}{HTML}{DF6679}
  \definecolor{C1}{HTML}{69A9CE}
  %
  \begin{tikzpicture}[outer sep=0,inner sep=0]

    \newcommand{\addfig}[2]{
    \begin{scope}
      \clip[rounded corners=3pt] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (#2) at (#1) {\includegraphics[width=1.05\figurewidth]{./fig/#2}};
    \end{scope}
    %\draw[rounded corners=3pt,line width=1.2pt,black!60] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
    }

    % The neural network
    \addfig{0,0}{banana-nn}

    % The nn2svgp
    \addfig{1.1\figurewidth,0}{banana-nn2svgp}

    % The update
    \addfig{2.2\figurewidth,0}{banana-hmc}

    % The arrow
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=14mm, minimum width=2pt, single arrow head extend=4pt, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    \tikzstyle{myblock} = [draw=black!80, minimum height=4mm, minimum width=7mm, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    \node[myarrow] (first-arr) at ($(banana-nn)!0.5!(banana-nn2svgp)$) {};
    \node[myblock] (second-arr) at ($(banana-nn2svgp)!0.5!(banana-hmc)$) {};

    % Arrow labels
    \node[font=\scriptsize\sc,color=white] at (first-arr) {nn2svgp};
    \node[font=\scriptsize\sc,color=white] at (second-arr) {\normalsize$\bm\approx$};
         
    % Labels
    \node[anchor=north, font=\small] at ($(banana-nn) + (0,-.55\figureheight)$) {Neural network prediction};
    \node[anchor=north, font=\small] at ($(banana-nn2svgp) + (0,-.55\figureheight)$) {Sparse GP representation};
    \node[anchor=north, font=\small] at ($(banana-hmc) + (0,-.55\figureheight)$) {HMC result as baseline};      

  \end{tikzpicture}
  \newcommand{\mycircle}{\protect\tikz[baseline=-.5ex]\protect\node[circle,inner sep=2pt,draw=black,fill=C0,opacity=.5]{};}
  \newcommand{\mysquare}{\protect\tikz[baseline=-.5ex]\protect\node[inner sep=2.5pt,draw=black,fill=C1,opacity=.5]{};}
  %
  \caption{Binary classification (\mysquare~vs.~\mycircle) on the {\sc banana} toy data data set. We convert the left-hand neural network  }
  \label{fig:banana}  
\end{figure}



\section{Background}
\label{sec:methods}
%
In supervised learning, given a data set $\dataset = \{(\mathbf{x}_{i} , \mathbf{y}_{i})\}_{i=1}^{n}$, with input $\mathbf{x}_i \in \inputDomain$ and output $\mathbf{y}_i \in \outputDomain$ pairs, the weights $\weights \in \R^{P}$ of a neural network, $f_\mathbf{w} : \inputDomain \to \outputDomain$, are usually trained to minimize the (regularized) empirical risk,
%
\begin{equation} \label{eq-empirical-risk}
  \weights^{*} = 
  \arg \min_{\weights} \mathcal{L}(\dataset,\weights) =
  \arg \min_{\weights} \sum_{i=1}^{n} \ell(f_\weights(\mathbf{x}_{i}), \mathbf{y}_i) + \delta \mathcal{R}(\weights).
\end{equation}
%
If $\ell(f_\weights(\mathbf{x}_{i}), \mathbf{y}_i) = -\log(p(\mathbf{y} \mid f_\weights(\mathbf{x}_{i}))$ and the regularizer $\mathcal{R}(\weights) = \frac{1}{2}\|\weights\|^{2}_2$, then we can view \cref{eq-empirical-risk} as the maximum {\it a~posteriori} (MAP) solution to a Bayesian objective, where the regularization weight takes the role of a prior precision parameter, \ie, $p(\vw) = \Norm(\vzeros, \delta^{-1} \MI)$.

%
\subsection{Linearization gives rise to a Gaussian process} 
\label{sec:nn2gp}
%
Quantifying uncertainty in a Bayesian framework boils down to finding a posterior of $p(\vw \mid \dataset)$, which represents the uncertainty over our parameters based on the prior and the data. The posterior structure is related to the loss-landscape around the MAP weights $\vw^*$, which induces a correlation structure on functions produced from the neural network's deterministic mapping. Samples from the weight posterior are perturbed versions of $f_{\vw^*}$ characterizing uncertainty in the learned model in the function space. Although the true posterior is intractable given the non-linearities of the neural network, sampling techniques available, such as Hamiltonian Monte-Carlo (HMC), in most cases, are not available given the computational burden.
 The uncertainty plotted \cref{fig:banana} shows the induced function space posterior by sampling through the model.

A common approach is to approximate the correlation structure of a distribution centred at the MAP estimate as done in the Laplace-GGN \citep{khan2019approximate, daxberger2021laplace, maddox2021fast}. The Laplace GGN takes the MAP solution and approximates the Hessian of our loss function  $\ell(f_\weights(\mathbf{x}_{i}), \mathbf{y}_i)$ with the GGN approximation. Another way of viewing the approximation is that we build the following approximate model $\hat{f}_\weights(\mathbf{x}_{i}) = \Jac{\weights}{\mathbf{x}_i} \vw $  where $\Jac{\weights}{\mathbf{x}} \coloneqq \nabla_\weights f(\mathbf{x})^\top \in \R^{C \times P}$ is the Jacobian as was pointed out in \citep{khan2019approximate}. Using the Hessian of the approximate model, we arrive at the Laplace-GGN posterior for $q(\vw)$.

In this paper we are concerned with learning posterior directly in the function space that is having a $q(\vf)$ instead of a $q(\vw)$. A common choice is to use a Gaussian process characterizes a distribution over functions in terms of the first two moments, a mean $\mu(\cdot)$ and covariance function $\kappa(\cdot,\cdot)$ (kernel). We aim to find a kernel and mean function based on the neural network to build the Gaussian process posterior.  A linear function in terms of parameters can be converted from the weight space to the function space (see Ch.~XXX in \cite{rasmussen2006gaussian} as follows:
\begin{equation} \label{eq:weight_func}
g_\weights(\mathbf{x}_{i}) = \phi^\top\!(\vx) \, \vw \implies \mu(\vx) = \vzeros \quad \kappa(\mathbf{x}, \mathbf{x}') = \frac{1}{\delta} \phi(\vx)^\top \phi(\vx)
\end{equation}






- distribution over w given to you by the non-linear function. One could sample over weights pass it over the function and you have distribution over functions. HMC is heavy, structure of the problem around the map estimate. characteruze this as the first two moments of the distributions over the functions restrict to the first two moments.  

Replace with a taylor series at the MAP first order taylor expansion in w. degenerate cov function.

- more citations there.

- GP induced by the trained neural network. not GP prior.

- Add the NTK bit back in.

- assume that when you are at the map estimate when you have converged this should be small. 

- Write out that w beomces zero under the expectation.

-  transition from background to methods. 

- More explination of the dual equation 5 the loss landscape link. Perbutation point linked directly to that here. 

- 



%A Gaussian process characterizes a distribution over functions in terms of the first two moments. The key quantity of a Gaussian process is the prior covariance function (or kernel function) $\kappa(\vx,\vx')$, where $\vx, \vx'$ are input vectors in $\R^{\mathrm{D}}$. We aim to find a kernel based on the neural network to build the Gaussian process posterior.  A linear function in terms of parameters can easily be converted from the weight space to the function space as follows:
%\begin{equation}
%g_\weights(\mathbf{x}_{i})) = \phi(\vx)^\top \vw \implies \mu(\vx) = \vzeros \quad \kappa(\mathbf{x}, \mathbf{x}') = \frac{1}{\delta} \phi(\vx)^\top \phi(\vx)
%\end{equation}
%
%
%
% The approximation can be seen as taking the map solution of \cref{eq-empirical-risk} as the mean of $q(\vw)$ but then substituting the following functional approximation for $\hat{f}_\weights(\mathbf{x}_{i}) = \Jac{\weights^*}{\mathbf{x}} $


%A Gaussian procss characterizes a distribution over functions in terms of the first two moments. The key quantity of 
%A Gaussian process is the prior covariance function (or kernel function) $\kappa(\vx,\vx')$, where $\vx, \vx'$ are input vectors in $\R^{\mathrm{D}}$. We aim to find a kernel based on the neural network to build the Gaussian process posterior. 
%A GP is fully characterized by a mean $\mu(\cdot)$ and covariance function $\kappa(\cdot,\cdot)$ (kernel).
%If we had the following function parameterization $g(\vx) = \phi(\vx)^\top \vw$ where $\phi(\vx)$ is a feature map, we can convert from weight space prior to function space prior giving the following kernel $\kappa(\vx, \vx') = \delta^{-1} \phi(\vx) \phi^\T(\vx')$ (see Ch.~XXX in \cite{rasmussen2006gaussian} for details).\todo{this part needs to be rewritten, starts talking about a general function parameterisation and then jumps directly to the neural network} 
%\subsection{Neural Network to GP Prior} 
%\label{sec:nn2gp}
%

A Gaussian process characterizes a distribution over functions in terms of the first two moments, a mean $\mu(\cdot)$ and covariance function $\kappa(\cdot,\cdot)$ (kernel). We aim to find a kernel and mean function based on the neural network to build the Gaussian process posterior.  A linear function in terms of parameters can be converted from the weight space to the function space (see Ch.~XXX in \cite{rasmussen2006gaussian} as follows:
\begin{equation} \label{eq:weight_func}
g_\weights(\mathbf{x}) = \phi^\top\!(\vx) \, \vw \implies \mu(\vx) = \vzeros \quad \kappa(\mathbf{x}, \mathbf{x}') = \frac{1}{\delta} \phi(\vx)^\top \phi(\vx)
\end{equation}
Given an approximate posterior $q(\vw)$, \citet{immerImprovingPredictionsBayesian2021} proposed to make the following linear approximation
$f^{\textrm{lin.}}_{\vw^*}(\vx; \vw) = f_{\vw^*}(\vx) + \Jac{\weights^*}{\mathbf{x}} \, (\vw - \vw^*)$ of the neural network, where $\Jac{\weights}{\mathbf{x}} \coloneqq \nabla_\weights f(\mathbf{x})^\top \in \R^{C \times P}$ is the Jacobian. The approximation can be used with any approximate inference technique but in the paper they focus on the Laplace-GGN. They find that the linear approximation helps to improve predictions by correcting the approximate posterior when sampled through a non-Gaussian likelihoods.

Furthermore, under the linear approximation they showed one can convert the weight space prior to the function space by combining with \cref{eq:weight_func} meaning the converged neural network GP prior is:
\begin{equation} \label{eq-laplace-approx-function-space}
% g(\vx) \sim \GP \left( \mu(\vx), \kappa(\vx, \vx') \right) \quad \text{with} \quad
  \mu(\vx) =  f^{\textrm{lin.}}_{\vw_*}(\vx; \vw = \vzeros) = f_{\vw^*}(\vx) - \Jac{\weights^*}{\mathbf{x}} \vw^* \quad \text{and} \quad
  \kappa(\mathbf{x}, \mathbf{x}')
  = \frac{1}{\delta} \Jac{\weights^*}{\mathbf{x}}\JacT{\weights^*}{\mathbf{x}'},
\end{equation}
%
%where the kernel is the so-called Neural Tangent Kernel (NTK, \cite{jacot2018neural}) with Jacobian $\Jac{\weights}{\mathbf{x}} \coloneqq \nabla_\weights f(\mathbf{x})^\top \in \R^{C \times P}$. Alternatively, we could have followed the derivation by \citet{immerImprovingPredictionsBayesian2021}, where they perform a local linearization of the neural network  $f^{\textrm{lin.}}_{\vw^*}(\vx; \vw) = f_{\vw^*}(\vx) + \Jac{\weights^*}{\mathbf{x}} \, (\vw - \vw^*)$. Their mean function differ from ours, \cref{eq-laplace-approx-function-space}, as theirs comes from setting  $\mu(\vx) = f^{\textrm{lin.}}_{\vw_*}(\vx; \vw = \vzeros) = f_{\vw^*}(\vx) - \Jac{\weights^*}{\mathbf{x}} \vw^*$, \ie the difference between the linear model approximation and the neural network function at $\vx$. %which does not seem useful as a prior mean function, 
%On the other hand, we set $\mu(\vx) = \vzeros$, as common in GP literature and derived from a feature map conversion. \todo{derived what?}

\section{Methods}
\paragraph{Linearization in the dual parameters}
As stated in \citet[Lemma~1]{csato2002sparse}, the posterior process mean and covariance functions of a GP prior, combined with a likelihood function and $\dataset$ can be stated as:
\begin{equation}  \label{eq:gp_pred}
 \myexpect_{p(f_i \mid\vy)}[f_i] = \vk_{\vx i}^\top \valpha \quad \text{and} \quad
\mathrm{Var}_{p(f\mid\vy)}[f_i] = \kappa_{ii} - \vk_{\vx i}^\top ( \MKxx + \diag(\vbeta)^{-1})^{-1} \vk_{\vx i},
\end{equation}
where $f_i = f_\weights(\vx_i)$, the $ij$\textsuperscript{th} entry of the matrix $\MKxx \in \R^{m \times m}$ is $\kappa(\vx_i,\vx_j)$, $\vk_{\vx i}$ denotes a vector where each $j$\textsuperscript{th} element is $\kappa(\vx_i, \vx_j)$, and $\kappa_{ii} = \kappa(\vx_i, \vx_i)$.  The dual parameters consist of the vector $\valpha \in \R^{m}$ and vector $\vbeta \in \R^{m}$, defined as:
\begin{equation}
\label{eq:dual_param}
\alpha_i \coloneqq \myexpect_{p(\vw \mid \vy)}[\nabla_{f_i}\log p(y_i \mid f_i)],  \quad
\beta_i \coloneqq - \myexpect_{p(\vw \mid \vy)}[\nabla^2_{f_i f_i}\log p(y_i \mid f_i)] ,
\end{equation}
%
The parameters $(\alpha_i, \beta_i)$ are the dual parameters that arise in the dual formulations used in, for example, support vector machines \citep{cortes1995support}, whose origins are found in work by \citet{kimeldorf1971some}. The above relations holds for generic likelihoods, and no approximations are involved as the expectation is under the exact posterior. The question is how do we form the dual variables without access to the exact posterior and different approximate inference techniques solve the question in different ways. Bound techniques such as \citet{jaakkola1997variational} attempt to bound the integral thus making the integral tractable, while Expecation Propogation (EP) as used in \citet{csato2002sparse} iteratively finds dual variables but sets them to log of the expectation of the approximate posterior. Meanwhile variational inference  \cite{khan2017conjugate, adam2021dual} find them using the variational approximation of the posterior to solve the integral.

\paragraph{Neural network dual variables}
As we are using the MAP solution of the neural network loss, we will use the Laplace approximation for the dual variables but other techniques could be used. As detailed in \cite{wilkinson2023bayes, rasmussen2006gaussian}, the dual variables of the Laplace approximation are the same as removing the posterior averaging from \cref{eq:dual_param}, meaning:
\begin{equation}
\label{eq:dual_param_laplace}
\hat{\alpha}_i \coloneqq \nabla_{f_i}\log p(y_i \mid f_i),  \quad \text{and} \quad
\hat{\beta}_i \coloneqq - \nabla^2_{f_i f_i}\log p(y_i \mid f_i).
\end{equation}

Replacing \cref{eq:dual_param_laplace} in to \cref{eq-laplace-approx-function-space} and we arrive at our GP model based on the converged neural network. Again this is similar to what was arrived at for \citet{immerImprovingPredictionsBayesian2021} for the posterior variance function, but they instead use the $f_{\vw^*}(\vx)$ for the posterior mean and fail to spot the significance of the dual variables. The problem with \cref{eq:gp_pred} is that to make predictions and compute variances we must incur a cost of $O(n^3)$ which limits the use of the GP on large data sets.


\subsection{Sparse dual Gaussian process} 
\label{sec:sparse-dual-gp}
%
Sparse Gaussian processes are a way to reduce the computational complexity of a full GP using a low-rank approximation. Many sparsifying approaches exist, we refer the reader to \cite{quinonero2005unifying} for an overview. Given that we have our dual variables derived from our neural network predictions and kernel function, we could use any approximation. We decide to follow the sparsifying approach of \citet{titsias2009variational} and also used in the DITC approximaton \cite{quinonero2005unifying}, which is to define the marginal distribution as $q_{\vu}(\vf)  = \int p(\vf  \mid \vu) q(\vu)  d\vu$. Given that we have $p(f_i  \mid \vu)$ determined by our GP prior, the goal is to find a $q(\vu)$. As was shown in \cite{adam2021dual}, the posterior under this model has a particular structure similar to \cref{eq:gp_pred}. In the paper, they spot the dual variables' importance for approximate inference but write them in the natural parameter form as this form is more suitable for optimization through natural gradients. In order to make the link to the dual variables defined in \cref{eq:dual_param} we write them in the sparse GP in the dual variable form. 

\begin{equation} 
	\valpha_{\vu}  =  \sum_{i \in \mathcal{D}_{n}}  \vkzi \, \hat{\alpha}_{i}, \quad
	\vbeta_{\vu} =  \sum_{i \in \mathcal{D}_{n}} \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T} ,    
\label{eq:dual_sparse}
\end{equation}
Using the sparse definition of the dual variables our sparse GP posterior takes the following form:
\begin{equation}\label{eq:dual_sparse_post}
   \myexpect_{q_{\vu}(\vf)}[f_i] = \vkzs^{\T} \MKzz^{-1}   \valpha_{\vu} , \quad 
   \textrm{Var}_{q_{\vu}(\vf)}[f_i]  = \kappa_{ii} - \vkzs^\top [\MKzz^{-1} - (\MKzz + \vbeta_{\vu})^{-1} ]\vkzs
\end{equation}
where $\MKzz$ and $\vkzs$ are define similarly to $\MKxx$ and $\vk_{\vx i}$ but now over the $\MZ \in \R^{M \times D} $ inducing points instead of the full dataset $\MX$. The key quantities we need to make predictions from our sparse GP from the converged neural network are $(\hat{\alpha}_i, \hat{\beta}_i)$ and a kernel function $\kappa$. Contrasting \cref{eq:dual_sparse_post} and \cref{eq:gp_pred} we can see that the computational complexity is now $\mathcal{O}(m^3)$ to $\mathcal{O}(n^3)$ where $m \ll n$.  Crucially given the structure of our probabilistic model, our sparse dual variables \cref{eq:dual_sparse} are a compact form of the full model projected using the kernel. Our approach contrasts to \citet{immerImprovingPredictionsBayesian2021}, which uses a subset of the full model, a less principled approach to build a sparse model given it ignores the information in the dataset. It is important to highlight that the sparsifying approach is independent of the approximate inference technique. The \cref{eq:dual_param} are determined from the neural network. More complicated inference techniques, such as variational inference, could be used, but given its simplicity, we only need a trained neural network; we use a Laplace approximation in our experiments.




\section{Sparse Function-space Sequential Learning}

We have presented a method for converting any trained neural network into a sparse function-space representation.
In this section, we demonstrate how sequential learning methods can benefit from our function-space representation and it's associated uncertainty estimates.


\subsection{Continual Learning}
In Continal Learning,

\begin{equation}
 	\bar{\MB}^{-1}_s = \MKzz^{-1} \vbeta_\vu \MKzz^{-1} \in \R^{m \times m}, \quad \forall s \in [1, t-1]
 \end{equation}

\begin{equation}
	\mathcal{R_\textit{SFR}}(\mathbf{w}) = \sum_{s=1}^{t-1}\left[\left(f_{\vw}(\MZ_{s}) - f_{\vw_{s}}(\MZ_s) \right)^\T \bar{\MB}^{-1}_{s} \left(f_{\vw}(\MZ_{s}) - f_{\vw_{s}}(\MZ_s) \right) \right]
\end{equation} 
with $f_\vw(\MZ_{s}), f_{\vw_s}(\MZ_s) \in \R^m$ are vectors of the outputs of the neural 


\subsection{Reinforcement learning}
A key challenge in RL is to balance the trade-off between exploration and exploitation.
That is, should the agent select an action that it knows will lead to a high reward (exploitation), or should it
hope to discover new actions which lead to high reward (exploration) by selecting actions that it has not selected before.
In this section, we detail how our method's principled uncertainty estimates can be used to balance this trade-off in a model-based RL algorithim.

We consider stochastic environments with states \(\state \in \stateDomain \subseteq \R^{D_{\state}} \),
actions \(\action \in \actionDomain \subseteq \R^{D_{\action}}\) and transition dynamics
\(\transitionFn: \stateDomain \times \actionDomain \rightarrow \stateDomain \), such that
$\state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t}$, where  $\noise_{t}$
is i.i.d. transition noise.
We consider the episodic setting where the system is reset to an initial state $\state_{0}$ at each episode and we
assume that there is a known reward function $r : \stateDomain \times \actionDomain \rightarrow \R$.
% Following the Markov decision process formulation \cite{bellmanMarkovianDecisionProcess1957a}, we
% denote the states \(\state \in \stateDomain \subseteq \R^{D_{\state}} \) and actions \(\action \in \actionDomain \subseteq \R^{D_{\action}}\)
% of the sytem, the reward function $r : \stateDomain \times \actionDomain \rightarrow \R$, and transition dynamics
% \(\transitionFn: \stateDomain \times \actionDomain \rightarrow \stateDomain \), such that
% $\state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t}$ where  $\noise_{t}$
% is i.i.d. transition noise.
% $\mathbb{E}_{\noise_{0:\infty}} \big[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \big]$
% \begin{align} \label{eq-model-free-objective}
% \policy^{*} = \arg \max_{\policy \in \policyDomain} J(\transitionFn, \policy) = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\noise_{0:\infty}} \bigg[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \bigg]
% \quad \text{s.t. } \state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t},
% \end{align}
The goal of RL is to find the policy \(\pi : \stateDomain \rightarrow \actionDomain\)
(from a set of policies $\Pi$) that maximises the sum of discounted rewards
in expectation over the transition noise,
\begin{align} \label{eq-model-free-objective}
J(\transitionFn, \policy) = \mathbb{E}_{\noise_{0:\infty}} \bigg[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \bigg]
\quad \text{s.t. } \state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t},
\end{align}
where $\gamma \in [0, 1)$ is a discount factor.
In this work, we consider model-based RL where a model of the transition dynamics is learned \(f_{\mathbf{w}} \approx \transitionFn\) and then used by a planning algorithm.
A simple approach is to use the learned dynamic model $f_{\mathbf{w}^{*}}$ and maximise the objective in \cref{eq-model-free-objective},
\begin{align} \label{eq-greedy}
  \policy^{\text{Greedy}}(\state) &= \arg \max_{\pi \in \Pi} J(f_{\mathbf{w}^{*}}, \pi).
\end{align}
However, we can leverage the method in \cref{sec:methods} to obtain a function-space posterior over the learned dynamics $q_{\mathbf{u}}(\hat{\transitionFn} \mid \dataset)$,
where $\mathcal{D}$ represents the state transition data set \(\mathcal{D} = \{(s_{i},a_{i}), s_{i+1}\}_{i=0}^{N}\).
Importantly, the uncertainty represented by this posterior distribution can be used to balance the exploration-exploitation trade-off,
using approaches such as posterior sampling \cite{osbandWhyPosteriorSampling2017,osbandMoreEfficientReinforcement2013},
\begin{align} \label{eq-posterior-sampling}
  \policy^{\text{PS}}(\state) &= \arg \max_{\pi \in \Pi} J(\hat{f}, \pi)
\quad \text{s.t. } \tilde{\transitionFn} \sim q_{\mathbf{u}}(\hat{\transitionFn} \mid \dataset),
\end{align}
where a function $\tilde{\transitionFn}$ is sampled from the (approximate) posterior $q_{\mathbf{u}}(\hat{\transitionFn} \mid \dataset)$ and used to find a policy as
in \cref{eq-greedy}.
Intuitively, this strategy will explore where the model has high uncertainty, which in turn will reduce the model's uncertainty as data is collected and used to
train the model.
See \cref{sec-rl-appendix} for more details of how we use

\textbf{Pathwise conditioning in function-space??}
\todo{could try to implement this}




% We start by obtaining a posterior over the weights of a trained neural network via the Laplace approximation.
% We then show how we can obtain a function-space posterior, (i.e. a GP posterior)
% by linearising the neural network around the weights Maximum a Posteriori (MAP) estimate.
% We then show that by linearising the neural network we can obtain a function-space posterior, i.e. a GP posterior.
% We paraterise our single-step dynamic model $\transitionFnWithParams : \inputDomain \rightarrow \outputDomain$
% as an $L\text{-layer}$ NN with weights $\weights$.

%A Gaussian process (GP) is a distribution over real-valued functions $f(\cdot): \mathcal{X} \rightarrow \R$ defined over $\mathcal{X}$.
%Formally, a GP is defined as,
%\begin{align}
%\label{eq-gp-prior}
%  f(\cdot) \sim \mathcal{N}\left( \mu(\cdot), k(\cdot,\cdot') \right) \quad
%  \mu(\cdot): \mathcal{X} \rightarrow \R \quad
%  k(\cdot,\cdot'): \mathcal{X} \times \mathcal{X} \rightarrow \R
%\end{align}
%where $\mu(\cdot)$ denotes a mean function and $k(\cdot,\cdot')$ a positive definite covariance function, also known as a kernel.
%Given a data set $\dataset = \{\mathbf{x}_{n} \in \inputDomain, \mathbf{y}_{n} \in \outputDomain\}_{n=0}^{N}$,
%we can obtain the GP posterior via multivariate normal conditioning,
%\begin{align}
%\label{eq-gp-predictive-posterior}
%  p(f(\mathbf{x}_{*}) \mid \mathbf{y}) &= \mathcal{N}
%  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{x}\mathbf{x}}^{-1} \mathbf{A}^{T} \right) \\
%  \left( f(\mathbf{x}_{*}) \mid \mu(\mathbf{x}_{*}) + \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} (\mathbf{y} - \bm\mu_{\mathbf{X}})),
%  k_{**} - \mathbf{k}_{*\mathbf{x}} \left(\mathbf{K}_{\mathbf{x}\mathbf{x}} \right)^{-1} \mathbf{k}_{*\mathbf{x}}^{T} \right)
%\end{align}
%% Our GP posterior is then given by,
%\todo{add mean func to GP posterior if we're using one}
%\begin{align}
%\label{eq-gp-predictive-posterior}
%  p(f(\mathbf{x}_{*}) \mid \mathbf{y}) &= \mathcal{N}
%  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{x}\mathbf{x}}^{-1} \mathbf{A}^{T} \right) \\
%  \left( f(\mathbf{x}_{*}) \mid \mu(\mathbf{x}_{*}) + \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} \mu_{\mathbf{x}},
%  k_{**} - \mathbf{k}_{*\mathbf{x}} \left(\mathbf{K}_{\mathbf{x}\mathbf{x}} + \bm\Lambda(\mathbf{y} ; \mathbf{f})^{-1} \right)^{-1} \mathbf{k}_{*\mathbf{x}}^{T} \right)
%\end{align}
%where $\bm\Lambda(\mathbf{y} ; \mathbf{f}) = \nabla^{2}_{\mathbf{f} \mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$ can be viewed as per-input noise. \todo{I'm unsure about this per input noise bit. Check with paul.}
%\todo{what about regression/classification likelhoods}
%This GP predictive posterior is computationally expensive as it requires inverting an $N\times N$ matrix.
%
%\textbf{Sparse Gaussian processes}
%% The GP predictive posterior in \cref{eq-gp-predictive-posterior} is computationally expensive as it requires inverting an $N\times N$ matrix.
%A practical and computationally appealing alternative is to augment the joint probability space with pseudo inputs $\mathbf{Z} \in \R^{M \times D}$
%and corresponding outputs
%$\mathbf{u} = f(\mathbf{Z}; \weights) \in \R^{M \times C}$, known as inducing variables, where $M \ll N$.
%\todo{cite sparse gp papers}
%Instead of collapsing the inducing variables like \cite{titsiasVariational2009}, they can be treated variationally
%$p(\mathbf{u} \mid \mathbf{y}) \approx q(\mathbf{u}) = \mathcal{N}\left( \mathbf{u} \mid \mathbf{m}, \mathbf{V} \right)$, where $\mathbf{m}$ and $\mathbf{V}$ are variational parameters
%which need to be optimised \citep{hensmanGaussian2013}.
%The sparse variational GP (SVGP) predictive posterior is then given by,
%\begin{align}
%\label{eq-dual-svgp-predictive-posterior}
%  p(f(\mathbf{x}_{*}) \mid \mathbf{y})
%&\approx \mathcal{N} \left( f(\mathbf{x}_{*}) \mid \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{m}^{*},
%  k_{**} - \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\mathbf{z}}^{T}
%  % \mathbf{k}_{*\inducingInput} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\inducingInput}^{T}
%  + \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}}  \mathbf{V}^{*}  \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\mathbf{z}}^{T}
%  \right)
%\coloneqq q_{\inducingVariable}(f(\mathbf{x}_{*}))
%\end{align}
%
%
%\textbf{Deep learning}
%Given the data set $\dataset = \{\mathbf{x}_{n} \in \inputDomain, \mathbf{y}_{n} \in \outputDomain\}_{n=0}^{N}$,
%the goal of (supervised) deep learning, is to train the weights $\weights \in \R^{P}$ of an $L\text{-layer}$ NN
%$f : \inputDomain \rightarrow \outputDomain$ to minimize the (regularized) empirical risk,
%\begin{align} \label{eq-empirical-risk}
%  \weights^{*} = \arg \min_{\weights \in \R^{D}} \mathcal{L}(\dataset;\weights) = \arg \min_{\weights \in \R^{D}} \left(
%  \sum_{n=0}^{N-1} l(f(\mathbf{x}_{n} ; \weights), \mathbf{y}_{n}) + R(\weights)  \right).
%\end{align}
%In practice, it is common to use the mean squared error (MSE) loss
%$l(f(\mathbf{x}_{n} ; \weights),\mathbf{y}_{n}) = \|f(\mathbf{x}_{n} ; \weights) - \mathbf{y}_{n} \|^{2}_{2}$
%and to use the weight decay regularizer $R(\weights)=\frac{1}{2}\gamma^{-2}\|\weights\|^{2}_{2}$.
%However, the practioner is free to choose any loss and regularizer. \todo{I think?}
%% \textbf{Bayesian neural networks}
%% \textbf{Weight space to function space}
%From a Bayesian perspective, we can interpret the terms in \cref{eq-empirical-risk} as the log-prior and the i.i.d. log likelihood, i.e.,
%\begin{align} \label{eq-log-prior}
%  R(\weights) &= \underbrace{\log p(\weights)}_{\text{log prior}} = \log \mathcal{N}(\weights \mid 0, \gamma^{2} \mathbf{I}) \\
%  l(f(\mathbf{x}_{n} ; \weights), \mathbf{y}_{n}) &= \underbrace{\log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}; \weights))}_{\text{log likelihood}}
%  = \log \mathcal{N} \left( \mathbf{y}_{n} \mid f(\mathbf{x}_{n}; \weights), \mathbf{I} \right) \label{eq-log-likelihood}
%\end{align}
%
%\textbf{NN to GP}
%Given this interpretation, we can represent the weight space prior from \cref{eq-log-prior}, in
%function space, by linearising the BNN around $\weights^{*}$ and interpretting it as a GP.
%The GP prior is then given by,
%\begin{align} \label{eq-laplace-approx-function-space}
%  % \transitionFn_{\weights_{i}}(\cdot) \sim \mathcal{N} \left( \mu_{\weights_{i}^{*}}(\cdot), K_{\weights_{i}^{*}}(\cdot, \cdot') \right) \qquad
%  f(\cdot ;\weights) \sim \mathcal{N} \left( \mu(\cdot), k(\cdot, \cdot') \right) \quad
%  % \mu_{\weights_{i}^{*}}(\cdot)
%  \mu(\cdot)
%  = 0 \quad
%  % = f(\cdot ;\weights^{*}) \quad
%  % K_{\weights_{i}^{*}}(\cdot, \cdot')
%  k(\cdot, \cdot')
%  = \frac{1}{\gamma^{2}} \mathbf{J}(\cdot) \mathbf{J}(\cdot')^{T},
%\end{align}
%where the kernel is the neural tangent kernel (NTK) \citep{immerImprovingPredictionsBayesian2021},
%\todo{what mean function shoudl this use???}
%\todo{NTK times some factor??}
%with Jacobian given by $\mathbf{J}(\cdot) = \odv{\transitionFn(\cdot; \weights)}{\weights}_{\weights=\weights^{*}}$.
%% The kernel in \cref{eq-laplace-approx-function-space} is the neural tangent kernel (NTK)
%% $k(\mathbf{x}, \mathbf{x}') = \sigma_{0}^{2} J_{\weights^{*}}(\mathbf{x}) J_{\weights^{*}}(\mathbf{x}')^{T}$
%% \citep{immerImprovingPredictionsBayesian2021}
%% to formulate the GP posterior $p(f(\mathbf{X} ;\weights_{0:i}) \mid \dataset_{0:i})$.
%Given this GP prior, we can use \cref{eq-gp-predictive-posterior} to obtain our GP posterior by conditioning on the data.
%% Given our GP prior, we can use the properties of multivariate normals to obtain our GP posterior by conditioning on the data.
%% where $\mathbf{A} = \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}}$
%In contrast to conventional GP kernels, the NTK does not have any hyperparameters which need to be learned.
%Intuitively, we can view the optimisation in \cref{eq-empirical-risk} as learning our kernel.
%As a result, our NTK may be highly non stationary as it is dependent on the NN architecture, for example, the activation functions.
%It is worth noting that the NTK linearises the network around the optimised parameters $\weights^{*}$,
%so the function space prior (and thus posterior) is only a locally linear approximation.
%
%We now have a method to obtain a GP posterior from a trained DNN.
%% In contrast to other approaches, our formulation has not restricted us to using a weight-space posterior obtained from the Laplace approximation.
%In contrast to other approaches, our formulation has not restricted us to using a weight-space posterior from a BNN.
%However, in practice, the GP posterior is not very useful as it requires inverting an $N\times N$ matrix which has complexity $\mathcal{O}(N^{3})$.
%The SVGP in \cref{eq-dual-svgp-predictive-posterior} offers a solution to this problem but it is not immediately clear how to set $\mathbf{m}$ and $\mathbf{V}$.
%In the next section, we show how we can directly obtain an SVGP posterior (i.e. set $\mathbf{m}$ and $\mathbf{V}$) directly from both
%1) a trained NN and 2) a BNN weight-space posterior.
%% $\mathbf{m}$ and $\mathbf{V}$
%% However, it is not obvious how we can use a SVGP to set $\mathbf{m}$ and $\mathbf{V}$
%
%
%
%% In contrast, to previous work, this approach did not require us Laplace approximation. we will not restrict our
%% It is worth noting that the NTK linearises the network around the optimised parameters $\weights^{*}$,
%% so the function space prior (and thus posterior) is only a locally linear approximation.
%% In contrast to conventional GP kernels, the NTK does not have any hyperparameters which need to be learned.
%% It is also worth noting that the NTK may be highly non stationary and is dependent on the NN architecture, for example, the activation functions.
%
%
%\section{Methods}
%In the next section we show how we can obtain a SVGP directly from 1) a trained NN and 2) a BNN posterior.
%
%We follow \cite{csatoSparseOnlineGaussian2002} and express our GP posterior in the dual parameter space.
%However, we consider the variational formulation introduced by \cite{adamDualParameterizationSparse2021}, where the optimal variational parameters are given by,
%\begin{align} \label{eq-dual-params}
%\mathbf{m}^{*} = \mathbf{V}^{*}\bm\alpha^{*} \quad \mathbf{V}^{*} = [\mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} + \bm\beta^{*}]
%\end{align}
%Importantly, we can calculate the natural parameters $(\bm\alpha, \bm\beta)$ in closed form,
%\todo{paul needs to update \cref{eq-dual-svgp-params}}
%\begin{align} \label{eq-dual-svgp-params}
%\bm\alpha_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right] \\
%\bm\beta_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right].
%\end{align}
%This simplifies SVGP inference \citep{hensmanGaussian2013} as it no longer requires the variational parameters $(\mathbf{m}, \mathbf{V})$ to be optimised.
%
%% Importantly, \cite{changFantasizingDualGPs2022} show that in the dual space,
%% conditioning on new observations reduces to suming the dual variables from the previous time
%% step with an update, given by,
%% \todo{pick best way to show new data (using superscript new or just time indexing?)}
%% \begin{align} \label{eq-dual-update-svgp}
%% \bm\alpha_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right] \\
%% \bm\beta_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right]
%% \end{align}
%
%% We follow \cite{csatoSparseOnlineGaussian2002} and express our GP posterior in the dual parameter space,
%% % to write GP posterior $p(f(\mathbf{X} ;\weights_{0:i}) \mid \dataset_{0:i})$ as,
%% \begin{align} \label{eq-gp-post}
%%   \mathbb{E}_{p(f(\mathbf{x}_{*}) \mid \mathbf{y}_{n})} \left[ f(\mathbf{x}_{*}) \right] &= \mu(\mathbf{x}_{*}) + \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%%   \mathbb{V}_{p(f(\mathbf{x}_{*}) \mid \mathbf{y}_{n})} \left[ f(\mathbf{x}_{*}) \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \text{diag}(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
%% \end{align}
%% % \begin{align} \label{eq-laplace-approx-function-space}
%% %   \mathbb{E}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%% %   \mathbb{V}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \diag(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
%% % \end{align}
%% where the dual parameters $(\bm\alpha_n, \bm\beta_n)$ are vectors of,
%% \begin{align} \label{eq-gp-dual-params}
%%   \bm\alpha_{n} \coloneqq \left\{ \mathbb{E}_{p(f(\mathbf{x}_{i}) \mid \mathbf{y}_{n})} \left[ \nabla_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \right\}_{i=1}^{N} \qquad
%%   \bm\beta_{n} \coloneqq \left\{ \mathbb{E}_{p(f(\mathbf{x}_{i}) \mid \mathbf{y}_{n})} \left[ \nabla^{2}_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \right\}_{i=1}^{N}
%% \end{align}
%% Importantly, this parameterisation
%
%% \textbf{dual-SVGP}
%% \begin{align} \label{eq-svgp-post}
%%   \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ f(\mathbf{x}_{*}) \right] &= \mu(\mathbf{x}_{*}) + \mathbf{k}_{\mathbf{z}*}^{T} \bm\alpha_{\mathbf{u}} \\
%%   \mathbb{V}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ f(\mathbf{x}_{*}) \right] &= k_{**} - \mathbf{k}_{\mathbf{z}*}^{T} \left[ \mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} - \left( \mathbf{K}_{\mathbf{z} \mathbf{z}} + \bm\beta_{\mathbf{u}} \right)^{-1} \right] \mathbf{k}_{\mathbf{z}*}
%% \end{align}
%% % \begin{align} \label{eq-laplace-approx-function-space}
%% %   \mathbb{E}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%% %   \mathbb{V}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \diag(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
%% % \end{align}
%% where the dual parameters $(\bm\alpha_{\mathbf{u}}, \bm\beta_{\mathbf{u}})$ are vectors of,
%% \begin{align} \label{eq-svgp-dual-params}
%%   \bm\alpha_{\mathbf{u}} \coloneqq \mathbf{k}_{\mathbf{z}i} \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ \nabla_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \qquad
%%   \bm\beta_{\mathbf{u}} \coloneqq \mathbf{k}_{\mathbf{z}i} \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ \nabla^{2}_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \mathbf{k}_{\mathbf{z}i}^{T}
%% \end{align}
%
%% weight space posterior in \cref{eq-laplace-approx-weight-space} in
%% function space by linearising the BNN and interpreting it as a GP.
%% \begin{align} \label{eq-laplace-approx-function-space}
%%   \transitionFn_{\weights_{i}}(\cdot) &\sim \mathcal{N} \left( \mu_{\weights_{i}^{*}}(\cdot), K_{\weights_{i}^{*}}(\cdot, \cdot') \right) \quad
%%   \mu_{\weights_{i}^{*}}(\cdot) &= \transitionFn_{\weights^{*}_{i}}(\cdot) \quad
%%   K_{\weights_{i}^{*}}(\cdot, \cdot') &= \mathbf{J}_{\weights^{*}_{i}}(\cdot) \bm\Sigma_{\weights^{*}_{i}} \mathbf{J}_{\weights^{*}_{i}}(\cdot')^{T}
%% \end{align}
%% where the Jacobian is given by $\mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}$.
%
%% \begin{align} \label{eq-laplace-approx-jacobian}
%% \mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}
%% \end{align}
%
%
%% % Note that the posterior in \cref{eq-laplace-approx-weight-space} is in weight space.
%% We can represent the weight space posterior in \cref{eq-laplace-approx-weight-space} in
%% function space by linearising the BNN and interpreting it as a GP.
%% \begin{align} \label{eq-laplace-approx-function-space}
%%   \transitionFn_{\weights_{i}^{*}}(\cdot) &\sim \mathcal{N} \left( \mu_{\weights_{i}^{*}}(\cdot), K_{\weights_{i}^{*}}(\cdot, \cdot') \right) \quad
%%   \mu_{\weights_{i}^{*}}(\cdot) &= \transitionFn_{\weights^{*}_{i}}(\cdot) \quad
%%   K_{\weights_{i}^{*}}(\cdot, \cdot') &= \mathbf{J}_{\weights^{*}_{i}}(\cdot) \bm\Sigma_{\weights^{*}_{i}} \mathbf{J}_{\weights^{*}_{i}}(\cdot')^{T}
%% \end{align}
%% where the Jacobian is given by $\mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}$.
%
%% \begin{align} \label{eq-laplace-approx-jacobian}
%% \mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}
%% \end{align}
%
%
%\subsection{Fast Updates}
%Importantly, \cite{changFantasizingDualGPs2022} show that in the dual space,
%conditioning on new observations reduces to suming the dual variables from the previous time
%step with an update, given by,
%\todo{pick best way to show new data (using superscript new or just time indexing?)}
%\begin{align} \label{eq-dual-update-svgp}
%\dualParam{1}^{t+1} &\leftarrow \dualParam{1}^{t} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))} \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right] \\
%\dualParam{2}^{t+1} &\leftarrow \dualParam{2}^{t} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))}  \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right]
%\end{align}
%Importantly, for a single new observation $((\state_{t}, \action_{t}), \state_{t+1})$ this update has
%complexity $\mathcal{O}(\numInducing^{2})$.
%This is a significant improvement to naive GP conditioning, which has complexity $\mathcal{O}((\numDataNew + \numDataOld)^{3})$
%and sparse GP conditioning, which has complexity $\mathcal{O}((\numDataNew + \numDataOld)\numInducing^{2})$.
%\todo{double check these complexities are right and cite them/show equaitons}
%It is worth highlighting that the complexity of \cite{changFantasizingDualGPs2022} update does not increase during an episode.
%
%\subsubsection{Model-based RL}
%
%\subsubsection{Efficiently Sampling Functions for Posterior Sampling}
%\cite{wilsonEfficiently2020}
%\cite{wilsonPathwise2021}
%
%% \begin{align} \label{eq-dual-update-svgp}
%%  \dualParam{1}^{\text{new}} &\leftarrow \dualParam{1}^{\text{old}} +
%%   \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}))}
%%  \left[ \log p(\state_{t+1}^{\text{new}} \mid \latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}) ) \right] \\
%%  \dualParam{2}^{\text{new}} &\leftarrow \dualParam{2}^{\text{old}} +
%%   \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}))}
%%  \left[ \log p(\state_t^{\horizon+1} \mid \latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}) ) \right] \\
%%  \dualParam{1}^{\horizon+1} &\leftarrow \dualParam{1}^{\horizon} +
%%   \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
%%  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right] \\
%%  \dualParam{2}^{\horizon+1} &\leftarrow \dualParam{2}^{\horizon} +
%%   \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
%%  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right]
%% \end{align}
%
%
%\begin{assumption} \label{assumption-ntk-linearisation}
%  Something about NTK being a linearisation around $\theta^{*}_{i}$ but each update moves away from $\theta^{*}_{i}$
%\end{assumption}
%
%\cite{rossellApproximateLaplaceApproximations2021}



\section{Experiments}
\label{sec:experiments}

\todo{Overview of experiments}

Our experiments seek to answer the following questions:
\begin{itemize}
  \item Does NN2SFR caputure good uncertainty estimates
  \item Can NN2SFR be used to build a summary of the current neural network to mitigate catastrophic forgetting over previous tasks?
\end{itemize}

\subsection{Capturing uncertainty in UCI tasks under supervised learning}
%


\begin{table}[t!] 
  \centering\scriptsize
  \caption{Negative log predictive density (NLPD) (lower better) for the proposed model TODO} 
	\label{tbl:uci}
	% Control table spacing
	\renewcommand{\arraystretch}{1.}
	\setlength{\tabcolsep}{2pt}
	\setlength{\tblw}{0.14\textwidth}  
	
	% Custom error formatting
	\newcommand{\val}[2]{%
		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
	} 

    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
	\input{tables/uci.tex}
\end{table}




\subsection{Supervised learning on image data sets}


\begin{table}[t!] 
  \centering\scriptsize
  \caption{Metrics for supervised learning with image data. SVGP is our method, and the number inside parentheses is the number of inducing points. SVGP NN is a modification of our method, where the mean is from the NN directly and the variance is from the GP model} 
	\label{tbl:imagesuper}
	% Control table spacing
	\renewcommand{\arraystretch}{1.}
	\setlength{\tabcolsep}{6pt}
	\setlength{\tblw}{0.2\textwidth}  
	
	% Custom error formatting
	\newcommand{\val}[2]{%
		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
	} 

    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
	\input{tables/img_super.tex}
\end{table}
\subsection{Updating the representation during continual learning}

\begin{table}[t!] 
  \centering\scriptsize
  \caption{
  TODO: CL Experiments. $^*$ Methods relying only on weight regularization. 
  \color{blue}{Results from \cite{rudner2022continual}.} 
  %\color{red}{Preliminary results need more runs or hyperparam tuning}
  }
	\label{tbl:cl_table_1}
	% Control table spacing
	\renewcommand{\arraystretch}{1.}
	\setlength{\tabcolsep}{2pt}
	\setlength{\tblw}{0.14\textwidth}  
	
	% Custom error formatting
	\newcommand{\val}[2]{%
		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
	} 
	
	\input{tables/cl_table_1}
\end{table}


\subsection{Reinforcement Learning}

In section \label{sec-rl} we detailed how our method can be used to

See \label{sec-rl-experiments-appendix} for more details of our experiments.



We compare a model-based RL agent using a simple MLP dynamic model with
\begin{itemize}
  \item uncertainty is good for exploration
\end{itemize}

\cref{fig:rl} shows training curves

\begin{figure}[!t]
 \centering
 \begin{subfigure}{.2\textwidth}
 \resizebox{\textwidth}{!}{%
 \begin{tikzpicture}[inner sep=0,outer sep=0]

   % Draw decorated 'ground'
   \draw[postaction={draw, decorate, decoration={border, angle=-45,
					amplitude=1cm, segment length=.5cm}}] (-3,-1.5) -- (12,-1.5);

   % The cart
   \draw[draw=black,fill=black!50,draw=black,line width=3pt,rounded corners=1mm] (0,0) rectangle (9cm,3cm);
   \node[fill=black,circle,minimum size=.5cm] (dot) at (4.5cm,3cm) {};

   % Wheels
   \node[fill=white,draw=black,line width=3pt,circle,minimum size=2cm,,fill=black!50] at (2cm,-.5cm) {};
   \node[fill=white,draw=black,line width=3pt,circle,minimum size=2cm,fill=black!50] at (7cm,-.5cm) {};

   % The arm
   \node[anchor=north,minimum width=1cm,minimum height=14cm,draw=black,rotate=20,rounded corners=5mm,yshift=7mm,xshift=-.3mm,fill=white,draw=black,line width=3pt] at (dot) {};
   \node[fill=black,circle,minimum size=.5cm] at (dot) {};

   % Markings
   \draw[loosely dashed,line width=1pt] (4.5,6) -- (4.5,-10);

   % Arrow
   \draw[->,black,line width=3pt,-{Latex[length=7mm,width=7mm]}] (1,5) --node[above,outer sep=6pt]{\large $x$} (4.5,5);

   \def\centerarc[#1](#2)(#3:#4:#5)% Syntax: [draw options] (center) (initial angle:final angle:radius)
   { \draw[#1] ($(#2)+({#5*cos(#3)},{#5*sin(#3)})$) arc (#3:#4:#5); }

   % Draw arc
   \centerarc[black,line width=3pt](dot)(270:290:11)
   \node at (6.5,-9) {$\theta$};

   \node[white,minimum size=1cm] at (4.5,-13) {};

 \end{tikzpicture}}
 \end{subfigure}
 \hfill
 \begin{subfigure}{.78\textwidth}
   \centering\scriptsize
   \setlength{\figurewidth}{\textwidth}
   \setlength{\figureheight}{.5\figurewidth}
   \pgfplotsset{axis on top}
   \input{./fig/rl.tex}
 \end{subfigure}
 \hfill
 \caption{\textbf{Reinforcement learning experiments} Training curves showing that our model's uncertainty estimates improve sample efficiency in RL.
   Our method (blue/green) converges to the optimal solution in fewer environment steps than the baseline model-based RL method (magenta) and (DDPG) the model-free baseline  (yellow).}
 \label{fig:rl}
\end{figure}

%


% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\textwidth, angle=270]{fig/weight-space-to-functio-space.pdf}
%   \caption{}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\textwidth, trim=0 100 0 10]{fig/cartpole-training-curves.pdf}
%   \caption{}
% \end{figure}

% \begin{table}
%   \caption{Negative test log likelihood (lower is better) on UCI classification tasks (2 hidden layers, 50 tanh). Our SVGP predictive outperforms the GLM predictive. }
% \end{table}

\section{Discussion and Conclusion}
\label{sec:conclusion}







A reference implementation of the methods presented in this paper is currently available as supplementary material and will be made available under the MIT License on GitHub upon acceptance.


%\section*{Broader Impact}

% \section*{References}
%\small
%\printbibliography
%\normalsize
% TODO make bibliography small a better way

%References follow the acknowledgments. Use unnumbered first-level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
%when listing the references.
%Note that the Reference section does not count towards the page limit.
%\medskip


\phantomsection%
\addcontentsline{toc}{section}{References}
\begingroup
\small
\bibliographystyle{abbrvnat}
\bibliography{zotero-library,bibliography}
\endgroup

\clearpage

\nipstitle{
    {\Large Supplementary Material:} \\
    PAPER TITLE GOES HERE}
\pagestyle{empty}

\appendix

\section{Method Details}

Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.

\section{Model-based Reinforcement Learning}
\label{sec-rl-appendix}


Model Predictive Path Integral (MPPI) control
\cite{panSample2015}
\cite{williamsModel2017}
\todo{what is correct citation for MPPI?}

% \cref{alg-mbrl} shows the typical model-based RL loop.
\begin{align} \label{eq-fast-update-mpc}
  \policy_{i+1}^{\text{PS}}(\state) = \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
\E \bigg[ \sum_{t=0}^{H-1} \gamma^{t} r(\state_{t},\action_{t}) \mid \state_{0}=\state  \bigg] + Q_{\theta}(\state_{\Horizon}, \action_{H})
\quad \text{s.t. } \state_{t+1} &= \hat{\transitionFn}(\state_{t}, \action_{t}) + \noise_{t}
\end{align}

with $\hat{\transitionFn} \sim p(\transitionFn \mid \dataset)$

We use deep deterministic policy gradient (DDPG) \cite{lillicrapContinuousControlDeep2016} to learn an action value function $Q_{\theta}$.
Note that we also learn a policy but its sole purpose is for learning the value function.

\subsection{Experiment Configuration}
This section details how we configured and ran our reinforcement learning experiments.

\textbf{Dynamic model}
In all experiments we used an MLP dynamic model with a single hidden layer of width 64 and TanH activation functions.
At each episode we used Adam \cite{adam} to optimise the NN parameters for $5000$ iterations with a learning rate of $0.001$.
We reset the optimizer after each episode.
As we are performing regression we instantiate the loss function in \cref{eq-empirical-risk} as the well-known mean squared error.
This corresponds to a Gaussian likelihood with unit variance.
We then set the prior precision as $\delta=0.0001$.
% It is worth noting that $\delta$ effects both the neur
In all experiments our sparse function-space representation uses $m=128$ inducing points and this seemed to be sufficient.



\textbf{Model predictive path integral (MPPI)}
MPPI is an online planning algorithim which iteratively improves the action trajectory $\action_{t:t+H}$ using samples.
At each iteration $j$, $N=256$ trajectories are sampled according to the currect action trajectory $\action^{j}_{t:t+H}$.
The $K=32$ top trajectories with highest returns $\sum_{h=0}^{H} = r(\state^{j}_{t+h}, \action^{j}_{t+h})$ are selected.
The next action trajectory $\action^{j+1}_{t:t+H}$ is then computed by taking the weighted average of the top $K=32$ trajectories
with weights from the softmax over returns from top $K=32$ trajectories.

$\gamma 0.9$
$\tau=0.005$
horizon $H=5$
temperature $0.5$
momentum $0.1$

\textbf{Initial data set}
We collect an initial data set using a random policy for one episode.

\textbf{DDPG}
action value function is an MLP with a single hidden layer of width $128$ with ELU activation functions.
We train the DDPG agent using Adam for $500$ iterations at each episode, using a learning rate $0.0001$.


\section{Extending the CL regularizer to multi-class settings}
\begin{equation}
	\bar{\MB}^{-1}_s = \MKzz^{-1} \vbeta_\vu \MKzz^{-1} \in \R^{C \times m \times m} \quad \MKzz \in \R^{C \times m \times m} 
\end{equation}

\begin{equation}
	\mathcal{R_\textit{SFR}}(\mathbf{w}) = \sum_{s=1}^{t-1}	\sum_{k \in 	C}\left[\left(f_{\vw, k}(\MZ_{s}) - f_{\vw_{s}, k}(\MZ_s) \right)^\T \bar{\MB}^{-1}_{s} \left(f_{\vw, k}(\MZ_{s}) - f_{\vw_{s, k}}(\MZ_s) \right) \right] 
\end{equation}






%
%\subsection{Model-based Reinforcement Learning (RL)}
%The goal of reinforcement learning is to find a policy \(\pi \in \Pi\) that maximises the sum of discounted
%rewards in expecation under the transition noise (aleatoric uncertainty),
%\begin{align} \label{eq-model-free-objective}
%\policy^{*} = \arg \max_{\policy \in \policyDomain} J(\transitionFn, \policy) = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\noise_{0:\infty}} \left[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \right],
%\end{align}
%where $\gamma \in [0, 1]$
%
%\textbf{Model-based}
%In Bayesian model-based RL, we obtain the posterior over the dynamics \(p(f\mid\mathcal{D})\) after performing (approximate) Bayesian
%inference given a state transition data set \(\mathcal{D} = \{\{(s_{t},a_{t}), s_{t+1}\}^{T_{i}}_{t=1}\}_{i=0}^{N}\).
%\cref{alg-mbrl} shows the typical model-based RL loop.
%Importantly, the dynamics are usually only updated after an episode $i$.
%
%\begin{algorithm}[!b]
%\caption{Model-based RL}\label{alg-mbrl}
%\begin{algorithmic}[1]
%  \Require Start state $\state_{0}$, initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%\For{$i  \in \{1, 2, \ldots, \text{num episodes} \}$}
%    \State Reset the system to $\state_{0}$ and reset trajectory buffers $\bm\tau_{t} = \emptyset \ \forall t$
%    \For{$t  \in \{1, 2, \ldots, \text{num steps} \}$}
%      % \State Collect  $\tau_{0:t} = \tau_{0:t-1} \cup (\state_{j}, \action_{j}, \state_{j+1}, r_{j+1})$
%      \State Use \cref{eq-greedy}/\cref{eq-posterior-sampling}/\cref{eq-ucrl} to collect data $\bm\tau_{t} = \bm\tau_{t-1} \cup (\state_{t}, \policy_{i}(\state_{t}), \transitionFn(\state_{t}, \policy_{i}(\state_{t})), r_{t+1})$
%      % \State Execute policy $\policy_{i}(\state_{t})$ in environment and update trajectory $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%    \EndFor
%    \State Update data set $\dataset_{0:i} = \dataset_{0:i-1} \cup \tau$
%    \State Train dynamics $p(\transitionFn \mid \dataset_{0:i}) \leftarrow \text{update\_dynamics}(\dataset_{0:i}, p(\transitionFn \mid \dataset_{0:i-1}))$
%    % \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$ using $\dataset_{0:i+1}$
%    % \State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%    \State Improve policy $\pi_{i+1} \leftarrow \text{update\_policy}(p\left(\transitionFn \mid \dataset_{0:i}), \dataset_{0:i} \right)$
%    %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%\EndFor
%\end{algorithmic}
%\end{algorithm}
%
%% \begin{minipage}{0.499\textwidth}
%% \begin{algorithm}[H]
%% \caption{Model-based RL}\label{alg-mbrl}
%% \begin{algorithmic}[1]
%%   \Require Initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%% \For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
%%     \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
%%       \State Execute policy $\policy_{i}(\state_{t})$ in environment
%%       \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%%     \EndFor
%%     \State Update data set $\mathcal{D}_{0:i+1} = \mathcal{D}_{0:i} \cup \tau_{i+1}$
%%     \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$
%%     \State Improve policy $\pi_{i+1}$
%%     %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%% \EndFor
%% \end{algorithmic}
%% \end{algorithm}
%% \end{minipage}
%% \hfill
%% \begin{minipage}{0.499\textwidth}
%% \begin{algorithm}[H]
%% \caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
%% \begin{algorithmic}[1]
%%   \Require Initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%%     % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
%% \For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
%%     \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
%%       \State Execute policy $\policy_{i}(\state_{t})$ in environment
%%       % \State Append transition $\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$ to trajectory $\tau_{i}$
%%       \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%%       \State {\color{blue}Update dynamics $p(\transitionFn \mid \dataset_{0:i} \cup \tau_{i+1})$}
%%     \EndFor
%%     \State Update data set $\mathcal{D}_{0:i+1} = \mathcal{D}_{0:i} \cup \tau_{i+1}$
%%     \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$
%%     \State Improve policy $\pi_{i+1}$
%%     %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%% \EndFor
%% \end{algorithmic}
%% \end{algorithm}
%% \end{minipage}
%
%
%
%\subsection{Exploration Strategies}
%\textbf{Greedy exploitation}
%Given the posterior dynamics \(p(\transitionFn \mid \mathcal{D})\),
%a common approach is to simply take the expecation over both the aleatoric and epistemic uncertainty,
%\begin{align} \label{eq-greedy}
%\policy_{i+1}^{\text{greedy}} = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\transitionFn \sim p(\transitionFn \mid \dataset_{0:i})} \left[ J(\transitionFn, \policy) \right],
%\end{align}
%This approach has been widely adopted, for example, in PILCO, PETS, GP-MPC
%\cite{deisenrothPILCO2011,chuaDeepReinforcementLearning2018,kamtheDataEfficient2018}.
%This approach helps to alleviate model bias as the posterior ``knows what the model does not know''.
%This is because the predictive posterior \(p(f(s_{t},a_{t}) \mid (s_{t},a_{t}),  \mathcal{D} )\) will be (or should be) uncertain when making
%predictions far away from the training data.
%The expectation considers all possible dynamics models which prevents the policy optimisation from
%exploiting innacuracies in the model.
%This approach has no guarantees for exploration in the general case.
%However, under specific dynamics and reward structures (e.g. PILCO) this objective can achieve sublinear regret.
%\todo{need to double check sublinear regret statement. And give a reference}
%
%
%\textbf{Posterior sampling}
%\cite{osbandWhyPosteriorSampling2017,osbandMoreEfficientReinforcement2013}
%\begin{align} \label{eq-posterior-sampling}
%\policy_{i+1}^{\text{PS}} = \arg \max_{\policy \in \policyDomain} \left[ J(\transitionFn, \policy) \right] \quad \text{s.t. } \transitionFn \sim p(\transitionFn \mid \dataset_{0:i})
%\end{align}
%
%\textbf{Hallucinated upper confidence RL}
%A more theoretically grounded exploration strategy is UCRL \citep{jakschNearoptimal2010}, which optimises joinly over
%policies and models inside the set
%\(\mathcal{M} = \{ f \mid | f(s,a) - \mu_{i}(s, a) | \leq \beta_{i} \Sigma_{i}(s, a) \quad \forall s, a \in \mathcal{S} \times \mathcal{A} \}\), representing all statistically plausible
%models under the posterior \(p(f(s,a) \mid \mathcal{D}_{0:i} \cup (s,a)) = \mathcal{N}(f(s,a) \mid \mu_{i}(s,a), \Sigma_{i}(s,a))\) at episode \(i\).
%This strategy is given by,
%\begin{align} \label{eq-ucrl}
%\policy_{i+1}^{\text{UCRL}} = \arg \max_{\policy \in \policyDomain} \max_{\transitionFn \in \mathcal{M}} J(\transitionFn, \policy).
%\end{align}
%This strategy optimises an optimistic policy over the set of plausible dynamics models.
%Although this joint optimisation is intractable in general,
%\cite{curiEfficient2020} proposed a practical alternative which is detailed in \cref{sec-hucrl}.
%
%\textbf{MPC vs policy learning}
%It is worth noting that the strategies in \cref{eq-greedy,eq-posterior-sampling,eq-ucrl} can be used with both model predictive control (MPC)
%techniques, such as the cross entoropy method (CEM), and model-free RL techniques, such as soft actor-critic (SAC).
%
%
%In this work we are interested in how we can use \(p(f \mid \mathcal{D})\) to alleviate some of the issues in model-based RL,
%for example, model bias and the exploration-exploitation trade-off.
%
%
%
%\todo{show how to get new $\dualParam{1}$ and $\dualParam{2}$ in Train dynamics line of \cref{alg-mbrl-fast-updates}}
%\begin{algorithm}[!t]
%\caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
%\begin{algorithmic}[1]
%  \Require Start state $\state_{0}$, initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$ (inc. dual parameters $\dualParam{1}, \dualParam{2}$), policy $\policy_{0}$
%    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
%\For{$i  \in \{1, 2, \ldots, \text{num episodes} \}$}
%    \State Reset the system to $\state_{0}$ and reset trajectory buffers $\bm\tau_{t} = \emptyset \ \forall t$
%    \For{$t  \in \{1, 2, \ldots, \text{num steps} \}$}
%      % \State Execute policy $\policy_{i}(\state_{t})$ (\cref{eq-fast-update-mpc}) in environment
%      \State Use \cref{eq-fast-update-mpc} to collect data $\bm\tau_{t} = \bm\tau_{t-1} \cup (\state_{t}, \policy_{i}(\state_{t}), \transitionFn(\state_{t}, \policy^{\text{fast}}_{i}(\state_{t})), r_{t+1})$
%      % \State Append transition $\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$ to trajectory $\tau_{i}$
%      % \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%      %\State {\color{blue}Update dynamics $p(\transitionFn \mid \dataset_{0:i} \cup \tau_{i+1})$}
%      \State {\color{blue}Update dynamics posterior using \cref{eq-dual-update-svgp}, i.e. fast update}
%      % \begin{align}
%      % \dualParam{1}^{t+1} &\leftarrow \dualParam{1}^{t} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))} \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right] \\
%      % \dualParam{2}^{t+1} &\leftarrow \dualParam{2}^{t} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))}  \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right]
%      % \end{align}}
%    \EndFor
%    \State Update data set $\dataset_{0:i} = \dataset_{0:i-1} \cup \tau$
%    \State Train dynamics $p(\transitionFn \mid \dataset_{0:i}) \leftarrow \text{update\_dynamics}(\dataset_{0:i}, p(\transitionFn \mid \dataset_{0:i-1}))$
%    \State Improve policy $\pi^{\text{fast}}_{i+1} \leftarrow \text{update\_policy}(p\left(\transitionFn \mid \dataset_{0:i}), \dataset_{0:i} \right)$
%\EndFor
%\end{algorithmic}
%\end{algorithm}
%
%
%\subsubsection{Fast updates}
%
%
%In this section we extend these fast updates to environment's with high dimensional state spaces and large data sets.
%% Our method draws on the connection between BNNs and GPs and formulates a function space SVGP posterior given
%Our method uses a BNN dynamic model and draws on the connection between BNNs and GPs
%\citep{khanApproximate2019} to formulate a function space SVGP posterior,
%where we can apply the fast updates from \cref{eq-dual-update-svgp}.
%At a high-level, we first use Laplace's approximation to obtain a weight space posterior for our BNN.
%We then linearise our BNN around the optimal parameters and interpret it as a GP.
%Finally, we formulate a lower rank approximation of this GP posterior (i.e. a SVGP posterior) using inducing variables.
%% formulates a function space SVGP posterior by drawing on the connection between BNNs and GPs.
%
%
%The strategies in \cref{eq-greedy,eq-posterior-sampling,eq-hucrl} do not update the dynamic model during an episode.
%A better approach would be to update the posterior at every time step during an episode, for example,
%\begin{subequations}
%\begin{align} \label{eq-fast-update-mpc}
%  \policy_{i+1}^{\text{greedy}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
%\E_{p(\transitionFn \mid \dataset_{0:i})} \left[J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) \right] + \stateValueFn(\state_{\Horizon+1}) \\
%  \policy_{i+1}^{\text{PS}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
%J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) + \stateValueFn(\state_{\Horizon+1}) \quad \text{s.t. } \transitionFn \sim p(\transitionFn \mid \dataset_{0:i} \cup \bm\tau_{t}) \\
%  \policy_{i+1}^{\text{UCRL}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}} \max_{\transitionFn \in \mathcal{M}}
%J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) + \stateValueFn(\state_{\Horizon+1}) \quad \text{s.t. } \mathcal{M} = \{\transitionFn(\state_{t},\action_{t}) - \mu_{i,t}(\state_{t},\action_{t}) \leq \beta_{i,t} \Sigma_{i,t}(\state_{t}, \action_{t})\} \\
%  \stateValueFn(\state) &= \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right] \label{eq-value-fn}
%\end{align}
%\end{subequations}
%\begin{subequations}
%\begin{align} \label{eq-fast-update-mpc-old}
%  \policy^{\text{fast}}(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}}
%  \mathbb{E}_{\state_{\horizon} \sim p(\state_{\horizon+1} \mid \transitionFn(\state_{\horizon}, \action_{\horizon}))} \left[ \sum_{\horizon=0}^{\Horizon}     \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \mid \state_{0}=\state \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
%  \stateValueFn(\state) &= \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right] \label{eq-fast-update-mpc}
%\end{align}
%\end{subequations}
%
%\begin{align} \label{}
%  \policy(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}} \max_{\optimisticTransition \in \optimisticTransitionSet}
%  \sum_{\horizon=0}^{\Horizon}  \mathbb{E}_{\noise_{\horizon}} \left[  \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
%  \text{s.t. } \state_{\horizon+1} &= \optimisticTransition(\state_{\horizon}, \action_{\horizon}) + \noise_{\horizon} \\
%  \optimisticTransition(\state_{\horizon}, \action_{\horizon}) &=
%\optimisticTransitionMean(\state_{\horizon}, \action_{\horizon}) \pm \beta_{i}
%\optimisticTransitionCov(\state_{\horizon}, \action_{\horizon})
%\end{align}
%
%\input{proof.tex}
%
%
%\section{Hallucinated Upper Confidence Reinforcement Learning (H-UCRL)} \label{sec-hucrl}
%\cite{curiEfficient2020} introduced a tractable approximation which retains some of the theoretical guarantees whilst
%being applicable with deep model-based RL.
%They introduce a function \(\eta: \mathcal{S} \times \mathcal{A} \rightarrow [-1, 1]^{p}\) which acts as a hallucinated control input.
%The strategy is given by,
%\begin{align} \label{eq-hucrl}
%\pi_i^{\text{UCRL}} = \arg \max_{\policy \in \policyDomain} \max_{\eta(\cdot) \in [-1,1]} J(\transitionFn, \policy) \quad \text{s.t.} \quad \transitionFn = \mu_{i}(\state_{t}, \action_{t}) + \beta_{i} \Sigma_{i}(\state_{t}, \action_{t}) \eta(\state_{t},\action_{t}).
%\end{align}
%Intuitively, \(\eta(\state,\action) \in [-1,1]\) enables the optimisation to select any dynamics model
%\(\transitionFn\) within \(\pm \beta \Sigma_{i}(\state_{t}, \action_{t})\) of the posterior mean \(\mu_{i}(\state_{t}, \action_{t})\).
%
%\section{Laplace Approximation} \label{sec-laplace-approximation}
%
%\textbf{Laplace approximation}
%The Laplace approximation \todo{add citation of original paper} constructs a Gaussian approximation of the weight-space posterior $p(\weights \mid \dataset)$
%by using a second-order Taylor expansion of $\mathcal{L}$ around $\weights^{*}$.
%The posterior approximation is given by,
%\begin{align} \label{eq-laplace-approx-weight-space}
%  p(\weights \mid \dataset) \approx \mathcal{N} \left( \weights \mid \weights^{*} , \bm\Sigma_{\weights^{*}} \right)
%  \quad \text{with} \quad \bm\Sigma_{\weights} =
% - \nabla_{\weights \weights}^{2} \mathcal{L} ( \dataset ; \weights)|_{\weights=\weights^{*}}
%\end{align}
%That is, it sets the posterior precision to the Hessian of the loss at the optimal parameters $\weights^{*}$.
%It is worth noting that computing the Hessian of the loss can be computationally intractable for large networks.
%The prior terms are usually trivial, so we focus on the likelihood here.
%The Jacobian and Hessian of the log likelihood can be expressed per data point,
%\begin{align} \label{eq-jac}
%  \nabla_{\weights} \log p(\mathbf{y} \mid \mathbf{f}(\mathbf{x}; \weights)) &= \mathbf{J}(\mathbf{x})^{T} \mathbf{r}(\mathbf{y} ; \mathbf{f}) \\
%  \nabla^{2}_{\weights\weights} \log p(\mathbf{y} \mid \mathbf{f}(\mathbf{x}; \weights)) &= \mathbf{H}(\mathbf{x})^{T}
%  \mathbf{r}(\mathbf{y};\mathbf{f}) - \mathbf{J}(\mathbf{x})^{T} \bm\Lambda(\mathbf{y};\mathbf{f}) \mathbf{J}(\mathbf{x}),
%\label{eq-hess}
%\end{align}
%through the Jacobian $\mathbf{J} \in \R^{C \times P}$ and Hessian $\mathbf{H} \in \R^{C \times P \times P}$ of the feature extractor $\mathbf{f}(\mathbf{x}; \bm\weights)$,
%\begin{align} \label{eq-jac}
%[\mathbf{J}(\cdot)]_{ci} = \odv{f_{c}(\cdot ; \weights)}{\weights_{i}}_{\weights=\weights^{*}} \qquad
%[\mathbf{H}(\cdot)]_{cij} = \odv{f_{c}(\cdot ; \weights)}{\weights_{i}\weights_{j}}_{\weights=\weights^{*}}
%\end{align}
%where
%$\mathbf{r}(\mathbf{y}; \mathbf{f}) = \nabla_{\mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$ can be interpreted as a residual and
%$\bm\Lambda(\mathbf{y} ; \mathbf{f}) = \nabla^{2}_{\mathbf{f} \mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$
%as per-input noise.
%Many approximations exist and can be used alongsied our method.
%We refer the reader to \cite{daxbergerLaplace2021} for more details.
%See \cref{sec-laplace-approximation} for further details on the Laplace approximation.
%
%
%\section{Experiment details}
%
%\subsection{UCI}
%
%\begin{table}[t!] 
%  \centering\scriptsize
%  \caption{Negative log predictive density (NLPD) (lower better) for the proposed model TODO} 
%	\label{tbl:uci_all}
%	% Control table spacing
%	\renewcommand{\arraystretch}{1.}
%	\setlength{\tabcolsep}{2pt}
%	\setlength{\tblw}{0.14\textwidth}  
%	
%	% Custom error formatting
%	\newcommand{\val}[2]{%
%		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
%	} 
%
%    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
%	\input{tables/uci_all.tex}
%\end{table}
%
%\section{Template stuff}
%\subsection{Generate TikZ Figures from Python}
%We can generate figures in \texttt{.tex} format directly from Python:
%\begin{verbatim}
%tikzplotlib.save("fig.tex", axis_width="\\figurewidth", axis_height="\\figureheight")
%\end{verbatim}
%\cref{fig:example} shows that we get nicely formatted lables/titles/etc when we include them in our paper.
%\begin{figure}[h]
%    \centering\footnotesize
%
%    % Set your figure size here
%    \setlength{\figurewidth}{.33\textwidth}
%    \setlength{\figureheight}{.75\figurewidth}
%
%    % Customize your plot here
%    % (scale only axis applies the size to the axis box and not entire figure)
%    \pgfplotsset{grid style={dotted},title={Foo},scale only axis}
%
%    % Use the subcaption package (= subfigure) for sub-plots, that is
%    % plot the separate plots separately in Python
%    \begin{subfigure}{.4\textwidth}
%        \centering
%        \input{./fig/example_fig.tex}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}{.4\textwidth}
%        \centering
%        \input{./fig/example_fig.tex}
%    \end{subfigure}
%    \caption{Foo}
%    \label{fig:example}
%\end{figure}
%
%\subsection{Generate Tables from Python}
%We can also generate tables straight from python using \href{https://github.com/astanin/python-tabulate}{tabulate}:
%\begin{verbatim}
%table = [["Sun",696000,1989100000],["Earth",6371,5973.6],
%        ["Moon",1737,73.5],["Mars",3390,641.85]]
%headers = ["Planet","R (km)", "mass (x 10^29 kg)"]
%table = tabulate(table, headers=headers, tablefmt="latex")
%with open("table.tex", 'w') as file:
%    file.write(table)
%\end{verbatim}
%
%\begin{table}[h]
%    \centering
%    \input{./tables/example_table.tex}
%\end{table}

\subsection{Biblatex}
Rember when using biblatex to use 'parencite' for \citep{kamtheDataEfficient2018} and when using natbib to use 'citep'.

%\bibliography{biblio.bib}
\end{document}
