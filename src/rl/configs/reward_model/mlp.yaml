_target_: src.rl.models.rewards.MLPRewardModel
_convert_: all
network:
  _target_: torch.nn.Sequential
  _args_:
    - _target_: torch.nn.Linear
      in_features: ${input_dim}
      # out_features: 512
      out_features: 64
    # - _target_: torch.nn.ReLU
    # - _target_: torch.nn.Linear
    #   in_features: 512
    #   out_features: 512
    # - _target_: torch.nn.ReLU
    - _target_: torch.nn.Tanh
    - _target_: torch.nn.Linear
      # in_features: 512
      in_features: 64
      out_features: 1
learning_rate: 1e-3
num_iterations: 5000
batch_size: 64
# num_workers: 1 # TODO what should I set this to?
num_inducing: 128
# jitter: 1e-4
delta: 1e-5
sigma_noise: 1.0
wandb_loss_name: "reward/loss"
# early_stopper:
#   _target_: src.rl.utils.EarlyStopper
#   patience: 50
#   min_delta: 0
device: ${device} # TODO should this always be cpu??
