%!TeX program = lualatex

\documentclass[final,12pt]{beamer}

% ====================
% Packages
% ====================

% Packages
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=a1,orientation=portrait,scale=1.4]{beamerposter}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{mathtools}
\usetheme{aalto}
\usecolortheme{aalto}
\usepackage{graphicx}
%\usepackage[square]{natbib}
%\setlength{\bibsep}{4pt plus 0.3ex}  % Squeeze line spacing in references
\usepackage{booktabs}
% Tikz
\usepackage{tikz}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations,backgrounds,arrows.meta,calc}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{pgfplots}
\usepackage{subcaption}
\usetikzlibrary{}
\usepackage{chronosys}
\newcommand{\blue}[1]{\textcolor{aaltoblue}{#1}}
\newcommand{\black}[1]{\textcolor{black}{#1}}
%\usepackage{color}
\usepackage{xcolor}
\definecolor{cgreen}{RGB}{63, 174, 67}

% Array/table packages
\usepackage{tabularx}
\usepackage{array,multirow}
\usepackage{colortbl}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newlength{\tblw}

% Our method
\newcommand{\our}{SFR}
	
% Custom error formatting
%\newcommand{\val}[2]{ $#1$\textcolor{gray}{\tiny ${\pm}#2$}} 



% Packages for bold math
\usepackage{bm}
\newcommand{\mathbold}[1]{\bm{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\renewcommand{\mid}{\,|\,}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}
\newcommand{\numDataNew}{\ensuremath{N^{\text{new}}}}
\newcommand{\numDataOld}{\ensuremath{N^{\text{old}}}}
\newcommand{\numInducing}{\ensuremath{M}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\inputDomain}{\ensuremath{\mathbb{R}^{D}}}
\newcommand{\outputDomain}{\ensuremath{\mathbb{R}^{C}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}


% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
% \newcommand{\weights}{\ensuremath{\bm\phi}}
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transitionFnWithParams}{\ensuremath{\transitionFn_{\weights}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}

% Math Macros
\newcommand{\MB}{\mbf{B}}
\newcommand{\MC}{\mbf{C}}
\newcommand{\MZ}{\mbf{Z}}
\newcommand{\MV}{\mbf{V}}
\newcommand{\MX}{\mbf{X}}
\newcommand{\MA}{\mbf{A}}
\newcommand{\MK}{\mbf{K}}
\newcommand{\MI}{\mbf{I}}
\newcommand{\MH}{\mbf{H}}
\newcommand{\T}{\top}
\newcommand{\vzeros}{\mbf{0}}
\newcommand{\vtheta}[0]{\mathbold{\theta}}
\newcommand{\valpha}[0]{\mathbold{\alpha}}
\newcommand{\vkappa}[0]{\mathbold{\kappa}}
\newcommand{\vbeta}[0]{\mathbold{\beta}}
\newcommand{\MBeta}[0]{\mathbold{B}}
\newcommand{\vlambda}[0]{\mathbold{\lambda}}
\newcommand{\diag}{\text{{diag}}}

\newcommand{\vm}{\mbf{m}}
\newcommand{\vz}{\mbf{z}}
\newcommand{\vf}{\mbf{f}}
\newcommand{\vu}{\mbf{u}}
\newcommand{\vx}{\mbf{x}}
\newcommand{\vy}{\mbf{y}}
\newcommand{\vw}{\mbf{w}}
\newcommand{\va}{\mbf{a}}

\newcommand{\Jac}[2]{\mathcal{J}_{#1}(#2)}
\newcommand{\JacT}[2]{\mathcal{J}_{#1}^\top(#2)}


\newcommand{\GP}{\mathcal{GP}}
\newcommand{\KL}[2]{\mathrm{D}_\textrm{KL} \dbar*{#1}{#2}}
\newcommand{\MKzz}{\mbf{K}_{\mbf{z}\mbf{z}}}
\newcommand{\MKzzc}{\mbf{K}_{\mbf{z}\mbf{z}, c}}
\newcommand{\MKxx}{\mbf{K}_{\mbf{x}\mbf{x}}}
\newcommand{\MKzx}{\mbf{K}_{\mbf{z}\mbf{x}}}
\newcommand{\MKxz}{\mbf{K}_{\mbf{x}\mbf{z}}}
\newcommand{\vkzi}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vkzic}{\mbf{k}_{\mbf{z}i,c}}
\newcommand{\vkzs}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vk}{\mbf{k}}
\newcommand{\MLambda}[0]{\mathbold{\Lambda}}
\newcommand{\MSigma}[0]{\mathbold{\Sigma}}
\newcommand{\N}{\mathrm{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\myexpect}{\mathbb{E}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\Norm}{\mathcal{N}}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% Math and miscellaneous
\renewcommand{\mid}[0]{\,|\,}


% Config for Arno's awesome TikZ plotting stuff
\newlength{\figurewidth}
\newlength{\figureheight}


% ====================
% Title
% ====================

\title{Sparse Function-space Representation \\ of Neural Networks}

\author{%
  Aidan Scannell\textsuperscript{\star}\,\inst{1}\,\inst{2} \quad
  Riccardo Mereu\textsuperscript{\star}\,\inst{1} \quad
  Paul Chang\,\inst{1} \\
  Ella Tamir\,\inst{1}\quad
  Joni Pajarinen\,\inst{1}\quad
  Arno Solin\,\inst{1}
}

%\author{Your Name\inst{1} ~~ Collaborator Name\inst{2} ~~ Arno Solin\inst{1}}

\institute[shortinst]{ \inst{1}Aalto University \qquad \inst{2}Finnish Center for Artificial Intelligence}


% ====================
% Footer (optional)
% ====================

\footercontent{
  International Conference in Machine Learning (ICML 2023) --- Hawaii, US \hfill
  \href{mailto:aidan.scannell@aalto.fi}{aidan.scannell@aalto.fi}}

% ====================
% Body
% ====================
\begin{document}

%\begin{table}[t!] 
%  % \centering\scriptsize
%	% \renewcommand{\arraystretch}{1.}
%	% \setlength{\tabcolsep}{1.2pt}
%	% \setlength{0.083\textwidth}  
%	
%	% Custom error formatting
%	\newcommand{\vali}[2]{%
%		$#1${\tiny ${\pm}#2$}
%	} 
%\begin{tabular}{lCCCCC||CCCCCC} \\
%\toprule
%% &&&&&&&&&&&&
%% & \multicolumn{5}{c}{}
%% & \multicolumn{4}{c}{Ablations ($M=32$)}
%% 
%
%Method &  nn map* &  mfvi* &  bnn* &  glm* & \our & { gp} full & { gp} subset & { gp} subset & \our & \our  \\
%Mean from & & & & &  gp&  gp&  gp&  nn&  gp &  nn \\
%\midrule
% australian & \vali{\textbf {0.31}}{.01} & \vali{0.34}{.01} & \vali{0.42}{.00} & \vali{\textbf {0.32}}{.02} & \vali{\textbf {0.32}}{.03} & \vali{\textbf {0.32}}{.03} & \vali{0.51}{.01} & \vali{\textbf {0.33}}{.02} & \vali{\textbf {0.33}}{.03} & \vali{\textbf {0.32}}{.03} \\
% cancer & \vali{\textbf {0.11}}{.02} & \vali{0.11}{.01} & \vali{0.19}{.00} & \vali{\textbf {0.10}}{.01} & \vali{\textbf {0.11}}{.03} & \vali{\textbf {0.11}}{.03} & \vali{0.41}{.02} & \vali{\textbf {0.11}}{.03} & \vali{\textbf {0.11}}{.03} & \vali{\textbf {0.10}}{.04} \\
% ionosphere & \vali{0.35}{.02} & \vali{0.41}{.01} & \vali{0.50}{.00} & \vali{\textbf {0.29}}{.01} & \vali{0.34}{.04} & \vali{\textbf {0.34}}{.04} & \vali{0.54}{.02} & \vali{\textbf {0.34}}{.05} & \vali{0.34}{.04} & \vali{\textbf {0.30}}{.06} \\
% glass & \vali{0.95}{.03} & \vali{1.06}{.01} & \vali{1.41}{.00} & \vali{\textbf {0.86}}{.01} & \vali{0.93}{.08} & \vali{\textbf {0.93}}{.08} & \vali{1.15}{.05} & \vali{0.99}{.07} & \vali{0.95}{.08} & \vali{\textbf {0.87}}{.07} \\
% vehicle & \vali{\textbf {0.42}}{.01} & \vali{0.50}{.01} & \vali{0.89}{.00} & \vali{0.43}{.01} & \vali{0.48}{.03} & \vali{\textbf {0.48}}{.03} & \vali{1.02}{.03} & \vali{0.58}{.02} & \vali{0.54}{.02} & \vali{\textbf {0.49}}{.02} \\
% waveform & \vali{\textbf {0.34}}{.00} & \vali{0.39}{.00} & \vali{0.52}{.00} & \vali{0.34}{.00} & \vali{\textbf {0.35}}{.02} & \vali{\textbf {0.35}}{.02} & \vali{0.57}{.02} & \vali{\textbf {0.36}}{.02} & \vali{\textbf {0.35}}{.02} & \vali{\textbf {0.34}}{.03} \\
% digits & \vali{\textbf {0.09}}{.00} & \vali{0.22}{.00} & \vali{0.88}{.00} & \vali{0.25}{.00} & \vali{0.37}{.02} & \vali{\textbf {0.08}}{.03} & \vali{1.65}{.04} & \vali{0.43}{.01} & \vali{0.43}{.01} & \vali{0.32}{.02} \\
% satellite & \vali{\textbf {0.23}}{.00} & \vali{0.31}{.00} & \vali{0.48}{.00} & \vali{0.24}{.00} & \vali{0.29}{.01} & \vali{\textbf {0.27}}{.01} & \vali{1.12}{.04} & \vali{0.36}{.01} & \vali{0.33}{.01} & \vali{\textbf {0.28}}{.01} \\
%\bottomrule
%\end{tabular}
%\end{table}



\begin{frame}[t]
\begin{columns}[t]
\begin{column}{2.05\colwidth}
\vspace{-0.8cm}
\begin{alertblock}{Summary}
Deep neural networks have limitations in \textit{estimating uncertainty}, \textit{incorporating new data}, and \textit{avoiding catastrophic forgetting}. To overcome these issues, we introduce a method that converts neural networks from weight-space to a \alert{\bf low-rank function-space} representation using \alert{\bf dual parameters}. Unlike previous methods, our approach, named \alert{\bf Sparse Function Representation (SFR)}, captures the \alert{\bf full joint distribution} of the entire data set, not just a subset. This allows for a concise and reliable way of capturing uncertainty and facilitates the inclusion of new data without the need for retraining. We provide a proof-of-concept quantifying uncertainty for supervised learning tasks on UCI benchmark data sets.
  \end{alertblock}\end{column}
\end{columns}
\begin{columns}[t]


\separatorcolumn

\begin{column}{\colwidth}
  \begin{minipage}{\textwidth}
    \vspace*{-26pt}
  	%\begin{figure}[t]
    \pgfplotsset{axis on top,scale only axis,width=\figurewidth,height=\figureheight, ylabel near ticks,ylabel style={yshift=0pt},xlabel style={yshift=-4pt},y tick label style={rotate=90},legend style={nodes={scale=.8, transform shape}},tick label style={font=\footnotesize},xtick={0,.4,.8,1.2,1.6,2.}}
  \pgfplotsset{xlabel={Input, $x$},axis line style={rounded corners=2pt}}
  %
  % Hack pgfplots addplot options
  \pgfplotsset{every axis plot/.append style={mark size=5,line width=1.2pt}}
  %
  % Set figure 
  \setlength{\figurewidth}{.4\textwidth}
  \setlength{\figureheight}{.31\textwidth}
  %
  \def\inducing{\large Sparse inducing points}
  %
  \begin{minipage}[c]{.45\textwidth}
    \raggedleft
    \pgfplotsset{ylabel={Output, $y$}}
    \input{./fig/regression-nn.tex}%
  \end{minipage}
  \hfill  
  \begin{minipage}[c]{.03\textwidth}
    \centering
    \tikz[overlay,remember picture]\node(p0){};
  \end{minipage}  
  \hfill
  \begin{minipage}[c]{.45\textwidth}
    \raggedleft
    \pgfplotsset{yticklabels={,,},ytick={\empty}}
    \input{./fig/regression-nn2svgp.tex}%
  \end{minipage}
%  \hfill  
%  \begin{subfigure}[c]{.01\textwidth}
%    \centering
%    \tikz[overlay,remember picture]\node(p1){};
%  \end{subfigure}  
%  \hfill
%  \begin{subfigure}[c]{.28\textwidth}
%    \raggedleft
%    \pgfplotsset{yticklabels={,,},ytick={\empty}}        
%    \input{./fig/regression-update.tex}%
%  \end{subfigure}
  % 
  \begin{tikzpicture}[remember picture,overlay]
      % Arrow style
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=3em, minimum width=3pt, single arrow head extend=6pt, fill=black!80, anchor=center, rotate=0, inner sep=10pt, rounded corners=1pt]
    % Arrows
    \node[myarrow] (p0-arr) at ($(p0) + (1.2em,1.5em)$) {};
    %\node[myarrow] (p1-arr) at ($(p1) + (1em,1.5em)$) {};
    % Arrow labels
    \node[font=\scriptsize\sc,color=white,scale=.8] at (p0-arr) {\our};
    %\node[font=\scriptsize\sc,color=white] at (p1-arr) {new data};   
  \end{tikzpicture}
  %\end{figure}
  
    \alert{\bf Regression w/ 2-layer MLP.} 
    Prediction from a trained neural network {\it (left)} and from our approach using inducing points to summarize the training data {\it (right)}. SFR captures the predictive mean and uncertainty, and can incorporate new data without retraining the model.
   \end{minipage}\\[0.5cm]

  \begin{block}{NN Function-space Representation}
  \alert{\bf Inputs} in a {\it supervised setting} for NNs $f_\mathbf{w}: \inputDomain \to \outputDomain$:\
  \begin{itemize}
  \item $\dataset = \{(\vx_{i} , \vy_{i})\}_{i=1}^{N}$, a data set w/ input $\vx_i \in \inputDomain$ and output $\vy_i \in \outputDomain$;
  \item $\weights \in \R^{P}$, the initial weights of the neural network.
  \end{itemize}
\alert{\bf Goal: }
  minimize the empirical (regularized) risk loss function:
\begin{align} 
  \weights^{*} =  \arg \min_{\weights} \mathcal{L}(\dataset,\weights) \nonumber
     = \arg \min_{\weights} \textstyle\sum_{i=1}^{N} \ell(f_\weights(\mathbf{x}_{i}), y_i) + \delta \mathcal{R}(\weights).
 \end{align}
\alert{\bf Output:} $\weights^*$, the Maximum A-Posteriori (MAP) weights of the NN.\\[0.5cm]

\alert{\bf How to capture distribution over NN model functions?}	\\
Use their first two moments, obtaining a Gaussian process with a \alert{mean function $\mu(\cdot)$} and a \alert{covariance function $\kappa(\cdot,\cdot)$} (or kernel). \\
{\it For GPs}, linear approximations in weight space lead to function-space equivalent approximations:
\begin{equation*}
	f_\weights(\vx) \approx 
\phi^\top\!(\vx) \, \vw \quad\implies\quad \mu(\vx) = 0 \quad \text{and} \quad \kappa(\vx, \vx') = \frac{1}{\delta} \, \phi^\T\!(\vx) \, \phi(\vx')
\end{equation*}\\
{\it For NNs}, we can use the \alert{Laplace-GGN approximation} to get a linear model of the neural network at the MAP as:
\begin{equation*}
	f_{\weights^*}(\vx) \approx \Jac{\weights_*}{\vx} \, \weights \implies   \mu(\vx) =  0 \quad \text{and} \quad
  \kappa(\vx, \vx')
  = \frac{1}{\delta} \, \Jac{\weights^*}{\vx} \, \JacT{\weights^*}{\vx'}, 
\end{equation*}
where $\Jac{\weights}{\vx} \coloneqq \left[ \nabla_\weights f_\weights(\vx)\right]^\top \in \R^{C \times P}$ is the Jacobian at $\weights^*$.
%\begin{itemize}
%	\item Maximum a-posteriori 
%\end{itemize}
\end{block}
\end{column}

\separatorcolumn

\begin{column}{\colwidth}
\begin{minipage}{\textwidth}
  % Colours
  \definecolor{C0}{HTML}{DF6679}
  \definecolor{C1}{HTML}{69A9CE}
\begin{figure}[t]
  \centering
  % Set figure size
  \setlength{\figurewidth}{.31\textwidth}
  \setlength{\figureheight}{\figurewidth}
  %
  \begin{tikzpicture}[outer sep=0,inner sep=0]
% This command wasn't working 
%    \newcommand{\addfig}[2]{
%    \begin{scope}
%      \clip[rounded corners=3pt] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
%      \node (#2) at (#1) {\includegraphics[width=1.05\figurewidth]{./fig/#2}};
%    \end{scope}
%    %\draw[rounded corners=3pt,line width=1.2pt,black!60] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
%    }
%
%    % The neural network
%    \addfig{0,0}{banana-nn}
    \begin{scope}
      \clip[rounded corners=3pt] ($(0,0)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (banana-nn) at (0,0) {\includegraphics[width=1.05\figurewidth]{./fig/banana-nn}};
    \end{scope}
    
    % The nn2svgp
    %\addfig{1.1\figurewidth,0}{banana-nn2svgp}
    \begin{scope}
      \clip[rounded corners=3pt] ($(1.1\figurewidth,0)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (banana-nn2svgp) at (1.1\figurewidth,0) {\includegraphics[width=1.05\figurewidth]{./fig/banana-nn2svgp}};
    \end{scope}
    

    % The update
    %\addfig{2.2\figurewidth,0}{banana-hmc}
    \begin{scope}
      \clip[rounded corners=3pt] ($(2.2\figurewidth,0)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (banana-hmc) at (2.2\figurewidth,0) {\includegraphics[width=1.05\figurewidth]{./fig/banana-hmc}};
    \end{scope}
	% The arrow
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=3em, minimum width=3pt, single arrow head extend=6pt, fill=black!80, anchor=center, rotate=0, inner sep=10pt, rounded corners=1pt]
    \tikzstyle{myblock} = [draw=black!80, minimum height=8mm, minimum width=7mm, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    \node[myarrow] (first-arr) at ($(banana-nn)!0.5!(banana-nn2svgp)$) {};
    \node[myblock] (second-arr) at ($(banana-nn2svgp)!0.5!(banana-hmc)$) {};

    % Arrow labels
    \node[font=\scriptsize\sc,color=white,scale=.8] at (first-arr) {\our};
    \node[font=\scriptsize,color=white] at (second-arr) {\normalsize$\bm\approx$};
         
    % Labels
    \node[anchor=north, font=\footnotesize] at ($(banana-nn) + (0,-.55\figureheight)$) {Neural network prediction};
    \node[anchor=north, font=\footnotesize] at ($(banana-nn2svgp) + (0,-.55\figureheight)$) {Sparse function-space representation};
    \node[anchor=north, font=\footnotesize] at ($(banana-hmc) + (0,-.55\figureheight)$) {HMC result as baseline};      
    
  \end{tikzpicture}

\end{figure}
  \newcommand{\mycircle}{\protect\tikz[baseline=-.6ex]\protect\node[circle,inner sep=5pt,draw=black,fill=C0,opacity=.5]{};}
  \newcommand{\mysquare}{\protect\tikz[baseline=-.6ex]\protect\node[inner sep=8pt,draw=black,fill=C1,opacity=.5]{};}
  \newcommand{\myinducing}{\protect\tikz[baseline=-.7ex]\protect\node[circle,inner sep=5pt,draw=black,fill=black]{};}
\alert{\bf Uncertainty quantification for classification (\,\mysquare~vs.~\mycircle).} We convert the trained neural network {\it (left)} to a sparse GP model with a set of inducing points~\myinducing\ {\it(middle)}. Results show a similar behaviour as running full Hamiltonian Monte Carlo (HMC) on the original NN model weights {\it (right)}. Marginal uncertainty depicted by colour intensity.
\end{minipage}\\[1cm]

 \begin{block}{SFR: Sparse Function Representation}
\alert{\bf GP} predictive posterior:
 \begin{align}  \label{eq:gp_pred}
  \myexpect_{p(f_i \mid\vy)}[f_i] &= \vk_{\vx i}^\top \valpha \quad \text{and} \\
  \mathrm{Var}_{p(f_i \mid \vy)}[f_i] &= k_{ii} - \vk_{\vx i}^\top ( \MKxx + \diag(\vbeta)^{-1})^{-1} \vk_{\vx i} 
\end{align}
with \alert{dual parameters} $\valpha = \{\alpha_i\}_{i=1}^N, \vbeta = \{ \beta_i\}_{i=1}^N$:
\begin{align}
  \alpha_i &\coloneqq {\color{red}{\myexpect_{p(\vw \mid \vy)}}}[\nabla_{f}\log p(y_i \mid f) |_{f=f_i}] 
  \quad \text{and} \\
  \beta_i &\coloneqq - {\color{red}{\myexpect_{p(\vw \mid \vy)}}}[\nabla^2_{f f}\log p(y_i \mid f_i) |_{f=f_i}]
\end{align}
We consider the MAP of ${\color{red}{p(\vw \mid \vy)}}$,
%\approx \color{cgreen}{q(\weights)} $, 
and get $\hat{\valpha}, \hat{\vbeta} \in \R^{N}$: 
\begin{align}
  \alpha_i \approx \hat{\alpha}_i &\coloneqq %{\color{cgreen}{\myexpect_{q(\vw)}}} [
  \nabla_{f}\log p(y_i \mid f) |_{f=f_i}
%  ]
  \quad \text{and} \\
  \beta_i \approx \hat{\beta}_i &\coloneqq - 
%  {\color{cgreen}{\myexpect_{q(\vw)}}} [ 
  \nabla^2_{ff}\log p(y_i \mid f) |_{f=f_i}
%  ]
\end{align}

{\bf Cons:} requires access to all data $\rightarrow \color{red}{\mathcal{O}(N^3)}$
%  $\rightarrow$ Sparsify the resulting GP model to get $\mathcal{O}(M^3)$

\alert{\bf Sparse GP (SFR)} predictive posterior:
%We can build a sparse GP posterior with the following form:
\begin{align}   
\myexpect_{p(f_i \mid\vy)}[f_i] \approx \myexpect_{q_{\vu}(\vf)}[f_i] &= \vkzs^{\T} \MKzz^{-1} \valpha_{\vu}
   \quad \text{and} \\ 
\mathrm{Var}_{p(f_i \mid \vy)}[f_i] \approx \textrm{Var}_{q_{\vu}(\vf)}[f_i] &= k_{ii} - \vkzs^\top [\MKzz^{-1} - (\MKzz + \MBeta_{\vu})^{-1} ]\vkzs
\end{align}
% where $q_\vu(f_i)=\int p(f_i|\vu) q(\vu)\,\textrm{d}\vu$ 
given inducing points $\vu_j=f_{\vw^*}(\vz_j)$ with   $\{\vz_j\}_{j=1}^M$ 
$\rightarrow \color{red}{\mathcal{O}(M^3) \text{ with } (M\ll N)}$

%\heading{Dual parameterization}
with \alert{\bf SFR dual parameters} $\valpha_{\vu} \in \R^M, \MBeta_{\vu} \in \R^{M \times M}$: 
\begin{equation}
\valpha_{\vu}  =  \sum_{i=1}^N  \vkzi \, \hat{\alpha}_{i} %\in \R^M
\quad \text{and} \quad
  \MBeta_{\vu} =  \sum_{i=1}^N \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T} %\in \R^{M \times M}
\end{equation}
%\heading{Sparse Neural Network GP}

\end{block}
\end{column}
\separatorcolumn
\end{columns}
\vspace{-1.6cm}

\begin{columns}
\begin{column}{2.05\colwidth}
\begin{block}{Results on UCI datasets}
%\begin{figure*}[t]
%  \centering\scriptsize
%  \setlength{\figurewidth}{.18\textwidth}
%  \setlength{\figureheight}{\figurewidth}
%  \pgfplotsset{axis on top,scale only axis,y tick label style={rotate=90}, x tick label style={font=\footnotesize},y tick label style={font=\footnotesize},title style={yshift=-4pt,font=\large}, y label style={font=\large},x label style={font=\large},grid=major, width=\figurewidth, height=\figureheight}
%  \pgfplotsset{grid style={line width=.1pt, draw=gray!10,dashed}}
%  \pgfplotsset{xlabel={$M$},ylabel style={yshift=-12pt}}  
%  %
%  \begin{minipage}[t]{.13\textwidth}
%    \raggedleft
%    \pgfplotsset{ylabel=NLPD}
%    \input{./fig/australian.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/breast_cancer.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/ionosphere.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/glass.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/vehicle.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/waveform.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/digits.tex}
%  \end{minipage}
%  \hfill
%  \begin{minipage}[t]{.12\textwidth}
%    \raggedleft
%    \input{./fig/satellite.tex}
%  \end{minipage}\\[-1em]
%  %
%  % Legend  
%  \definecolor{steelblue31119180}{RGB}{31,119,180}
%  \definecolor{darkorange25512714}{RGB}{255,127,14}  
%  \newcommand{\myline}[1]{\protect\tikz[baseline=-.5ex,line width=1.6pt]\protect\draw[draw=#1](0,0)--(1.2em,0);}
%  \caption{Comparison of convergence in terms of number of inducing points $M$ in NLPD (mean over 10 seeds) on UCI classification tasks: \our (thick) vs.\ subsets \citep[][thin]{immer2021improving}. Orange lines (\myline{darkorange25512714}) use the GP mean, whereas blue lines (\myline{steelblue31119180}) the NN MAP estimate as mean. Our \our converges fast for all cases showing clear benefits of its ability to summarize all the data onto a sparse set of inducing points.\looseness-1}
%  \label{fig:uci}
%  %\vspace*{-6pt}
%\end{figure*}


\begin{figure}
\centering
\begin{subfigure}[b]{0.5\textwidth}\centering
	\includegraphics[scale=0.5]{fig/table.pdf}	
\end{subfigure}\hspace{2cm}
\begin{subfigure}[b]{0.3\textwidth}\centering
	\includegraphics[scale=0.2]{fig/nlpd.pdf}
%	\definecolor{steelblue31119180}{RGB}{31,119,180}
%  \definecolor{darkorange25512714}{RGB}{255,127,14}  
%  \newcommand{\myline}[1]{\protect\tikz[baseline=-.5ex,line width=1.6pt]\protect\draw[draw=#1](0,0)--(1.2em,0);}
  \caption{\footnotesize Convergence w.r.t. the number of inducing points $M$ in NLPD on UCI classification tasks: \our (thick) vs.\ GP subset. Orange lines 
%  (\myline{darkorange25512714}) 
  use the GP mean, whereas blue lines 
%  (\myline{steelblue31119180}) 
the NN MAP estimate as mean. \our converges faster for all cases.}
%	\caption{aaaaa}
\end{subfigure}
\end{figure}

%\begin{table}[t!]
%	\newcommand{\vali}[2]{%
%		$#1${\tiny ${\pm}#2$}
%	} 
%	\resizebox{\textwidth}{!}{
%\centering\tiny
%\begin{tabular}{@{}lccccc|cccc@{}}
%\toprule
% &  &  &  &  &  & \multicolumn{4}{c}{\tiny Ablations (M=32)} \\ \cmidrule(l){7-10} 
% & NN MAP & BNN & GLM & \begin{tabular}[c]{@{}c@{}}SFR\\ (GP)\end{tabular} & Full GP & \begin{tabular}[c]{@{}c@{}}GP Subset\\ (GP)\end{tabular} & \begin{tabular}[c]{@{}c@{}}GP Subset\\ (NN)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SFR \\ (GP)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SFR\\ (NN)\end{tabular} \\ \midrule
% {\tiny AUSTRALIAN}		& \vali{\bf {0.31}}{.01} 	& \vali{0.42}{.00} & \vali{\bf {0.32}}{.02} & \vali{\bf {0.32}}{.03} & \vali{\bf {0.32}}{.03} & \vali{0.51}{.01} & \vali{\bf {0.33}}{.02} & \vali{\bf {0.33}}{.03} & \vali{\bf {0.32}}{.03} \\
% {\tiny CANCER}				& \vali{\bf {0.11}}{.02}	& \vali{0.19}{.00} & \vali{\bf {0.10}}{.01} & \vali{\bf {0.11}}{.03} & \vali{\bf {0.11}}{.03} & \vali{0.41}{.02} & \vali{\bf {0.11}}{.03} & \vali{\bf {0.11}}{.03} & \vali{\bf {0.10}}{.04} \\
% {\tiny IONOSPHERE}		& \vali{0.35}{.02} 						& \vali{0.50}{.00} & \vali{\bf {0.29}}{.01} & \vali{0.34}{.04} & \vali{\bf {0.34}}{.04} & \vali{0.54}{.02} & \vali{\bf {0.34}}{.05} & \vali{0.34}{.04} & \vali{\bf {0.30}}{.06} \\
% {\tiny GLASS}				& \vali{0.95}{.03} 						& \vali{1.41}{.00} & \vali{\bf {0.86}}{.01} & \vali{0.93}{.08} & \vali{\bf {0.93}}{.08} & \vali{1.15}{.05} & \vali{0.99}{.07} & \vali{0.95}{.08} & \vali{\bf {0.87}}{.07} \\
% {\tiny VEHICLE}			& \vali{\bf {0.42}}{.01} 	& \vali{0.89}{.00} & \vali{0.43}{.01} & \vali{0.48}{.03} & \vali{\bf {0.48}}{.03} & \vali{1.02}{.03} & \vali{0.58}{.02} & \vali{0.54}{.02} & \vali{\bf {0.49}}{.02} \\
% {\tiny WAVEFORM}			& \vali{\bf {0.34}}{.00} 	& \vali{0.52}{.00} & \vali{0.34}{.00} & \vali{\bf {0.35}}{.02} & \vali{\bf {0.35}}{.02} & \vali{0.57}{.02} & \vali{\bf {0.36}}{.02} & \vali{\bf {0.35}}{.02} & \vali{\bf {0.34}}{.03} \\
% {\tiny DIGITS}				& \vali{\bf {0.09}}{.00} 	& \vali{0.88}{.00} & \vali{0.25}{.00} & \vali{0.37}{.02} & \vali{\bf {0.08}}{.03} & \vali{1.65}{.04} & \vali{0.43}{.01} & \vali{0.43}{.01} & \vali{0.32}{.02} \\
% {\tiny SATELLITE} 		& \vali{\bf {0.23}}{.00} 	& \vali{0.48}{.00} & \vali{0.24}{.00} & \vali{0.29}{.01} & \vali{\bf {0.27}}{.01} & \vali{1.12}{.04} & \vali{0.36}{.01} & \vali{0.33}{.01} & \vali{\bf {0.28}}{.01} \\
% \bottomrule
%\end{tabular}
%}
%\end{table}
\end{block}
\end{column}
\end{columns}

\begin{columns}
\begin{column}{2.05\colwidth}
\vspace{-0.5cm}
%  \vspace*{1em}

  \nocite{*} % <-- This lists all references that are in the bib file
  \begin{block}{References}
    \vspace*{-.25em}
    \footnotesize{\bibliographystyle{ieeetr}\bibliography{bibliography}}
  \end{block}

\end{column}
\end{columns}
\end{frame}
%\begin{table}[t!]
%	\newcommand{\vali}[2]{%
%		$#1${\tiny ${\pm}#2$}
%	} 
%	\resizebox{\textwidth}{!}{
%\centering\tiny
%\begin{tabular}{@{}lccccc|cccc@{}}
%\toprule
% &  &  &  &  &  & \multicolumn{4}{c}{\tiny Ablations (M=32)} \\ \cmidrule(l){7-10} 
% & NN MAP & BNN & GLM & \begin{tabular}[c]{@{}c@{}}SFR\\ (GP)\end{tabular} & Full GP & \begin{tabular}[c]{@{}c@{}}GP Subset\\ (GP)\end{tabular} & \begin{tabular}[c]{@{}c@{}}GP Subset\\ (NN)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SFR \\ (GP)\end{tabular} & \begin{tabular}[c]{@{}c@{}}SFR\\ (NN)\end{tabular} \\ \midrule
% {\tiny AUSTRALIAN}		& \vali{\bf {0.31}}{.01} 	& \vali{0.42}{.00} & \vali{\bf {0.32}}{.02} & \vali{\bf {0.32}}{.03} & \vali{\bf {0.32}}{.03} & \vali{0.51}{.01} & \vali{\bf {0.33}}{.02} & \vali{\bf {0.33}}{.03} & \vali{\bf {0.32}}{.03} \\
% {\tiny CANCER}				& \vali{\bf {0.11}}{.02}	& \vali{0.19}{.00} & \vali{\bf {0.10}}{.01} & \vali{\bf {0.11}}{.03} & \vali{\bf {0.11}}{.03} & \vali{0.41}{.02} & \vali{\bf {0.11}}{.03} & \vali{\bf {0.11}}{.03} & \vali{\bf {0.10}}{.04} \\
% {\tiny IONOSPHERE}		& \vali{0.35}{.02} 						& \vali{0.50}{.00} & \vali{\bf {0.29}}{.01} & \vali{0.34}{.04} & \vali{\bf {0.34}}{.04} & \vali{0.54}{.02} & \vali{\bf {0.34}}{.05} & \vali{0.34}{.04} & \vali{\bf {0.30}}{.06} \\
% {\tiny GLASS}				& \vali{0.95}{.03} 						& \vali{1.41}{.00} & \vali{\bf {0.86}}{.01} & \vali{0.93}{.08} & \vali{\bf {0.93}}{.08} & \vali{1.15}{.05} & \vali{0.99}{.07} & \vali{0.95}{.08} & \vali{\bf {0.87}}{.07} \\
% {\tiny VEHICLE}			& \vali{\bf {0.42}}{.01} 	& \vali{0.89}{.00} & \vali{0.43}{.01} & \vali{0.48}{.03} & \vali{\bf {0.48}}{.03} & \vali{1.02}{.03} & \vali{0.58}{.02} & \vali{0.54}{.02} & \vali{\bf {0.49}}{.02} \\
% {\tiny WAVEFORM}			& \vali{\bf {0.34}}{.00} 	& \vali{0.52}{.00} & \vali{0.34}{.00} & \vali{\bf {0.35}}{.02} & \vali{\bf {0.35}}{.02} & \vali{0.57}{.02} & \vali{\bf {0.36}}{.02} & \vali{\bf {0.35}}{.02} & \vali{\bf {0.34}}{.03} \\
% {\tiny DIGITS}				& \vali{\bf {0.09}}{.00} 	& \vali{0.88}{.00} & \vali{0.25}{.00} & \vali{0.37}{.02} & \vali{\bf {0.08}}{.03} & \vali{1.65}{.04} & \vali{0.43}{.01} & \vali{0.43}{.01} & \vali{0.32}{.02} \\
% {\tiny SATELLITE} 		& \vali{\bf {0.23}}{.00} 	& \vali{0.48}{.00} & \vali{0.24}{.00} & \vali{0.29}{.01} & \vali{\bf {0.27}}{.01} & \vali{1.12}{.04} & \vali{0.36}{.01} & \vali{0.33}{.01} & \vali{\bf {0.28}}{.01} \\
% \bottomrule
%\end{tabular}
%}
%\end{table}
\end{document}
