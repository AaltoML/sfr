\documentclass{article}

%\title{Investigatin Uncertainty Quantification in Model-based Reinforcement Learning}
% \title{Model-based Reinforcement Learning with Fast Posterior Updates}
\title{Sequential Decision-Making under Uncertainty with Big Data}
\author{%
  Aidan ~Scannell \\
  Aalto University\\
  %Finnish Center for Artificial Intelligence \\
  \texttt{aidan.scannell@aalto.fi}
  \And
  Paul ~Chang \\
  Aalto University\\
  \texttt{paul.chang@aalto.fi}
  \And
  Ella ~Tamir \\
  Aalto University\\
  \texttt{ella.tamir@aalto.fi}
  \And
  Arno ~Solin \\
  Aalto University\\
  \texttt{arno.solin@aalto.fi}
  \And
  Joni ~Pajarinen \\
  Aalto University\\
  \texttt{joni.pajarinen@aalto.fi}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


% NeurIPS packages
\usepackage[preprint,nonatbib]{neurips_2022}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Bibliography
\usepackage[maxcitenames=1, maxbibnames=4, doi=false, isbn=false, eprint=true, backend=bibtex, hyperref=true, url=false, style=authoryear-comp]{biblatex}
\addbibresource{zotero-library.bib}
% \addbibresource{paper/zotero-library.bib}

% Our packages
\usepackage{todonotes}
\usepackage[colorlinks=true,linkcolor=blue,allcolors=blue]{hyperref}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{derivative}

\usepackage{tikz,pgfplots}
\usepackage{subcaption}
\usetikzlibrary{}

\input{aidans-utils.tex}

% Short section names etc
% This must be imported last!
%\usepackage{cleveref}
\usepackage[capitalise,nameinlink]{cleveref}
\crefname{section}{Sec.}{Secs.}
\crefname{algorithm}{Alg.}{Algs.}
\crefname{appendix}{App.}{Apps.}
\crefname{definition}{Def.}{Defs.}
\crefname{table}{Tab.}{Tabs}

% Config for Arno's awesome TikZ plotting stuff
\newlength{\figurewidth}
\newlength{\figureheight}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\action}{\ensuremath{\mathbf{a}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}
\newcommand{\numDataNew}{\ensuremath{N^{\text{new}}}}
\newcommand{\numDataOld}{\ensuremath{N^{\text{old}}}}
\newcommand{\numInducing}{\ensuremath{M}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\inputDomain}{\ensuremath{\mathbb{R}^{D}}}
\newcommand{\outputDomain}{\ensuremath{\mathbb{R}^{C}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}

% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
\newcommand{\transitionParams}{\ensuremath{\bm\phi}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transitionFnWithParams}{\ensuremath{\transitionFn_{\transitionParams}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}

\begin{document}

\maketitle

\begin{abstract}
  % Reinforcement learning (RL) agents typically do not utilise the information previously gained during an episode.
  % as they perform learning offline, i.e. after an episode.
  % Reinforcement learning (RL) agents typically perform learning after an episode, ignoring the potential information gain whilst collecting data.
  % Reinforcement learning (RL) agents typically perform learning offline and do not utilise the information gained during an episode.
  % We present a Bayesian model-based RL algorithm which incorporates this information by updating a dynamic model's posterior during an episode.
  % Our method leverages the fast posterior updates available in the dual parameterisation of sparse variational Gaussian processes.
  % Importantly, our algorithm scales to 1) environment's with high dimensional state-spaces as it is built upon the connection between Bayesian neural networks (BNNs) and
  % Gaussian processes (GPs) and 2) long episodes, as the complexity of our updates is independent of episode length.
  % We demonstrate that our fast posterior updates improve sample efficiency in environments with action penalties, a notoriously difficult
  % challenge for model-based RL algorithms.
  %
  % Reinforcement learning (RL) agents typically perform learning offline and do not utilise the information gained during an episode.
  % We present a Bayesian model-based RL algorithm which incorporates this information by updating a dynamic model's posterior during an episode.
  % Typically training a neural network dynamic model is not feasible and nonparameteric methods such as Gaussian processes scale poorly with data.
  % Importantly, our algorithm scales to 1) environment's with high dimensional state-spaces, 2) large data sets and 3) long episodes,
  % as the complexity of our updates is independent of episode length.
  % We demonstrate that our fast posterior updates improve sample efficiency in environments with action penalties, a notoriously difficult
  % challenge for model-based RL algorithms.
  %
  Deep neural networks

  Obtain SVGP posterior on trained deep neural network (NN) without optimising any variational parameters.

  Dual formulation enables fast updates for 1) model-based RL 2) pathwise, function-space conditioning.
  % Reinforcement learning (RL) agents typically perform learning offline and do not utilise the information gained during an episode.
  % We present a Bayesian model-based RL algorithm which incorporates this information by updating a dynamic model's posterior during an episode.
  % Typically training a neural network dynamic model is not feasible and nonparameteric methods such as Gaussian processes scale poorly with data.
  % Importantly, our algorithm scales to 1) environment's with high dimensional state-spaces, 2) large data sets and 3) long episodes,
  % as the complexity of our updates is independent of episode length.
  % We demonstrate that our fast posterior updates improve sample efficiency in environments with action penalties, a notoriously difficult
  % challenge for model-based RL algorithms.
\end{abstract}

\section{Introduction} \label{sec:intro}

\begin{itemize}
  \item DNNs scale to high-dimensional and large data sets but 1) no UQ and 2) cannot be updated quickly
  \begin{itemize}
    \item In many domains we want to make decisions under a learned model's uncertainty, e.g. in healthcare, robotics, autonomous vehicles. (safey, sample efficiency (cost/time))
    \item In continual learning (e.g. online learning/RL) we need to update model quicker than retraining NN with SGD.
    \item Poorly calibrated model uncertainty
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item BNNs give UQ over high-dimensional parameter space,
  \begin{itemize}
    \item Exact inference intractable
    \item Sampling in parameter space vs Sampling in function space
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item BNN to dual SVGP
  \begin{itemize}
    \item Keep in spirit of Laplace approx and fit SVGP with no more optimisation
    \item Now we can update posterior without retraining, which is fast (good for model-based RL and TS BO?)
    \item Now we can sample in function space with pathwise conditioning (because we have SVGP).
    \begin{itemize}
      \item Our pathwise conditioning will be better as we can use dual update
    \end{itemize}
  \end{itemize}
\end{itemize}

\section{Background} \label{sec:background}
% \section{dual SVGP}
% In this section we extend these fast updates to environment's with high dimensional state spaces and large data sets.
% % Our method draws on the connection between BNNs and GPs and formulates a function space SVGP posterior given
% Our method uses a BNN dynamic model and draws on the connection between BNNs and GPs
% \parencite{khanApproximate2019} to formulate a function space SVGP posterior,
% where we can apply the fast updates from \cref{eq-dual-update-svgp}.
% At a high-level, we first use Laplace's approximation to obtain a weight space posterior for our BNN.
% We then linearise our BNN around the optimal parameters and interpret it as a GP.
% Finally, we formulate a lower rank approximation of this GP posterior (i.e. a SVGP posterior) using inducing variables.
% % formulates a function space SVGP posterior by drawing on the connection between BNNs and GPs.


In this section, we recap how to fit a GP posterior to a trained deep neural network (DNN).
We start by obtaining a posterior over the weights of a trained neural network via the Laplace approximation.
We then show how we can obtain a function-space posterior, (i.e. a GP posterior)
by linearising the neural network around the weights Maximum a Posteriori (MAP) estimate.
% We then show that by linearising the neural network we can obtain a function-space posterior, i.e. a GP posterior.

% We paraterise our single-step dynamic model $\transitionFnWithParams : \inputDomain \rightarrow \outputDomain$
% as an $L\text{-layer}$ NN with weights $\transitionParams$.
Given a data set $\dataset = \{\mathbf{x}_{n} \in \inputDomain, \mathbf{y}_{n} \in \outputDomain\}_{n=0}^{N}$,
the goal of (supervised) deep learning, is to train the weights $\transitionParams \in \R^{P}$ of an $L\text{-layer}$ NN
$f : \inputDomain \rightarrow \outputDomain$ to minimize the (regularized) empirical risk,
\begin{align}
  \transitionParams^{*} = \arg \min_{\transitionParams \in \R^{D}} \mathcal{L}(\dataset;\transitionParams) = \arg \min_{\transitionParams \in \R^{D}} \left(
  R(\transitionParams) + \sum_{n=0}^{N-1} l(f(\mathbf{x}_{n} ; \transitionParams), \mathbf{y}_{n}) \right).
\end{align}
In practice, it is common to use the mean squared error (MSE) loss
$l(f(\mathbf{x}_{n} ; \transitionParams),\mathbf{y}_{n}) = \|f(\mathbf{x}_{n} ; \transitionParams) - \mathbf{y}_{n} \|^{2}_{2}$
and to use the weight decay regularizer $R(\transitionParams)=\frac{1}{2}\gamma^{-2}\|\transitionParams\|^{2}_{2}$.
However, the practioner is free to choose any loss and regularizer. \todo{I think?}

\textbf{Laplace approximation}
The Laplace approximation \todo{add citation of original paper} constructs a Gaussian approximation of the weight-space posterior $p(\transitionParams \mid \dataset)$
by using a second-order Taylor expansion of $\mathcal{L}$ around $\transitionParams^{*}$.
The posterior approximation is given by,
\begin{align} \label{eq-laplace-approx-weight-space}
  p(\transitionParams \mid \dataset) \approx \mathcal{N} \left( \transitionParams \mid \transitionParams^{*} , \bm\Sigma_{\transitionParams^{*}} \right)
  \quad \text{with} \quad \bm\Sigma_{\transitionParams} =
 - \nabla_{\transitionParams \transitionParams}^{2} \mathcal{L} ( \dataset ; \transitionParams)|_{\transitionParams=\transitionParams^{*}}
\end{align}
That is, it sets the posterior precision to the Hessian of the loss at the optimal parameters $\transitionParams^{*}$.
It is worth noting that computing the Hessian of the loss can be computationally intractable for large networks.
The prior terms are usually trivial, so we will focus on the likelihood here.
The Jacobian and Hessian of the log likelihood can be expressed per data point,
\begin{align} \label{eq-jac}
  \nabla_{\transitionParams} \log p(\mathbf{y} \mid \mathbf{f}(\mathbf{x}; \transitionParams)) &= \mathbf{J}(\mathbf{x})^{T} \mathbf{r}(\mathbf{y} ; \mathbf{f}) \\
  \nabla^{2}_{\transitionParams\transitionParams} \log p(\mathbf{y} \mid \mathbf{f}(\mathbf{x}; \transitionParams)) &= \mathbf{H}(\mathbf{x})^{T}
  \mathbf{r}(\mathbf{y};\mathbf{f}) - \mathbf{J}(\mathbf{x})^{T} \bm\Lambda(\mathbf{y};\mathbf{f}) \mathbf{J}(\mathbf{x}),
\label{eq-hess}
\end{align}
through the Jacobian $\mathbf{J} \in \R^{C \times P}$ and Hessian $\mathbf{H} \in \R^{C \times P \times P}$ of the feature extractor $\mathbf{f}(\mathbf{x}; \bm\transitionParams)$,
\begin{align} \label{eq-jac}
[\mathbf{J}(\cdot)]_{ci} = \odv{f_{c}(\cdot ; \transitionParams)}{\transitionParams_{i}}_{\transitionParams=\transitionParams^{*}} \qquad
[\mathbf{H}(\cdot)]_{cij} = \odv{f_{c}(\cdot ; \transitionParams)}{\transitionParams_{i}\transitionParams_{j}}_{\transitionParams=\transitionParams^{*}}
\end{align}
where
$\mathbf{r}(\mathbf{y}; \mathbf{f}) = \nabla_{\mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$ can be interpreted as a residual and
$\bm\Lambda(\mathbf{y} ; \mathbf{f}) &= \nabla^{2}_{\mathbf{f} \mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$
as per-input noise.
Many approximations exist and can be used alongsied our method.
We refer the reader to \cite{daxbergerLaplace2021} for more details.
See \cref{sec-laplace-approximation} for further details on the Laplace approximation.

% \begin{align}
% \Sigma_{\transitionParams_{i}}^{-1} = - \nabla_{\transitionParams \transitionParams}^{2} \log p(\transitionParams \mid \dataset_{0:i})|_{\transitionParams=\transitionParams^{*}_{i}}
%   + \frac{1}{\sigma^{2}_{0}} \mathbf{I}
% \end{align}



\textbf{Weight space to function space}
% We can represent the log joint density in the function space,
% \begin{align} \label{eq-laplace-approx-function-space}
%   \sum_{n=1}^{N} \log p \left(y_{n} \mid f(\mathbf{x}_{n} ; \transitionParams_{0:i}) \right) + \log p(f(\mathbf{X}; \transitionParams_{0:i}))
% \end{align}
We can represent the weight space posterior from \cref{eq-laplace-approx-weight-space}, in
function space, by linearising the BNN and interpretting it as a GP.
The GP prior is then given by,
\begin{align} \label{eq-laplace-approx-function-space}
  % \transitionFn_{\transitionParams_{i}}(\cdot) \sim \mathcal{N} \left( \mu_{\transitionParams_{i}^{*}}(\cdot), K_{\transitionParams_{i}^{*}}(\cdot, \cdot') \right) \qquad
  f(\cdot ;\transitionParams) \sim \mathcal{N} \left( \mu(\cdot), k(\cdot, \cdot') \right) \qquad
  % \mu_{\transitionParams_{i}^{*}}(\cdot)
  \mu(\cdot)
  = f(\cdot ;\transitionParams^{*}) \qquad
  % K_{\transitionParams_{i}^{*}}(\cdot, \cdot')
  k(\cdot, \cdot')
  = \gamma^{2} \mathbf{J}(\cdot) \mathbf{J}(\cdot')^{T},
\end{align}
where the kernel is the neural tangent kernel (NTK) \parencite{immerImprovingPredictionsBayesian2021}.
\todo{what mean function shoudl this use???}
\todo{NTK times some factor??}
% with Jacobian given by $\mathbf{J}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\transitionParams}_{\transitionParams=\transitionParams_{i}^{*}}$.
% The kernel in \cref{eq-laplace-approx-function-space} is the neural tangent kernel (NTK)
% $k(\mathbf{x}, \mathbf{x}') = \sigma_{0}^{2} J_{\transitionParams^{*}}(\mathbf{x}) J_{\transitionParams^{*}}(\mathbf{x}')^{T}$
% \parencite{immerImprovingPredictionsBayesian2021}
% to formulate the GP posterior $p(f(\mathbf{X} ;\transitionParams_{0:i}) \mid \dataset_{0:i})$.
It is worth noting that the NTK linearises the network around the optimised parameters $\transitionParams_{i}^{*}$,
so the function space prior (and thus posterior) is only a locally linear approximation.
In contrast to conventional GP kernels, the NTK does not have any hyperparameters which need to be learned.
It is also worth noting that the NTK may be highly non stationary and is dependent on the NN architecture, for example, the activation functions.
Given our GP prior, we can use the properties of multivariate normals to obtain our GP posterior by conditioning on the data.
Our GP posterior is given by,
\todo{add mean func to GP posterior if we're using one}
\begin{align} \label{eq-gp-predictive-posterior}
  p(f(\mathbf{x}_{*}) \mid \mathbf{y}) &= \mathcal{N}
  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{x}\mathbf{x}}^{-1} \mathbf{A}^{T} \right) \\
  \left( f(\mathbf{x}_{*}) \mid \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} \mathbf{m}^{*},
  k_{**} - \mathbf{k}_{*\mathbf{x}} \left(\mathbf{K}_{\mathbf{x}\mathbf{x}} + \bm\Lambda(\mathbf{y} ; \mathbf{f})^{-1} \right)^{-1} \mathbf{k}_{*\mathbf{x}}^{T} \right)
\end{align}


\section{Dual SVGP} \label{sec:method}
The GP predictive posterior in \cref{eq-gp-predictive-posterior} is computationally expensive
as it requires inverting $N\times N$ matrix.
Sparse approximations using pseudo points ...
\begin{align} \label{eq-svgp-predictive-posterior}
  p(f(\mathbf{x}_{*}) \mid \mathbf{y}) &= \mathcal{N}
  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{x}\mathbf{x}}^{-1} \mathbf{A}^{T} \right) \\
  \left( f(\mathbf{x}_{*}) \mid \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} \mathbf{m}^{*},
  k_{**} - \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} \mathbf{k}_{*\mathbf{x}}^{T} \right) \\
  % &\approx \mathcal{N}
  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} \mathbf{A}^{T} + \mathbf{A} \mathbf{V}^{*} \mathbf{A}^{T} \right) \\
&\approx \mathcal{N} \left( f(\mathbf{x}_{*}) \mid \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{m}^{*},
  k_{**} - \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\mathbf{z}}^{T}
  % \mathbf{k}_{*\inducingInput} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\inducingInput}^{T}
  + \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}}  \mathbf{V}^{*}  \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\mathbf{z}}^{T}
  \right)
\coloneqq q_{\inducingVariable}(f(\mathbf{x}_{*}))
\end{align}
% where $\mathbf{A} = \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}}$

We follow \cite{csatoSparseOnlineGaussian2002} and express our GP posterior in the dual parameter space.
However, we consider the variational formulation introduced by \cite{adamDualParameterizationSparse2021}, where the optimal variational parameters are given by,
\begin{align} \label{eq-dual-params}
\mathbf{m}^{*} = \mathbf{V}^{*}\bm\alpha^{*} \quad \mathbf{V}^{*} = [\mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} + \bm\beta^{*}]
\end{align}
Importantly, we can calculate the natural parameters $(\bm\alpha, \bm\beta)$ in closed form,
\todo{paul needs to update \cref{eq-dual-svgp-params}}
\begin{align} \label{eq-dual-svgp-params}
\bm\alpha_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right] \\
\bm\beta_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right].
\end{align}
This simplifies SVGP inference \parencite{hensmanGaussian2013} as it no longer requires the variational parameters $(\mathbf{m}, \mathbf{V})$ to be optimised.

% Importantly, \cite{changFantasizingDualGPs2022} show that in the dual space,
% conditioning on new observations reduces to suming the dual variables from the previous time
% step with an update, given by,
% \todo{pick best way to show new data (using superscript new or just time indexing?)}
% \begin{align} \label{eq-dual-update-svgp}
% \bm\alpha_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right] \\
% \bm\beta_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right]
% \end{align}

% We follow \cite{csatoSparseOnlineGaussian2002} and express our GP posterior in the dual parameter space,
% % to write GP posterior $p(f(\mathbf{X} ;\transitionParams_{0:i}) \mid \dataset_{0:i})$ as,
% \begin{align} \label{eq-gp-post}
%   \mathbb{E}_{p(f(\mathbf{x}_{*}) \mid \mathbf{y}_{n})} \left[ f(\mathbf{x}_{*}) \right] &= \mu(\mathbf{x}_{*}) + \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%   \mathbb{V}_{p(f(\mathbf{x}_{*}) \mid \mathbf{y}_{n})} \left[ f(\mathbf{x}_{*}) \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \text{diag}(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
% \end{align}
% % \begin{align} \label{eq-laplace-approx-function-space}
% %   \mathbb{E}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
% %   \mathbb{V}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \diag(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
% % \end{align}
% where the dual parameters $(\bm\alpha_n, \bm\beta_n)$ are vectors of,
% \begin{align} \label{eq-gp-dual-params}
%   \bm\alpha_{n} \coloneqq \left\{ \mathbb{E}_{p(f(\mathbf{x}_{i}) \mid \mathbf{y}_{n})} \left[ \nabla_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \right\}_{i=1}^{N} \qquad
%   \bm\beta_{n} \coloneqq \left\{ \mathbb{E}_{p(f(\mathbf{x}_{i}) \mid \mathbf{y}_{n})} \left[ \nabla^{2}_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \right\}_{i=1}^{N}
% \end{align}
% Importantly, this parameterisation

% \textbf{dual-SVGP}
% \begin{align} \label{eq-svgp-post}
%   \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ f(\mathbf{x}_{*}) \right] &= \mu(\mathbf{x}_{*}) + \mathbf{k}_{\mathbf{z}*}^{T} \bm\alpha_{\mathbf{u}} \\
%   \mathbb{V}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ f(\mathbf{x}_{*}) \right] &= k_{**} - \mathbf{k}_{\mathbf{z}*}^{T} \left[ \mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} - \left( \mathbf{K}_{\mathbf{z} \mathbf{z}} + \bm\beta_{\mathbf{u}} \right)^{-1} \right] \mathbf{k}_{\mathbf{z}*}
% \end{align}
% % \begin{align} \label{eq-laplace-approx-function-space}
% %   \mathbb{E}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
% %   \mathbb{V}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \diag(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
% % \end{align}
% where the dual parameters $(\bm\alpha_{\mathbf{u}}, \bm\beta_{\mathbf{u}})$ are vectors of,
% \begin{align} \label{eq-svgp-dual-params}
%   \bm\alpha_{\mathbf{u}} \coloneqq \mathbf{k}_{\mathbf{z}i} \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ \nabla_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \qquad
%   \bm\beta_{\mathbf{u}} \coloneqq \mathbf{k}_{\mathbf{z}i} \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ \nabla^{2}_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \mathbf{k}_{\mathbf{z}i}^{T}
% \end{align}

% weight space posterior in \cref{eq-laplace-approx-weight-space} in
% function space by linearising the BNN and interpreting it as a GP.
% \begin{align} \label{eq-laplace-approx-function-space}
%   \transitionFn_{\transitionParams_{i}}(\cdot) &\sim \mathcal{N} \left( \mu_{\transitionParams_{i}^{*}}(\cdot), K_{\transitionParams_{i}^{*}}(\cdot, \cdot') \right) \quad
%   \mu_{\transitionParams_{i}^{*}}(\cdot) &= \transitionFn_{\transitionParams^{*}_{i}}(\cdot) \quad
%   K_{\transitionParams_{i}^{*}}(\cdot, \cdot') &= \mathbf{J}_{\transitionParams^{*}_{i}}(\cdot) \bm\Sigma_{\transitionParams^{*}_{i}} \mathbf{J}_{\transitionParams^{*}_{i}}(\cdot')^{T}
% \end{align}
% where the Jacobian is given by $\mathbf{J}_{\transitionParams^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\transitionParams}_{\transitionParams=\transitionParams_{i}^{*}}$.

% \begin{align} \label{eq-laplace-approx-jacobian}
% \mathbf{J}_{\transitionParams^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\transitionParams}_{\transitionParams=\transitionParams_{i}^{*}}
% \end{align}


% % Note that the posterior in \cref{eq-laplace-approx-weight-space} is in weight space.
% We can represent the weight space posterior in \cref{eq-laplace-approx-weight-space} in
% function space by linearising the BNN and interpreting it as a GP.
% \begin{align} \label{eq-laplace-approx-function-space}
%   \transitionFn_{\transitionParams_{i}^{*}}(\cdot) &\sim \mathcal{N} \left( \mu_{\transitionParams_{i}^{*}}(\cdot), K_{\transitionParams_{i}^{*}}(\cdot, \cdot') \right) \quad
%   \mu_{\transitionParams_{i}^{*}}(\cdot) &= \transitionFn_{\transitionParams^{*}_{i}}(\cdot) \quad
%   K_{\transitionParams_{i}^{*}}(\cdot, \cdot') &= \mathbf{J}_{\transitionParams^{*}_{i}}(\cdot) \bm\Sigma_{\transitionParams^{*}_{i}} \mathbf{J}_{\transitionParams^{*}_{i}}(\cdot')^{T}
% \end{align}
% where the Jacobian is given by $\mathbf{J}_{\transitionParams^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\transitionParams}_{\transitionParams=\transitionParams_{i}^{*}}$.

% \begin{align} \label{eq-laplace-approx-jacobian}
% \mathbf{J}_{\transitionParams^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\transitionParams}_{\transitionParams=\transitionParams_{i}^{*}}
% \end{align}


\subsection{Fast Updates}
Importantly, \cite{changFantasizingDualGPs2022} show that in the dual space,
conditioning on new observations reduces to suming the dual variables from the previous time
step with an update, given by,
\todo{pick best way to show new data (using superscript new or just time indexing?)}
\begin{align} \label{eq-dual-update-svgp}
\dualParam{1}^{t+1} &\leftarrow \dualParam{1}^{t} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))} \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right] \\
\dualParam{2}^{t+1} &\leftarrow \dualParam{2}^{t} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))}  \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right]
\end{align}
Importantly, for a single new observation $((\state_{t}, \action_{t}), \state_{t+1})$ this update has
complexity $\mathcal{O}(\numInducing^{2})$.
This is a significant improvement to naive GP conditioning, which has complexity $\mathcal{O}((\numDataNew + \numDataOld)^{3})$
and sparse GP conditioning, which has complexity $\mathcal{O}((\numDataNew + \numDataOld)\numInducing^{2})$.
\todo{double check these complexities are right and cite them/show equaitons}
It is worth highlighting that the complexity of \cite{changFantasizingDualGPs2022} update does not increase during an episode.

\subsubsection{Model-based RL}

\subsubsection{Efficiently Sampling Functions for Posterior Sampling}
\cite{wilsonEfficiently2020}
\cite{wilsonPathwise2021}

% \begin{align} \label{eq-dual-update-svgp}
%  \dualParam{1}^{\text{new}} &\leftarrow \dualParam{1}^{\text{old}} +
%   \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}))}
%  \left[ \log p(\state_{t+1}^{\text{new}} \mid \latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}) ) \right] \\
%  \dualParam{2}^{\text{new}} &\leftarrow \dualParam{2}^{\text{old}} +
%   \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}))}
%  \left[ \log p(\state_t^{\horizon+1} \mid \latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}) ) \right] \\
%  \dualParam{1}^{\horizon+1} &\leftarrow \dualParam{1}^{\horizon} +
%   \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
%  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right] \\
%  \dualParam{2}^{\horizon+1} &\leftarrow \dualParam{2}^{\horizon} +
%   \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
%  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right]
% \end{align}


\begin{assumption} \label{assumption-ntk-linearisation}
  Something about NTK being a linearisation around $\theta^{*}_{i}$ but each update moves away from $\theta^{*}_{i}$
\end{assumption}

\cite{rossellApproximateLaplaceApproximations2021}



\section{Experiments}
Environments
\begin{itemize}
  \item Compare updated posterior with non updated posterior
  \item Need to show update is fast enough for practical use?
  \begin{itemize}
    \item How frequently can we do the posterior update? Every time step or every $K$ steps?
    \item Update depends on number of inducing points?
    \item What experiments can we show for this? Wall clock time vs planning horizon with different numbers of inducing points?
  \end{itemize}
  \item Is linearising NN mean and using fast updates during episode better than keeping non-linearised mean and not updating?
  \item How does number of inducing points affect performance?
  \begin{itemize}
    \item More inducing points means better approximation but larger computational complexity.
  \end{itemize}
  \item How good is the assumption that we can linearise BNN to get NTK?
  \begin{itemize}
    \item Does model's performance get worse during an episode because linearisation becomes more wrong?
    \item SVGP does't have this issue so compare to SVGP in cartpole?
  \end{itemize}
  \item What length planning horizon to use?
  \begin{itemize}
    \item Too small (e.g. $\Horizon=0$) would mean no exploration?
    \item Can $\Horizon$ be too large?
  \end{itemize}
  \begin{itemize}
  \item Compare for different strategies of making decisions under dynamic model's uncertainty:
    \begin{itemize}
      \item \textbf{Greedy exploitation} \(\pi_{\text{greedy}}\)
      \item \textbf{Hallucinated-UCRL} \(\pi_{\text{HUCRL}}\)
      \item \textbf{Thompson sampling} \(\pi_{\text{TS}}\)
    \end{itemize}
  \end{itemize}
  \item Compare impact of stationary vs non-stationary priors
  \begin{itemize}
    \item First do this with GP
    \item Then try to do with Laplace BNN
  \end{itemize}
  \item Compare impact of function vs weight space
\end{itemize}

\section{Conclusion} \label{sec:conclusion}


\section*{Broader Impact}

\section*{Acknowledgements}
Aidan Scannell is funded by the Finnish Center for Artificial Intelligence.

% \section*{References}
\small
\printbibliography
\normalsize
% TODO make bibliography small a better way

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip



\appendix

\section{Appendix}

Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.

\section{Model-based Reinforcement Learning with Fast Updates}


\subsection{Problem Statement and Background} \label{sec:problem-statement}
We consider environments with states \(\state \in \stateDomain \), actions \(\action \in \actionDomain\) and transition dynamics \(\transitionFn: \stateDomain \times \actionDomain \rightarrow \stateDomain \) subject to
iid noise \(\noise_{t}\), given by,
\begin{align}
\state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t}.
\end{align}
\begin{assumption}
  (system properties) The environment's dynamics are $L_{\transitionFn}\text{-Lipschitz}$ continuous and the transition noise $\noise_{t}$ is $\sigma\text{-sub-Gaussian}$ for all $t \geq 0$.
\end{assumption}

\subsection{Model-based Reinforcement Learning (RL)}
The goal of reinforcement learning is to find a policy \(\pi \in \Pi\) that maximises the sum of discounted
rewards in expecation under the transition noise (aleatoric uncertainty),
\begin{align} \label{eq-model-free-objective}
\policy^{*} = \arg \max_{\policy \in \policyDomain} J(\transitionFn, \policy) = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\noise_{0:\infty}} \left[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \right],
\end{align}
where $\gamma \in [0, 1]$

\textbf{Model-based}
In Bayesian model-based RL, we obtain the posterior over the dynamics \(p(f\mid\mathcal{D})\) after performing (approximate) Bayesian
inference given a state transition data set \(\mathcal{D} = \{\{(s_{t},a_{t}), s_{t+1}\}^{T_{i}}_{t=1}\}_{i=0}^{N}\).
\cref{alg-mbrl} shows the typical model-based RL loop.
Importantly, the dynamics are usually only updated after an episode $i$.

\begin{algorithm}[!t]
\caption{Model-based RL}\label{alg-mbrl}
\begin{algorithmic}[1]
  \Require Start state $\state_{0}$, initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
\For{$i  \in \{1, 2, \ldots, \text{num episodes} \}$}
    \State Reset the system to $\state_{0}$ and reset trajectory buffers $\bm\tau_{t} = \emptyset \ \forall t$
    \For{$t  \in \{1, 2, \ldots, \text{num steps} \}$}
      % \State Collect  $\tau_{0:t} = \tau_{0:t-1} \cup (\state_{j}, \action_{j}, \state_{j+1}, r_{j+1})$
      \State Use \cref{eq-greedy}/\cref{eq-posterior-sampling}/\cref{eq-ucrl} to collect data $\bm\tau_{t} = \bm\tau_{t-1} \cup (\state_{t}, \policy_{i}(\state_{t}), \transitionFn(\state_{t}, \policy_{i}(\state_{t})), r_{t+1})$
      % \State Execute policy $\policy_{i}(\state_{t})$ in environment and update trajectory $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
    \EndFor
    \State Update data set $\dataset_{0:i} = \dataset_{0:i-1} \cup \tau$
    \State Train dynamics $p(\transitionFn \mid \dataset_{0:i}) \leftarrow \text{update\_dynamics}(\dataset_{0:i}, p(\transitionFn \mid \dataset_{0:i-1}))$
    % \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$ using $\dataset_{0:i+1}$
    % \State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
    \State Improve policy $\pi_{i+1} \leftarrow \text{update\_policy}(p\left(\transitionFn \mid \dataset_{0:i}), \dataset_{0:i} \right)$
    %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
\EndFor
\end{algorithmic}
\end{algorithm}

% \begin{minipage}{0.499\textwidth}
% \begin{algorithm}[H]
% \caption{Model-based RL}\label{alg-mbrl}
% \begin{algorithmic}[1]
%   \Require Initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
% \For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
%     \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
%       \State Execute policy $\policy_{i}(\state_{t})$ in environment
%       \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%     \EndFor
%     \State Update data set $\mathcal{D}_{0:i+1} = \mathcal{D}_{0:i} \cup \tau_{i+1}$
%     \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$
%     \State Improve policy $\pi_{i+1}$
%     %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \hfill
% \begin{minipage}{0.499\textwidth}
% \begin{algorithm}[H]
% \caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
% \begin{algorithmic}[1]
%   \Require Initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%     % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
% \For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
%     \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
%       \State Execute policy $\policy_{i}(\state_{t})$ in environment
%       % \State Append transition $\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$ to trajectory $\tau_{i}$
%       \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%       \State {\color{blue}Update dynamics $p(\transitionFn \mid \dataset_{0:i} \cup \tau_{i+1})$}
%     \EndFor
%     \State Update data set $\mathcal{D}_{0:i+1} = \mathcal{D}_{0:i} \cup \tau_{i+1}$
%     \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$
%     \State Improve policy $\pi_{i+1}$
%     %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}



\subsection{Exploration Strategies}
\textbf{Greedy exploitation}
Given the posterior dynamics \(p(\transitionFn \mid \mathcal{D})\),
a common approach is to simply take the expecation over both the aleatoric and epistemic uncertainty,
\begin{align} \label{eq-greedy}
\policy_{i+1}^{\text{greedy}} = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\transitionFn \sim p(\transitionFn \mid \dataset_{0:i})} \left[ J(\transitionFn, \policy) \right],
\end{align}
This approach has been widely adopted, for example, in PILCO, PETS, GP-MPC
\cite{deisenrothPILCO2011,chuaDeepReinforcementLearning2018,kamtheDataEfficient2018}.
This approach helps to alleviate model bias as the posterior ``knows what the model does not know''.
This is because the predictive posterior \(p(f(s_{t},a_{t}) \mid (s_{t},a_{t}),  \mathcal{D} )\) will be (or should be) uncertain when making
predictions far away from the training data.
The expectation considers all possible dynamics models which prevents the policy optimisation from
exploiting innacuracies in the model.
This approach has no guarantees for exploration in the general case.
However, under specific dynamics and reward structures (e.g. PILCO) this objective can achieve sublinear regret.
\todo{need to double check sublinear regret statement. And give a reference}


\textbf{Posterior sampling}
\cite{osbandWhyPosteriorSampling2017,osbandMoreEfficientReinforcement2013}
\begin{align} \label{eq-posterior-sampling}
\policy_{i+1}^{\text{PS}} = \arg \max_{\policy \in \policyDomain} \left[ J(\transitionFn, \policy) \right] \quad \text{s.t. } \transitionFn \sim p(\transitionFn \mid \dataset_{0:i})
\end{align}

\textbf{Hallucinated upper confidence RL}
A more theoretically grounded exploration strategy is UCRL \autocite{jakschNearoptimal2010}, which optimises joinly over
policies and models inside the set
\(\mathcal{M} = \{ f \mid | f(s,a) - \mu_{i}(s, a) | \leq \beta_{i} \Sigma_{i}(s, a) \quad \forall s, a \in \mathcal{S} \times \mathcal{A} \}\), representing all statistically plausible
models under the posterior \(p(f(s,a) \mid \mathcal{D}_{0:i} \cup (s,a)) = \mathcal{N}(f(s,a) \mid \mu_{i}(s,a), \Sigma_{i}(s,a))\) at episode \(i\).
This strategy is given by,
\begin{align} \label{eq-ucrl}
\policy_{i+1}^{\text{UCRL}} = \arg \max_{\policy \in \policyDomain} \max_{\transitionFn \in \mathcal{M}} J(\transitionFn, \policy).
\end{align}
This strategy optimises an optimistic policy over the set of plausible dynamics models.
Although this joint optimisation is intractable in general,
\cite{curiEfficient2020} proposed a practical alternative which is detailed in \cref{sec-hucrl}.

\textbf{MPC vs policy learning}
It is worth noting that the strategies in \cref{eq-greedy,eq-posterior-sampling,eq-ucrl} can be used with both model predictive control (MPC)
techniques, such as the cross entoropy method (CEM), and model-free RL techniques, such as soft actor-critic (SAC).


In this work we are interested in how we can use \(p(f \mid \mathcal{D})\) to alleviate some of the issues in model-based RL,
for example, model bias and the exploration-exploitation trade-off.



\todo{show how to get new $\dualParam{1}$ and $\dualParam{2}$ in Train dynamics line of \cref{alg-mbrl-fast-updates}}
\begin{algorithm}[!t]
\caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
\begin{algorithmic}[1]
  \Require Start state $\state_{0}$, initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$ (inc. dual parameters $\dualParam{1}, \dualParam{2}$), policy $\policy_{0}$
    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
\For{$i  \in \{1, 2, \ldots, \text{num episodes} \}$}
    \State Reset the system to $\state_{0}$ and reset trajectory buffers $\bm\tau_{t} = \emptyset \ \forall t$
    \For{$t  \in \{1, 2, \ldots, \text{num steps} \}$}
      % \State Execute policy $\policy_{i}(\state_{t})$ (\cref{eq-fast-update-mpc}) in environment
      \State Use \cref{eq-fast-update-mpc} to collect data $\bm\tau_{t} = \bm\tau_{t-1} \cup (\state_{t}, \policy_{i}(\state_{t}), \transitionFn(\state_{t}, \policy^{\text{fast}}_{i}(\state_{t})), r_{t+1})$
      % \State Append transition $\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$ to trajectory $\tau_{i}$
      % \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
      %\State {\color{blue}Update dynamics $p(\transitionFn \mid \dataset_{0:i} \cup \tau_{i+1})$}
      \State {\color{blue}Update dynamics posterior using \cref{eq-dual-update-svgp}, i.e. fast update}
      % \begin{align}
      % \dualParam{1}^{t+1} &\leftarrow \dualParam{1}^{t} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))} \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right] \\
      % \dualParam{2}^{t+1} &\leftarrow \dualParam{2}^{t} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))}  \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right]
      % \end{align}}
    \EndFor
    \State Update data set $\dataset_{0:i} = \dataset_{0:i-1} \cup \tau$
    \State Train dynamics $p(\transitionFn \mid \dataset_{0:i}) \leftarrow \text{update\_dynamics}(\dataset_{0:i}, p(\transitionFn \mid \dataset_{0:i-1}))$
    \State Improve policy $\pi^{\text{fast}}_{i+1} \leftarrow \text{update\_policy}(p\left(\transitionFn \mid \dataset_{0:i}), \dataset_{0:i} \right)$
\EndFor
\end{algorithmic}
\end{algorithm}


\subsubsection{Fast updates}


In this section we extend these fast updates to environment's with high dimensional state spaces and large data sets.
% Our method draws on the connection between BNNs and GPs and formulates a function space SVGP posterior given
Our method uses a BNN dynamic model and draws on the connection between BNNs and GPs
\parencite{khanApproximate2019} to formulate a function space SVGP posterior,
where we can apply the fast updates from \cref{eq-dual-update-svgp}.
At a high-level, we first use Laplace's approximation to obtain a weight space posterior for our BNN.
We then linearise our BNN around the optimal parameters and interpret it as a GP.
Finally, we formulate a lower rank approximation of this GP posterior (i.e. a SVGP posterior) using inducing variables.
% formulates a function space SVGP posterior by drawing on the connection between BNNs and GPs.


The strategies in \cref{eq-greedy,eq-posterior-sampling,eq-hucrl} do not update the dynamic model during an episode.
A better approach would be to update the posterior at every time step during an episode, for example,
\begin{subequations}
\begin{align} \label{eq-fast-update-mpc}
  \policy_{i+1}^{\text{greedy}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
\E_{p(\transitionFn \mid \dataset_{0:i})} \left[J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) \right] + \stateValueFn(\state_{\Horizon+1}) \\
  \policy_{i+1}^{\text{PS}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) + \stateValueFn(\state_{\Horizon+1}) \quad \text{s.t. } \transitionFn \sim p(\transitionFn \mid \dataset_{0:i} \cup \bm\tau_{t}) \\
  \policy_{i+1}^{\text{UCRL}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}} \max_{\transitionFn \in \mathcal{M}}
J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) + \stateValueFn(\state_{\Horizon+1}) \quad \text{s.t. } \mathcal{M} = \{\transitionFn(\state_{t},\action_{t}) - \mu_{i,t}(\state_{t},\action_{t}) \leq \beta_{i,t} \Sigma_{i,t}(\state_{t}, \action_{t})\} \\
  \stateValueFn(\state) &= \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right] \label{eq-value-fn}
\end{align}
\end{subequations}
\begin{subequations}
\begin{align} \label{eq-fast-update-mpc-old}
  \policy^{\text{fast}}(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}}
  \mathbb{E}_{\state_{\horizon} \sim p(\state_{\horizon+1} \mid \transitionFn(\state_{\horizon}, \action_{\horizon}))} \left[ \sum_{\horizon=0}^{\Horizon}     \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \mid \state_{0}=\state \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
  \stateValueFn(\state) &= \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right] \label{eq-fast-update-mpc}
\end{align}
\end{subequations}

\begin{align} \label{}
  \policy(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}} \max_{\optimisticTransition \in \optimisticTransitionSet}
  \sum_{\horizon=0}^{\Horizon}  \mathbb{E}_{\noise_{\horizon}} \left[  \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
  \text{s.t. } \state_{\horizon+1} &= \optimisticTransition(\state_{\horizon}, \action_{\horizon}) + \noise_{\horizon} \\
  \optimisticTransition(\state_{\horizon}, \action_{\horizon}) &=
\optimisticTransitionMean(\state_{\horizon}, \action_{\horizon}) \pm \beta_{i}
\optimisticTransitionCov(\state_{\horizon}, \action_{\horizon})
\end{align}

\input{proof.tex}


\section{Hallucinated Upper Confidence Reinforcement Learning (H-UCRL)} \label{sec-hucrl}
\cite{curiEfficient2020} introduced a tractable approximation which retains some of the theoretical guarantees whilst
being applicable with deep model-based RL.
They introduce a function \(\eta: \mathcal{S} \times \mathcal{A} \rightarrow [-1, 1]^{p}\) which acts as a hallucinated control input.
The strategy is given by,
\begin{align} \label{eq-hucrl}
\pi_i^{\text{UCRL}} = \arg \max_{\policy \in \policyDomain} \max_{\eta(\cdot) \in [-1,1]} J(\transitionFn, \policy) \quad \text{s.t.} \quad \transitionFn = \mu_{i}(\state_{t}, \action_{t}) + \beta_{i} \Sigma_{i}(\state_{t}, \action_{t}) \eta(\state_{t},\action_{t}).
\end{align}
Intuitively, \(\eta(\state,\action) \in [-1,1]\) enables the optimisation to select any dynamics model
\(\transitionFn\) within \(\pm \beta \Sigma_{i}(\state_{t}, \action_{t})\) of the posterior mean \(\mu_{i}(\state_{t}, \action_{t})\).

\section{Laplace Approximation} \label{sec-laplace-approximation}


\section{Template stuff}
\subsection{Generate TikZ Figures from Python}
We can generate figures in \texttt{.tex} format directly from Python:
\begin{verbatim}
tikzplotlib.save("fig.tex", axis_width="\\figurewidth", axis_height="\\figureheight")
\end{verbatim}
\cref{fig:example} shows that we get nicely formatted lables/titles/etc when we include them in our paper.
\begin{figure}[h]
    \centering\footnotesize

    % Set your figure size here
    \setlength{\figurewidth}{.33\textwidth}
    \setlength{\figureheight}{.75\figurewidth}

    % Customize your plot here
    % (scale only axis applies the size to the axis box and not entire figure)
    \pgfplotsset{grid style={dotted},title={Foo},scale only axis}

    % Use the subcaption package (= subfigure) for sub-plots, that is
    % plot the separate plots separately in Python
    \begin{subfigure}{.4\textwidth}
        \centering
        \input{./figs/example_fig.tex}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.4\textwidth}
        \centering
        \input{./figs/example_fig.tex}
    \end{subfigure}
    \caption{Foo}
    \label{fig:example}
\end{figure}

\subsection{Generate Tables from Python}
We can also generate tables straight from python using \href{https://github.com/astanin/python-tabulate}{tabulate}:
\begin{verbatim}
table = [["Sun",696000,1989100000],["Earth",6371,5973.6],
        ["Moon",1737,73.5],["Mars",3390,641.85]]
headers = ["Planet","R (km)", "mass (x 10^29 kg)"]
table = tabulate(table, headers=headers, tablefmt="latex")
with open("table.tex", 'w') as file:
    file.write(table)
\end{verbatim}

\begin{table}[h]
    \centering
    \input{./tables/example_table.tex}
\end{table}

\subsection{Biblatex}
Rember when using biblatex to use 'parencite' for \parencite{kamtheDataEfficient2018} and when using natbib to use 'citep'.


\end{document}
