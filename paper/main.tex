\documentclass{article}

%\title{Investigatin Uncertainty Quantification in Model-based Reinforcement Learning}
\title{Model-based Reinforcement Learning with Fast Posterior Updates}
\author{%
  Aidan ~Scannell \\
  Aalto University\\
  %Finnish Center for Artificial Intelligence \\
  \texttt{aidan.scannell@aalto.fi}
  \And
  Paul ~Chang \\
  Aalto University\\
  \texttt{paul.chang@aalto.fi}
  \And
  Ella ~Tamir \\
  Aalto University\\
  \texttt{ella.tamir@aalto.fi}
  \And
  Arno ~Solin \\
  Aalto University\\
  \texttt{arno.solin@aalto.fi}
  \And
  Joni ~Pajarinen \\
  Aalto University\\
  \texttt{joni.pajarinen@aalto.fi}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


% NeurIPS packages
\usepackage[preprint,nonatbib]{neurips_2022}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Bibliography
\usepackage[maxcitenames=1, maxbibnames=4, doi=false, isbn=false, eprint=true, backend=bibtex, hyperref=true, url=false, style=authoryear-comp]{biblatex}
\addbibresource{zotero-library.bib}
% \addbibresource{paper/zotero-library.bib}

% Our packages
\usepackage{todonotes}
\usepackage[colorlinks=true,linkcolor=blue,allcolors=blue]{hyperref}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{tikz,pgfplots}
\usepackage{subcaption}
\usetikzlibrary{}

% This must be imported last!
%\usepackage{cleveref}
\usepackage[capitalise,nameinlink]{cleveref}

% Config for Arno's awesome TikZ plotting stuff
\newlength{\figurewidth}
\newlength{\figureheight}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\action}{\ensuremath{\mathbf{a}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}

% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
\newcommand{\transitionParams}{\ensuremath{\phi}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transition}{\ensuremath{\mathbf{d}_{\transitionParams}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}

\begin{document}

\maketitle

\begin{abstract}
  %Reinforcement learning (RL) agents typically perform learning after an episode and do not utilise the information gained during an episode.
  Reinforcement learning (RL) agents typically perform learning after an episode and do not utilise the information gained during an episode.
  We present a Bayesian model-based RL algorithm which incorporates this information by updating a dynamic model's posterior during an episode.
  Importantly, our dynamic model (and it's converge guarantees) scales to high dimensional problems via neural networks with stationary priors.
  We demonstrate that our fast posterior updates lead to improved sample efficiency in environments with action penalties, a notoriously difficult
  challenge for model-based RL algorithms.

  % Model-based reinforcement learning (RL) typically learns
\end{abstract}

\section{Introduction} \label{sec:intro}

\section{Problem Statement and Background} \label{sec:problem-statement}
We consider environments with states \(\state \in \stateDomain \), actions \(\action \in \actionDomain\) and transition dynamics \(\transitionFn: \stateDomain \times \actionDomain \rightarrow \stateDomain \) subject to
iid noise \(\noise_{t}\), given by,
\begin{align}
\state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t}.
\end{align}

\textbf{Reinforcement learning (RL)}
The goal of reinforcement learning is to find a policy \(\pi \in \Pi\) that maximises the sum of discounted
rewards in expecation under the transition noise (aleatoric uncertainty),
\begin{align} \label{eq-model-free-objective}
\policy^{*} = \arg \max_{\policy \in \policyDomain} J(\transitionFn, \policy) = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\noise_{0:\infty}} \left[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \right].
\end{align}

\textbf{Model-based RL}
In Bayesian model-based RL, we obtain the posterior over the dynamics \(p(f\mid\mathcal{D})\) after performing (approximate) Bayesian
inference given a state transition data set \(\mathcal{D} = \{\{(s_{t},a_{t}), s_{t+1}\}^{T_{i}}_{t=1}\}_{i=0}^{N}\).
In this work we are interested in how we can use \(p(f \mid \mathcal{D})\) to alleviate some of the issues in model-based RL,
for example, model bias and the exploration-exploitation trade-off.

\textbf{Greedy exploitation}
Given the posterior dynamics \(p(f \mid \mathcal{D})\),
a common approach is to simply take the expecation over both the aleatoric and epistemic uncertainty,
\begin{align} \label{eq-greedy}
\pi_{\text{greedy}} = \arg \max_{\pi} \mathbb{E}_{f \sim p(f \mid \mathcal{D})} \left[ J(f, \pi) \right],
\end{align}
This approach has been widely adopted, for example, in PILCO, PETS, GP-MPC

\cite{deisenrothPILCO2011,chuaDeepReinforcementLearning2018,kamtheDataEfficient2018}.
This approach helps to alleviate model bias as the posterior ``knows what the model does not know''.
This is because the predictive posterior \(p(f(s_{t},a_{t}) \mid (s_{t},a_{t}),  \mathcal{D} )\) will be (or should be) uncertain when making
predictions far away from the training data.
The expectation considers all possible dynamics models which prevents the policy optimisation from
exploiting innacuracies in the model.
This approach has no guarantees for exploration in the general case.
However, under specific dynamics and reward structures (e.g. PILCO) this objective can achieve sublinear regret.
\todo{need to double check sublinear regret statement. And give a reference}


\textbf{Hallucinated upper confidence RL}
A more theoretically grounded exploration strategy is UCRL \autocite{jakschNearoptimal2010}, which optimises joinly over
policies and models inside the set
\(\mathcal{M} = \{ f \mid | f(s,a) - \mu_{i}(s, a) | \leq \beta_{i} \Sigma_{i}(s, a) \quad \forall s, a \in \mathcal{S} \times \mathcal{A} \}\), representing all statistically plausible
models under the posterior \(p(f(s,a) \mid \mathcal{D}_{0:i} \cup (s,a)) = \mathcal{N}(f(s,a) \mid \mu_{i}(s,a), \Sigma_{i}(s,a))\) at episode \(i\).
This strategy is given by,
\begin{align}
\pi_{\text{UCRL}} = \arg \max_{\pi} \max_{f \in \mathcal{M}} J(f, \pi).
\end{align}
This strategy optimises an optimistic policy over the set of plausible dynamics models.
However, the joint optimisation is intractable in general.
\cite{curiEfficient2020} introduced a tractable approximation which retains some of the theoretical guarantees whilst
being applicable with deep model-based RL.
They introduce a function \(\eta: \mathcal{S} \times \mathcal{A} \rightarrow [-1, 1]^{p}\) which acts as a hallucinated control input.
The strategy is given by,
\begin{align} \label{eq-hucrl}
\pi_{\text{UCRL}} = \arg \max_{\pi} \max_{\eta(\cdot) \in [-1,1]} J(f, \pi) \quad \text{s.t.} \quad f = \mu_{i}(s_{t}, a_{t}) + \beta \Sigma_{i}(s_{t}, a_{t}) \eta(s_{t},a_{t}).
\end{align}
Intuitively, \(\eta(s,a) \in [-1,1]\) enables the optimisation to select any dynamics model
\(f\) within \(\pm \beta \Sigma_{i}(s_{t}, a_{t})\) of the posterior mean \(\mu_{i}(s_{t}, a_{t})\).

\textbf{MPC vs policy learning}
It is worth noting that the strategies in \cref{eq-hucrl,eq-greedy} can be used with both model predictive control (MPC)
techniques, such as the cross entoropy method (CEM), and model-free RL techniques, such as soft actor-critic (SAC).


\section{Model-based RL with Fast Bayesian Updates}
The strategy in \cref{eq-hucrl} does not update the dynamics model during an episode.
A better approach would be to update the posterior at every time step during an episode, for example,
\begin{align} \label{eq-objective-with-online-updates}
\pi_{\text{online}} = \arg \max_{\pi} \mathbb{E}_{\bm\epsilon_{0:\infty}} \left[ \sum_{t=0}^{K-1}
\mathbb{E}_{\underbrace{f \sim p(f \mid \mathcal{D} \cup \{(s_{j}, a_{j}), s_{j+1}\}_{j=0}^{t-1})}_{\text{updated posterior}}} \left[ \gamma^{t} r(s_{t}, a_{t}) \mid s_{0}=s \right] \right] +
\gamma^{K} Q(s_{K},a_{K})
\end{align}
We can do this with GPs becuase they are non parameteric but doing it with BNNs is harder. But Me and Paul think we can do it.

The idea is then to compare all of the ideas in the previous paper with and without updating the dynamics during an episode.

\begin{align} \label{eq-fast-update-mpc}
  \policy(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}}
  \mathbb{E}_{\state_{\horizon} \sim p(\state_{\horizon+1} \mid \transitionFn(\state_{\horizon}, \action_{\horizon}))} \left[ \sum_{\horizon=0}^{\Horizon}     \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \mid \state_{0}=\state \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1})
\end{align}

\begin{align} \label{eq-fast-update-mpc}
  \stateValueFn(\state) = \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right]
\end{align}

\begin{align} \label{eq-fast-update-mpc}
  \policy(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}} \max_{\optimisticTransition \in \optimisticTransitionSet}
  \sum_{\horizon=0}^{\Horizon}  \mathbb{E}_{\noise_{\horizon}} \left[  \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
  \text{s.t. } \state_{\horizon+1} &= \optimisticTransition(\state_{\horizon}, \action_{\horizon}) + \noise_{\horizon} \\
  \optimisticTransition(\state_{\horizon}, \action_{\horizon}) &=
\optimisticTransitionMean(\state_{\horizon}, \action_{\horizon}) \pm \beta_{i}
\optimisticTransitionCov(\state_{\horizon}, \action_{\horizon})
\end{align}

\begin{align} \label{eq-svgp-predictive-posterior}
  q_{\inducingVariable}(\transitionFn(\state_{t}, \action_{t})) = \mathcal{N}
  \left( \transitionFn(\state_{t}, \action_{t}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} \mathbf{A}^{T} + \mathbf{A} \mathbf{V}^{*} \mathbf{A}^{T} \right)
\end{align}

\begin{align} \label{eq-matrix-A}
  \mathbf{A} = \mathbf{K}_{\mathbf{X}\mathbf{Z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}}
\end{align}

\begin{align} \label{eq-dual-params}
  \mathbf{m}^{*} = \mathbf{V}^{*}\dualParam{1}^{*} \quad \mathbf{V}^{*} = [\mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} + \dualParam{2}^{*}]
\end{align}

\begin{align} \label{eq-dual-update-svgp-old-new}
 \dualParam{1}^{\text{new}} &\leftarrow \dualParam{1}^{\text{old}} +
  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\text{new}}, \action_{\text{new}}))}
 \left[ \log p(\state_{\text{new}} \mid \latentFn(\state_{\text{new}}, \action_{\text{new}}) ) \right] \\
 \dualParam{2}^{\text{new}} &\leftarrow \dualParam{2}^{\text{old}} +
  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\text{new}}, \action_{\text{new}}))}
 \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\text{new}}, \action_{\text{new}}) ) \right]
\end{align}

\begin{align} \label{eq-dual-update-svgp}
 \dualParam{1}^{\horizon+1} &\leftarrow \dualParam{1}^{\horizon} +
  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
 \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right] \\
 \dualParam{2}^{\horizon+1} &\leftarrow \dualParam{2}^{\horizon} +
  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
 \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right]
\end{align}

\begin{algorithm}[H]
\caption{Model-based RL}\label{alg-mbrl}
\begin{algorithmic}[1]
    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
\For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
    \State Train dynamics using $\dataset_{0:i}$
    \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
      \State Execute policy $\policy(\state_{t})$ in environment
      \State Set $\mathcal{D}_{i} = \mathcal{D}_{i} \cup (\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$
    \EndFor
    \State Update data $\mathcal{D}_{0:i} = \mathcal{D}_{0:i-1} \cup \mathcal{D}_{i}$
\EndFor
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
\caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
\begin{algorithmic}[1]
    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
\For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
    \State Train dynamics using $\dataset_{0:i}$
    \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
      \State Execute policy $\policy(\state_{t})$ in environment
      \State Set $\mathcal{D}_{i} = \mathcal{D}_{i} \cup (\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$
      \State {\color{blue}Update dynamics posterior}
    \EndFor
    \State Update data $\mathcal{D}_{0:i} = \mathcal{D}_{0:i-1} \cup \mathcal{D}_{i}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{minipage}{0.499\textwidth}
\begin{algorithm}[H]
\caption{Model-based RL}\label{alg-mbrl}
\begin{algorithmic}[1]
  % \Require{Start state $\state_0$, initial data set $\dataset_0$, policy $\pi_{0}$, dynamic model}
    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
\For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
    \State Train dynamics using $\dataset_{0:i}$
    \State Train value function $\stateValueFn$ using $\dataset_{0:i}$
    \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
      % \State Calculate $\action_{t}$ using \cref{eq-fast-update-mpc}
      % \State Execute 1 step in environment using $\policy(\state_{t})$ and observe $(\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$
      \State Execute policy $\policy(\state_{t})$ in environment
      \State Set $\mathcal{D}_{i} = \mathcal{D}_{i} \cup (\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$
    \EndFor
    \State Update data $\mathcal{D}_{0:i} = \mathcal{D}_{0:i-1} \cup \mathcal{D}_{i}$
\EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.499\textwidth}
\begin{algorithm}[H]
\caption{Model-based RL with fast updates}\label{alg-mbrl-with-fast-updates-simple}
\begin{algorithmic}[1]
  % \Require{Start state $\state_0$, initial data set $\dataset_0$, policy $\pi_{0}$, dynamic model}
    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
\For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
    \State Train dynamics using $\dataset_{0:i}$
    \State Train value function $\stateValueFn$ using $\dataset_{0:i}$
    \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
      \State Execute policy $\policy(\state_{t})$ in environment
      \State Set $\mathcal{D}_{i} = \mathcal{D}_{i} \cup (\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$
      \State {\color{blue}Update dynamics posterior}
    \EndFor
    \State Update data $\mathcal{D}_{0:i} = \mathcal{D}_{0:i-1} \cup \mathcal{D}_{i}$
\EndFor
\end{algorithmic}
\end{algorithm}
\end{minipage}

\begin{algorithm}[!t]
\caption{Model-based RL with fast updates}\label{alg-mbrl-with-fast-updates}
\begin{algorithmic}[1]
  % \Require{Start state $\state_0$, initial data set $\dataset_0$, policy $\pi_{0}$, dynamic model}
    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
\For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
    \State Train dynamics using $\dataset_{0:i}$ to get $p(\mathbf{\transitionFn} \mid \dataset_{0:i})$
    \State Train value function $\stateValueFn$ using $\dataset_{0:i}$
    % \While{not converged}
    %     \State Sample $N_{b}$ state transitions $\mathcal{B} \sim \dataset_{0:i}$
    %     \State Update dynamics using with $\mathcal{B}$
    % \EndWhile
    \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
      \State Calculate $\action_{t}$ using \cref{eq-fast-update-mpc}
      \State Execute $\action_{t}$ in environment and observe $(\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$
      \State Update dynamics posterior (i.e. fast update via dual parameters)
      \begin{align}
      \dualParam{1}^{\horizon+1} &\leftarrow \dualParam{1}^{\horizon} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))} \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right] \\
      \dualParam{2}^{\horizon+1} &\leftarrow \dualParam{2}^{\horizon} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right]
      \end{align}
    \EndFor
    % \State Calculate $\action_{\horizon}$ using \cref{eq-fast-update-mpc}
    % \State Execute $\action_{\horizon}$ in environment and observe $(\state_{\horizon}, \action_{\horizon}, \state_{\horizon+1}, r_{\horizon+1})$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experiments}
Environments
\begin{itemize}
  \item Compare updated posterior with non updated posterior
  \begin{itemize}
  \item Compare for different strategies of making decisions under dynamic model's uncertainty:
    \begin{itemize}
      \item \textbf{Greedy exploitation} \(\pi_{\text{greedy}}\)
      \item \textbf{Hallucinated-UCRL} \(\pi_{\text{HUCRL}}\)
      \item \textbf{Thompson sampling} \(\pi_{\text{TS}}\)
    \end{itemize}
  \end{itemize}
  \item Compare impact of stationary vs non-stationary priors
  \begin{itemize}
    \item First do this with GP
    \item Then try to do with Laplace BNN
  \end{itemize}
  \item Compare impact of function vs weight space
\end{itemize}

\section{Conclusion} \label{sec:conclusion}


\section*{Broader Impact}

\section*{Acknowledgements}
Aidan Scannell is funded by the Finnish Center for Artificial Intelligence.

% \section*{References}
\small
\printbibliography
\normalsize
% TODO make bibliography small a better way

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip



\section{Template stuff}
\subsection{Generate TikZ Figures from Python}
We can generate figures in \texttt{.tex} format directly from Python:
\begin{verbatim}
tikzplotlib.save("fig.tex", axis_width="\\figurewidth", axis_height="\\figureheight")
\end{verbatim}
\cref{fig:example} shows that we get nicely formatted lables/titles/etc when we include them in our paper.
\begin{figure}[h]
    \centering\footnotesize

    % Set your figure size here
    \setlength{\figurewidth}{.33\textwidth}
    \setlength{\figureheight}{.75\figurewidth}

    % Customize your plot here
    % (scale only axis applies the size to the axis box and not entire figure)
    \pgfplotsset{grid style={dotted},title={Foo},scale only axis}

    % Use the subcaption package (= subfigure) for sub-plots, that is
    % plot the separate plots separately in Python
    \begin{subfigure}{.4\textwidth}
        \centering
        \input{./figs/example_fig.tex}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.4\textwidth}
        \centering
        \input{./figs/example_fig.tex}
    \end{subfigure}
    \caption{Foo}
    \label{fig:example}
\end{figure}

\subsection{Generate Tables from Python}
We can also generate tables straight from python using \href{https://github.com/astanin/python-tabulate}{tabulate}:
\begin{verbatim}
table = [["Sun",696000,1989100000],["Earth",6371,5973.6],
        ["Moon",1737,73.5],["Mars",3390,641.85]]
headers = ["Planet","R (km)", "mass (x 10^29 kg)"]
table = tabulate(table, headers=headers, tablefmt="latex")
with open("table.tex", 'w') as file:
    file.write(table)
\end{verbatim}

\begin{table}[h]
    \centering
    \input{./tables/example_table.tex}
\end{table}

\subsection{Biblatex}
Rember when using biblatex to use 'parencite' for \parencite{kamtheDataEfficient2018} and when using natbib to use 'citep'.



\appendix

\section{Appendix}

Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.


\end{document}
