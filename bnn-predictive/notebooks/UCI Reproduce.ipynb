{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce7f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import MultivariateNormal, Normal\n",
    "from preds.models import MLPS\n",
    "\n",
    "from preds.likelihoods import CategoricalLh\n",
    "from preds.datasets import UCIClassificationDatasets\n",
    "from preds.laplace import Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8dc32",
   "metadata": {},
   "source": [
    "Includes some playing around with the Immer et al. dataset and model classes and a script for running the UCI experiemnts (with hyperparameters as in the paper appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc00ad",
   "metadata": {},
   "source": [
    "Testing the dataset and model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd036c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 50 # as in Immer et al. \n",
    "depth = 2 # as in Immer et al. \n",
    "prior_prec = np.logspace(-2, 2, num=10)[0] # as in Immer et al. but depends on dataset\n",
    "lr = 1e-3 # as in Immer et al. \n",
    "n_epochs = 10000\n",
    "n_samples = 1000\n",
    "train_size = 0.70 # as in Immer et al. \n",
    "lh = CateoricalLh()  \n",
    "uci_dataset = 'glass'\n",
    "root_dir = '../data/'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbdd49f",
   "metadata": {},
   "source": [
    "Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b75edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = UCIClassificationDatasets(train=True, data_set=uci_dataset, split_train_size=train_size, double=False, root=root_dir)\n",
    "X_train, y_train = ds_train.data.to(device), ds_train.targets.to(device).unsqueeze(1)\n",
    "train_loader = [(X_train, y_train)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202d34d",
   "metadata": {},
   "source": [
    "Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410628e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = UCIClassificationDatasets(train=False,valid=True, data_set=uci_dataset, split_train_size=train_size, double=False, root=root_dir)\n",
    "X_val, y_val = ds_val.data.to(device), ds_val.targets.to(device).unsqueeze(1)\n",
    "val_loader = [(X_val, y_val)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f517c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPS(X_train.shape[1], [width]*depth, 1, activation='tanh', flatten=False).to(device)\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "losses = list()\n",
    "for i in range(n_epochs):\n",
    "    f = model(X_train)\n",
    "    w = parameters_to_vector(model.parameters())\n",
    "    reg = 0.5 * prior_prec * w @ w\n",
    "    loss = - lh.log_likelihood(y_train, f) + reg\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    losses.append(loss.item())\n",
    "    model.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap = Laplace(model, float(prior_prec), lh)\n",
    "\n",
    "\n",
    "def get_pred_for(x, model_type='glm', cov_type='full'):\n",
    "    #### INFERENCE (Posterior approximation) ####\n",
    "    lap.infer(train_loader, cov_type=cov_type, dampen_kron=model_type=='bnn')\n",
    "    if model_type == 'glm':\n",
    "        #### GLM PREDICTIVE ####\n",
    "        mu, var = lap.predictive_samples_glm(x, n_samples=n_samples)\n",
    "    elif model_type == 'bnn':\n",
    "        #### BNN PREDICTIVE ####\n",
    "        samples = lap.predictive_samples_bnn(x, n_samples=n_samples)\n",
    "        mu = samples.mean(axis=0)\n",
    "        var = samples.cov(axis=0)\n",
    "    else:\n",
    "        raise ValueError('unsupported model_type.')\n",
    "    mu = mu.detach().cpu().squeeze()\n",
    "    var = var.detach().cpu().squeeze()\n",
    "    return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtype)\n",
    "print(X_val.dtype)\n",
    "print(mu.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bae456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM\n",
    "\n",
    "mu_glm, var_glm = get_pred_for(X_val, 'glm', 'full')# runs\n",
    "# mu_glm_kron, var_glm_kron = get_pred_for(X_val, 'glm', 'kron') # doesn't run\n",
    "#mu, var = get_pred_for(X_val, 'glm', 'diag')# doesn't run\n",
    "\n",
    "# BNN\n",
    "mu_bnn, var_bnn = get_pred_for(X_val, 'bnn', 'full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_glm = Normal(mu_glm, var_glm)\n",
    "print(-torch.mean(lh_glm.log_prob(y_val.squeeze(-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_bnn = Normal(mu_bnn, var_bnn)\n",
    "print(-torch.mean(lh_bnn.log_prob(y_val.squeeze(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601765b1",
   "metadata": {},
   "source": [
    "<h2>Run the classification pipeline for chosen dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc6093",
   "metadata": {},
   "source": [
    "The classification.py script tests 10 options for the prior precision, with a fixed train/val/test split. The results reported in the paper are over 10 different train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d088e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['australian', 'breast_cancer', 'digits', 'glass',\n",
    "            'ionosphere', 'satellite', 'vehicle', 'waveform']\n",
    "seeds = [711, 1, 75, 359, 17, 420, 129, 666, 69, 36]\n",
    "dataset = 'breast_cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760da084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to ../experiments/results\n",
      "Reading data from ../data\n",
      "100%|██████████████████████████████████████████| 10/10 [42:12<00:00, 253.22s/it]\n",
      "Writing results to ../experiments/results\n",
      "Reading data from ../data\n",
      "100%|██████████████████████████████████████████| 10/10 [53:43<00:00, 322.32s/it]\n",
      "Writing results to ../experiments/results\n",
      "Reading data from ../data\n",
      "100%|██████████████████████████████████████████| 10/10 [44:43<00:00, 268.37s/it]\n",
      "Writing results to ../experiments/results\n",
      "Reading data from ../data\n",
      " 60%|█████████████████████████▊                 | 6/10 [24:45<16:41, 250.35s/it]"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    if dataset in ['satellite', 'digits']:\n",
    "        logmin = -1.0\n",
    "    else:\n",
    "        logmin = -2.0\n",
    "    !python3 ../experiments/classification.py -d {dataset} --root_dir ../ --seed {seed} --n_layers 2 --activation tanh --name tanh_2 --logd_min {logmin}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9320234e",
   "metadata": {},
   "source": [
    "The experiment in experiments/uci_classification_commands.py could otherwise be used, but it does not take into account the different hyperparameter settings required for satellite & digits datasets (different range for prior precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25be1a",
   "metadata": {},
   "source": [
    "Read the result pickle (for a single experiment with various prior precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc0053d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No results for seed 36\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "dataset = 'breast_cancer'\n",
    "name = 'sparse_quarter'\n",
    "datasets = ['australian', 'breast_cancer', 'digits', 'glass',\n",
    "            'ionosphere', 'satellite', 'vehicle', 'waveform']\n",
    "seeds = [711, 1, 75, 359, 17, 420, 129, 666, 69, 36]\n",
    "result_list = []\n",
    "for seed in seeds:\n",
    "    file_name = f'../experiments/results/uci/classification_{dataset}_{name}_{seed}.pkl'\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as handle:\n",
    "            result_list.append(pickle.load(handle))\n",
    "    else:\n",
    "        print(f'WARNING: No results for seed {seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fd793",
   "metadata": {},
   "source": [
    "Find  which prior precision gives the best validation NLL for the dataset (based on average NLL performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdd6af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 10)\n",
      "dict_keys(['losses', 'elbos_bbb', 'train_nll_map', 'train_acc_map', 'train_ece_map', 'test_nll_map', 'test_acc_map', 'test_ece_map', 'valid_nll_map', 'valid_acc_map', 'valid_ece_map', 'train_nll_bbb', 'train_acc_bbb', 'train_ece_bbb', 'test_nll_bbb', 'test_acc_bbb', 'test_ece_bbb', 'valid_nll_bbb', 'valid_acc_bbb', 'valid_ece_bbb', 'train_nll_svgp_ntk', 'train_acc_svgp_ntk', 'train_ece_svgp_ntk', 'test_nll_svgp_ntk', 'test_acc_svgp_ntk', 'test_ece_svgp_ntk', 'valid_nll_svgp_ntk', 'valid_acc_svgp_ntk', 'valid_ece_svgp_ntk', 'train_nll_glm', 'train_acc_glm', 'train_ece_glm', 'test_nll_glm', 'test_acc_glm', 'test_ece_glm', 'valid_nll_glm', 'valid_acc_glm', 'valid_ece_glm', 'train_nll_glmd', 'train_acc_glmd', 'train_ece_glmd', 'test_nll_glmd', 'test_acc_glmd', 'test_ece_glmd', 'valid_nll_glmd', 'valid_acc_glmd', 'valid_ece_glmd', 'train_nll_nn', 'train_acc_nn', 'train_ece_nn', 'test_nll_nn', 'test_acc_nn', 'test_ece_nn', 'valid_nll_nn', 'valid_acc_nn', 'valid_ece_nn', 'train_nll_nnd', 'train_acc_nnd', 'train_ece_nnd', 'test_nll_nnd', 'test_acc_nnd', 'test_ece_nnd', 'valid_nll_nnd', 'valid_acc_nnd', 'valid_ece_nnd', 'losses_lap', 'train_nll_glmLap', 'train_acc_glmLap', 'train_ece_glmLap', 'test_nll_glmLap', 'test_acc_glmLap', 'test_ece_glmLap', 'valid_nll_glmLap', 'valid_acc_glmLap', 'valid_ece_glmLap', 'train_nll_glmLapd', 'train_acc_glmLapd', 'train_ece_glmLapd', 'test_nll_glmLapd', 'test_acc_glmLapd', 'test_ece_glmLapd', 'valid_nll_glmLapd', 'valid_acc_glmLapd', 'valid_ece_glmLapd', 'elbos_vi', 'elbo_glm', 'train_nll_glmVI', 'train_acc_glmVI', 'train_ece_glmVI', 'test_nll_glmVI', 'test_acc_glmVI', 'test_ece_glmVI', 'valid_nll_glmVI', 'valid_acc_glmVI', 'valid_ece_glmVI', 'elbos_vid', 'elbo_glmd', 'train_nll_glmVId', 'train_acc_glmVId', 'train_ece_glmVId', 'test_nll_glmVId', 'test_acc_glmVId', 'test_ece_glmVId', 'valid_nll_glmVId', 'valid_acc_glmVId', 'valid_ece_glmVId'])\n",
      "[0.69271989 0.69182367 0.68979826 0.68626479 0.67913689 0.64152681\n",
      " 0.61060485 0.60099355 0.59318826 0.59240958]\n"
     ]
    }
   ],
   "source": [
    "valid_nlls = np.zeros((len(result_list), len(result_list[0]['deltas'])))\n",
    "print(valid_nlls.shape)\n",
    "print(result_list[0]['results'][0].keys())\n",
    "for i, results in enumerate(result_list):\n",
    "    for j, res in enumerate(results['results']):\n",
    "        if 'valid_nll_svgp_ntk' not in res:\n",
    "            print(seeds[i])\n",
    "            continue\n",
    "        valid_nlls[i, j] = res['train_nll_svgp_ntk']\n",
    "        \n",
    "mean_nlls = np.mean(valid_nlls, axis=0)\n",
    "print(mean_nlls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b80928",
   "metadata": {},
   "source": [
    "Check the test set performance of the selected prior precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5bbaaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31400694251060485\n",
      "[0.30834961 0.28497851 0.3024272  0.29026619 0.39062044 0.30780569\n",
      " 0.30664775 0.29315224 0.34323138 0.31259042]\n",
      "0.02970791712585548\n"
     ]
    }
   ],
   "source": [
    "prec_idx = 7\n",
    "test_nlls = np.zeros(len(result_list))\n",
    "for i, results in enumerate(result_list):\n",
    "    test_nlls[i] = results['results'][prec_idx]['test_nll_glm']\n",
    "print(np.mean(test_nlls))\n",
    "print(test_nlls)\n",
    "print(np.std(test_nlls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = result_list[0]\n",
    "n_test = results['N_test']\n",
    "n_train = results['N_train']\n",
    "deltas = results['deltas']\n",
    "print(f'Results keys: {results.keys()}')\n",
    "print(results['K'])\n",
    "print(f'Train set size: {n_train}')\n",
    "print(f'Test set size: {n_test}')\n",
    "for i, res in enumerate(results['results']):\n",
    "    print(f'Prior precision: {deltas[i]}')\n",
    "    print(f'Validation set NLL Map: {res[\"valid_nll_map\"]}')\n",
    "    print(f'Validation set NLL GLM: {res[\"valid_nll_glm\"]}')\n",
    "    print(f'Validation set NLL BNN: {res[\"valid_nll_nn\"]}')\n",
    "    \n",
    "    for key in res:\n",
    "        if 'valid_nll' in key:\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cab4f7",
   "metadata": {},
   "source": [
    "<h2>Run Image experiments </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c46c905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'MNIST'\n",
    "model = 'MLP'\n",
    "seed = 117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a10bcad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to ../experiments/results/MNIST\n",
      "Reading data from ../data\n",
      "Dataset: MNIST\n",
      "Seed: 117\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "9920512it [00:00, 12327924.77it/s]                                              \n",
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "32768it [00:00, 298645.29it/s]\n",
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "1654784it [00:00, 6915425.85it/s]                                               \n",
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "8192it [00:00, 29604.16it/s]                                                    \n",
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Processing...\n",
      "/Users/tamire1/miniconda3/envs/bnn-env-3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629430416/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "Done!\n",
      "  3%|█▏                                        | 14/500 [01:12<42:17,  5.22s/it]^C\n",
      "  3%|█▏                                        | 14/500 [01:13<42:32,  5.25s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"../experiments/imgclassification.py\", line 164, in <module>\n",
      "    main(ds_train, ds_test, model_name, seed, n_epochs, batch_size, lr, deltas, device, fname, res_dir)\n",
      "  File \"../experiments/imgclassification.py\", line 98, in main\n",
      "    loss.backward()\n",
      "  File \"/Users/tamire1/miniconda3/envs/bnn-env-3/lib/python3.7/site-packages/torch/tensor.py\", line 185, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/Users/tamire1/miniconda3/envs/bnn-env-3/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 127, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 ../experiments/imgclassification.py -d {ds} -m {model} -s {seed}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
