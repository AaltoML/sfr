@article{2020NumPy-Array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {Fern{\'a}ndez del R{\'i}o}, Jaime and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  journal = {Nature},
  volume = {585},
  pages = {357--362},
  doi = {10.1038/s41586-020-2649-2}
}

@article{2020SciPy-NMeth,
  title = {{{SciPy}} 1.0: {{Fundamental}} Algorithms for Scientific Computing in Python},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year = {2020},
  journal = {Nature Methods},
  volume = {17},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2},
  adsurl = {https://rdcu.be/b08Wh}
}

@book{abrahamManifolds1988,
  title = {Manifolds, {{Tensor Analysis}}, and {{Applications}}},
  author = {Abraham, Ralph and Marsden, J. E. and Ratiu, Tudor},
  year = {1988},
  series = {Applied {{Mathematical Sciences}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4612-1029-0},
  abstract = {The purpose of this book is to provide core material in nonlinear analysis for mathematicians, physicists, engineers, and mathematical biologists. The main goal is to provide a working knowledge of manifolds, dynamical systems, tensors, and differential forms. Some applications to Hamiltonian mechanics, fluid me\- chanics, electromagnetism, plasma dynamics and control thcory arc given in Chapter 8, using both invariant and index notation. The current edition of the book does not deal with Riemannian geometry in much detail, and it does not treat Lie groups, principal bundles, or Morse theory. Some of this is planned for a subsequent edition. Meanwhile, the authors will make available to interested readers supplementary chapters on Lie Groups and Differential Topology and invite comments on the book's contents and development. Throughout the text supplementary topics are given, marked with the symbols \textasciitilde{} and \{l:;J. This device enables the reader to skip various topics without disturbing the main flow of the text. Some of these provide additional background material intended for completeness, to minimize the necessity of consulting too many outside references. We treat finite and infinite-dimensional manifolds simultaneously. This is partly for efficiency of exposition. Without advanced applications, using manifolds of mappings, the study of infinite-dimensional manifolds can be hard to motivate.\vphantom\}},
  isbn = {978-0-387-96790-5},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/PAPUN7FY/9780387967905.html}
}

@article{almubarakSafety2021,
  title = {Safety {{Embedded Differential Dynamic Programming}} Using {{Discrete Barrier States}}},
  author = {Almubarak, Hassan and Stachowicz, Kyle and Sadegh, Nader and Theodorou, Evangelos A.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.14608 [cs, eess]},
  eprint = {2105.14608},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Certified safe control is a growing challenge in robotics, especially when performance and safety objectives are desired to be concurrently achieved. In this work, we extend the barrier state (BaS) concept, recently proposed for stabilization of continuous time systems, to enforce safety for discrete time systems by creating a discrete barrier state (DBaS). The constructed DBaS is embedded into the discrete model of the safety-critical system in order to integrate safety objectives into performance objectives. We subsequently use the proposed technique to implement a safety embedded stabilizing control for nonlinear discrete systems. Furthermore, we employ the DBaS method to develop a safety embedded differential dynamic programming (DDP) technique to plan and execute safe optimal trajectories. The proposed algorithm is leveraged on a differential wheeled robot and on a quadrotor to safely perform several tasks including reaching, tracking and safe multi-quadrotor movement. The DBaS-based DDP (DBaS-DDP) is compared to the penalty method used in constrained DDP problems where it is shown that the DBaS-DDP consistently outperforms the penalty method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/3JZHF338/Almubarak et al. - 2021 - Safety Embedded Differential Dynamic Programming u.pdf;/Users/scannea1/Zotero/storage/QN5N9CF7/2105.html}
}

@article{almubarakSafety2021a,
  title = {Safety {{Embedded Control}} of {{Nonlinear Systems}} via {{Barrier States}}},
  author = {Almubarak, Hassan and Sadegh, Nader and Theodorou, Evangelos A.},
  year = {2021},
  month = mar,
  journal = {arXiv:2102.10253 [cs, eess]},
  eprint = {2102.10253},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {In many safety critical control systems, possibly opposing safety restrictions and control performance objectives may arise. To confront such a conflict, this paper proposes a methodology that embeds safety into stability of control systems. The development enforces safety by means of barrier functions used in optimization which are used to construct \textbackslash textit\{barrier states\} (BaS) that are \textbackslash textit\{embedded\} in the control system's model. As a result, as long as the equilibrium point of interest of the closed loop system is asymptotically stable, the generated trajectories are guaranteed to be safe. Consequently, a conflict between control objectives and safety constraints is substantially avoided. To show the efficacy of the proposed technique, we employ the simple pole placement method on a linear control system to generate a safely stabilizing controller. Optimal control is subsequently employed to fulfill safety, stability and performance objectives by solving the associated Hamilton-Jacobi-Bellman (HJB) which minimizes a cost functional that can involve the barrier states. Following this further, we exploit optimal control on a second dimensional pendulum on a cart model that is desired to avoid low velocities regions where the system may exhibit some controllability loss and on two simple mobile robots that are sent to opposite targets with an obstacle on the way which may potentially result in a collision.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/3L49BDYL/Almubarak et al. - 2021 - Safety Embedded Control of Nonlinear Systems via B.pdf;/Users/scannea1/Zotero/storage/NXP4GVNR/2102.html}
}

@book{altmanConstrained1999,
  title = {Constrained {{Markov Decision Processes}}: {{Stochastic Modeling}}},
  shorttitle = {Constrained {{Markov Decision Processes}}},
  author = {Altman, Eitan},
  year = {1999},
  publisher = {{Routledge}},
  doi = {10.1201/9781315140223},
  abstract = {This book provides a unified approach for the study of constrained Markov decision processes with a finite state space and unbounded costs. Unlike the single controller case considered in many other books, the author considers a single controller with several objectives, such as minimizing delays and loss, probabilities, and maximization of throughputs. It is desirable to design a controller that minimizes one cost objective, subject to inequality constraints on other cost objectives. This framework describes dynamic decision problems arising frequently in many engineering fields. A thorough overview of these applications is presented in the introduction. The book is then divided into three sections that build upon each other.}
}

@article{amesControl2017,
  title = {Control {{Barrier Function Based Quadratic Programs}} for {{Safety Critical Systems}}},
  author = {Ames, Aaron D. and Xu, Xiangru and Grizzle, Jessy W. and Tabuada, Paulo},
  year = {2017},
  month = aug,
  journal = {IEEE Transactions on Automatic Control},
  volume = {62},
  number = {8},
  pages = {3861--3876},
  issn = {1558-2523},
  doi = {10.1109/TAC.2016.2638961},
  abstract = {Safety critical systems involve the tight coupling between potentially conflicting control objectives and safety constraints. As a means of creating a formal framework for controlling systems of this form, and with a view toward automotive applications, this paper develops a methodology that allows safety conditions-expressed as control barrier functions-to be unified with performance objectives-expressed as control Lyapunov functions-in the context of real-time optimization-based controllers. Safety conditions are specified in terms of forward invariance of a set, and are verified via two novel generalizations of barrier functions; in each case, the existence of a barrier function satisfying Lyapunov-like conditions implies forward invariance of the set, and the relationship between these two classes of barrier functions is characterized. In addition, each of these formulations yields a notion of control barrier function (CBF), providing inequality constraints in the control input that, when satisfied, again imply forward invariance of the set. Through these constructions, CBFs can naturally be unified with control Lyapunov functions (CLFs) in the context of a quadratic program (QP); this allows for the achievement of control objectives (represented by CLFs) subject to conditions on the admissible states of the system (represented by CBFs). The mediation of safety and performance through a QP is demonstrated on adaptive cruise control and lane keeping, two automotive control problems that present both safety and performance considerations coupled with actuator bounds.},
  keywords = {Automotive engineering,Barrier function,control Lyapunov function,Cruise control,Electrical engineering,Electronic mail,Lyapunov methods,nonlinear control,quadratic program,safety,Safety,set invariance},
  file = {/Users/scannea1/Zotero/storage/9NUGNXTJ/Ames et al. - 2017 - Control Barrier Function Based Quadratic Programs .pdf}
}

@inproceedings{amesControl2019,
  title = {Control {{Barrier Functions}}: {{Theory}} and {{Applications}}},
  shorttitle = {Control {{Barrier Functions}}},
  booktitle = {2019 18th {{European Control Conference}} ({{ECC}})},
  author = {Ames, Aaron D. and Coogan, Samuel and Egerstedt, Magnus and Notomista, Gennaro and Sreenath, Koushil and Tabuada, Paulo},
  year = {2019},
  month = jun,
  pages = {3420--3431},
  doi = {10.23919/ECC.2019.8796030},
  abstract = {This paper provides an introduction and overview of recent work on control barrier functions and their use to verify and enforce safety properties in the context of (optimization based) safety-critical controllers. We survey the main technical results and discuss applications to several domains including robotic systems.},
  file = {/Users/scannea1/Zotero/storage/HMGX6K3G/Ames et al. - 2019 - Control Barrier Functions Theory and Applications.pdf;/Users/scannea1/Zotero/storage/9BY4GJA5/8796030.html}
}

@inproceedings{amosModelBasedStochasticValue2021,
  title = {On the {{Model-Based Stochastic Value Gradient}} for {{Continuous Reinforcement Learning}}},
  booktitle = {Proceedings of the 3rd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Amos, Brandon and Stanton, Samuel and Yarats, Denis and Wilson, Andrew Gordon},
  year = {2021},
  month = may,
  pages = {6--20},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Model-based reinforcement learning approaches add explicit domain knowledge to agents in hopes of improving the sample-efficiency in comparison to model-free agents. However, in practice model-based methods are unable to achieve the same asymptotic performance on challenging continuous control tasks due to the complexity of learning and controlling an explicit world model. In this paper we investigate the stochastic value gradient (SVG),which is a well-known family of methods for controlling continuous systems which includes model-based approaches that distill a model-based value expansion into a model-free policy. We consider a variant of the model-based SVG that scales to larger systems and uses 1) an entropy regularization to help with exploration,2) a learned deterministic world model to improve the short-horizon value estimate, and 3) a learned model-free value estimate after the model's rollout. This SVG variation captures the model-free soft actor-critic method as an instance when the model rollout horizon is zero,and otherwise uses short-horizon model rollouts to improve the value estimate for the policy update. We surpass the asymptotic performance of other model-based methods on the proprioceptive MuJoCo locomotion tasks from the OpenAI gym,including a humanoid. We notably achieve these results with a simple deterministic world model without requiring an ensemble.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Amos et al-2021 On the Model-Based Stochastic Value Gradient for Continuous Reinforcement/Amos et al_2021_On the Model-Based Stochastic Value Gradient for Continuous Reinforcement.pdf}
}

@article{andersonNonCentral1946,
  title = {The {{Non-Central Wishart Distribution}} and {{Certain Problems}} of {{Multivariate Statistics}}},
  author = {Anderson, T. W.},
  year = {1946},
  journal = {The Annals of Mathematical Statistics},
  volume = {17},
  number = {4},
  pages = {409--431},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {The non-central Wishart distribution is the joint distribution of the sums of squares and cross-products of the deviations from the sample means when the observations arise from a set of normal multivariate populations with constant covariance matrix but expected values that vary from observation to observation. The characteristic function for this distribution is obtained from the distribution of the observations (Theorem 1). By using the characteristic functions it is shown that the convolution of several non-central Wishart distributions is another non-central Wishart distribution (Theorem 2). A simple integral representation of the distribution in the general case is given (Theorem 3). The integrand is a function of the roots of a determinantal equation involving the matrix of sums of squares and cross-products of deviations of observations and the matrix of sums of squares and cross-products of deviations of corresponding expected values. The knowledge of the non-central Wishart distribution is applied to two general problems of multivariate normal statistics. The moments of the generalized variance, which is the determinant of sums of squares and cross-products multiplied by a constant, are given for the cases of the expected values of the variates lying on a line (Theorem 4) and lying on a plane (Theorem 5). The likelihood ratio criterion for testing linear hypotheses can be expressed as the ratio of two determinants or as a symmetric function of the roots of a determinantal equation. In either case there is involved a matrix having a Wishart distribution and another matrix independently distributed such that the sum of these two matrices has a non-central Wishart distribution. When the null hypothesis is not true the moments of this criterion are given in the non-central planar case (Theorem 6).},
  file = {/Users/scannea1/Zotero/storage/Y35MHNGC/Anderson - 1946 - The Non-Central Wishart Distribution and Certain P.pdf}
}

@inproceedings{andrychowiczHindsightExperienceReplay2017,
  title = {Hindsight {{Experience Replay}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Andrychowicz et al-2017 Hindsight Experience Replay/Andrychowicz et al_2017_Hindsight Experience Replay.pdf}
}

@article{aoyamaReceding2021,
  title = {Receding {{Horizon Differential Dynamic Programming Under Parametric Uncertainty}}},
  author = {Aoyama, Yuichiro and Saravanos, Augustinos D. and Theodorou, Evangelos A.},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.10836 [math]},
  eprint = {2104.10836},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {Generalized Polynomial Chaos (gPC) theory has been widely used for representing parametric uncertainty in a system, thanks to its ability to propagate uncertainty evolution. In an optimal control context, gPC can be combined with several optimization techniques to achieve a control policy that handles effectively this type of uncertainty. Such a suitable method is Differential Dynamic Programming (DDP), leading to an algorithm that inherits the scalability to high-dimensional systems and fast convergence nature of the latter. In this paper, we expand this combination aiming to acquire probabilistic guarantees on the satisfaction of constraints. In particular, we exploit the ability of gPC to express higher order moments of the uncertainty distribution - without any Gaussianity assumption - and we incorporate chance constraints that lead to expressions involving the state variance. Furthermore, we demonstrate that by implementing our algorithm in a receding horizon fashion, we compute control policies that effectively reduce the accumulation of uncertainty on the trajectory. The applicability of our method is verified through simulation results on a differential wheeled robot and a quadrotor that perform obstacle avoidance tasks.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/Users/scannea1/Zotero/storage/CXB6FM5C/Aoyama et al. - 2021 - Receding Horizon Differential Dynamic Programming .pdf;/Users/scannea1/Zotero/storage/5XLXZG2Z/2104.html}
}

@inproceedings{arcariDualStochasticMPC2020,
  title = {Dual {{Stochastic MPC}} for {{Systems}} with {{Parametric}} and {{Structural Uncertainty}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Arcari, Elena and Hewing, Lukas and Schlichting, Max and Zeilinger, Melanie},
  year = {2020},
  month = jul,
  pages = {894--903},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Designing controllers for systems affected by model uncertainty can prove to be a challenge, especially when seeking the optimal compromise between the conflicting goals of identification and control. This trade-off is explicitly taken into account in the dual control problem, for which the exact solution is provided by stochastic dynamic programming. Due to its computational intractability, we propose a sampling-based approximation for systems affected by both parametric and structural model uncertainty. The approach proposed in this paper separates the prediction horizon in a dual and an exploitation part. The dual part is formulated as a scenario tree that actively discriminates among a set of potential models while learning unknown parameters. In the exploitation part,  achieved information is fixed for each scenario, and open-loop control sequences are computed for the remainder of the horizon. As a result, we solve one optimization problem over a collection of control sequences for the entire horizon, explicitly considering the knowledge gained in each scenario, leading to a dual model predictive control formulation.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Arcari et al-2020 Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty/Arcari et al_2020_Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty.pdf}
}

@inproceedings{arcariDualStochasticMPC2020a,
  title = {Dual {{Stochastic MPC}} for {{Systems}} with {{Parametric}} and {{Structural Uncertainty}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Arcari, Elena and Hewing, Lukas and Schlichting, Max and Zeilinger, Melanie},
  year = {2020},
  month = jul,
  pages = {894--903},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Designing controllers for systems affected by model uncertainty can prove to be a challenge, especially when seeking the optimal compromise between the conflicting goals of identification and control. This trade-off is explicitly taken into account in the dual control problem, for which the exact solution is provided by stochastic dynamic programming. Due to its computational intractability, we propose a sampling-based approximation for systems affected by both parametric and structural model uncertainty. The approach proposed in this paper separates the prediction horizon in a dual and an exploitation part. The dual part is formulated as a scenario tree that actively discriminates among a set of potential models while learning unknown parameters. In the exploitation part,  achieved information is fixed for each scenario, and open-loop control sequences are computed for the remainder of the horizon. As a result, we solve one optimization problem over a collection of control sequences for the entire horizon, explicitly considering the knowledge gained in each scenario, leading to a dual model predictive control formulation.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Arcari et al-2020 Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty/Arcari et al_2020_Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty2.pdf}
}

@article{ariafarADMMBO2019,
  title = {{{ADMMBO}}: {{Bayesian Optimization}} with {{Unknown Constraints}} Using {{ADMM}}},
  shorttitle = {{{ADMMBO}}},
  author = {Ariafar, Setareh and {Coll-Font}, Jaume and Brooks, Dana and Dy, Jennifer},
  year = {2019},
  month = jan,
  journal = {Journal of machine learning research : JMLR},
  volume = {20},
  abstract = {There exist many problems in science and engineering that involve optimization of an unknown or partially unknown objective function. Recently, Bayesian Optimization (BO) has emerged as a powerful tool for solving optimization problems whose objective functions are only available as a black box and are expensive to evaluate. Many practical problems, however, involve optimization of an unknown objective function subject to unknown constraints. This is an important yet challenging problem for which, unlike optimizing an unknown function, existing methods face several limitations. In this paper, we present a novel constrained Bayesian optimization framework to optimize an unknown objective function subject to unknown constraints. We introduce an equivalent optimization by augmenting the objective function with constraints, introducing auxiliary variables for each constraint, and forcing the new variables to be equal to the main variable. Building on the Alternating Direction Method of Multipliers (ADMM) algorithm, we propose ADMM-Bayesian Optimization (ADMMBO) to solve the problem in an iterative fashion. Our framework leads to multiple unconstrained subproblems with unknown objective functions, which we then solve via BO. Our method resolves several challenges of state-of-the-art techniques: it can start from infeasible points, is insensitive to initialization, can efficiently handle 'decoupled problems' and has a concrete stopping criterion. Extensive experiments on a number of challenging BO benchmark problems show that our proposed approach outperforms the state-of-the-art methods in terms of the speed of obtaining a feasible solution and convergence to the global optimum as well as minimizing the number of total evaluations of unknown objective and constraints functions.}
}

@inproceedings{arvanitidisLatent2018a,
  title = {Latent {{Space Oddity}}: On the {{Curvature}} of {{Deep Generative Models}}},
  shorttitle = {Latent {{Space Oddity}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  year = {2018},
  month = feb,
  abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear "generator" function that maps latent points into the input...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/8Z7PQAKV/Arvanitidis et al. - 2018 - Latent Space Oddity on the Curvature of Deep Gene.pdf;/Users/scannea1/Zotero/storage/UJFPWJWH/forum.html}
}

@misc{asConstrainedPolicyOptimization2022,
  title = {Constrained {{Policy Optimization}} via {{Bayesian World Models}}},
  author = {As, Yarden and Usmanova, Ilnura and Curi, Sebastian and Krause, Andreas},
  year = {2022},
  month = feb,
  number = {arXiv:2201.09802},
  eprint = {2201.09802},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.09802},
  abstract = {Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/As et al-2022 Constrained Policy Optimization via Bayesian World Models/As et al_2022_Constrained Policy Optimization via Bayesian World Models.pdf;/Users/scannea1/Zotero/storage/SYXWY3QE/2201.html}
}

@article{aswaniProvably2013,
  title = {Provably Safe and Robust Learning-Based Model Predictive Control},
  author = {Aswani, Anil and Gonzalez, Humberto and Sastry, S. Shankar and Tomlin, Claire},
  year = {2013},
  month = may,
  journal = {Automatica},
  volume = {49},
  number = {5},
  pages = {1216--1226},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2013.02.003},
  abstract = {Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics.},
  langid = {english},
  keywords = {Learning control,Predictive control,Robustness,Safety analysis,Statistics},
  file = {/Users/scannea1/Zotero/storage/NM75P263/Aswani et al. - 2013 - Provably safe and robust learning-based model pred.pdf;/Users/scannea1/Zotero/storage/L22HPPJE/S0005109813000678.html}
}

@inproceedings{atkesonComparison1997,
  title = {A Comparison of Direct and Model-Based Reinforcement Learning},
  booktitle = {Proceedings of {{International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Atkeson, C.G. and Santamaria, J.C.},
  year = {1997},
  month = apr,
  volume = {4},
  pages = {3557-3564 vol.4},
  doi = {10.1109/ROBOT.1997.606886},
  abstract = {This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals.},
  keywords = {Computational modeling,Control system synthesis,Control systems,Educational institutions,Force control,Jacobian matrices,Learning,Robots,State-space methods,Training data},
  file = {/Users/scannea1/Zotero/storage/ANHXHIXS/Atkeson and Santamaria - 1997 - A comparison of direct and model-based reinforceme.pdf;/Users/scannea1/Zotero/storage/P4GM3WQN/606886.html}
}

@article{auerUsing2002,
  title = {Using {{Confidence Bounds}} for {{Exploitation-Exploration Trade-offs}}},
  author = {Auer, Peter},
  year = {2002},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Nov},
  pages = {397--422},
  issn = {ISSN 1533-7928},
  abstract = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
  file = {/Users/scannea1/Zotero/storage/T7P7KJVN/Auer - 2002 - Using Confidence Bounds for Exploitation-Explorati.pdf}
}

@inproceedings{baiGaussian2022,
  title = {Gaussian {{Mixture Variational Autoencoder}} with {{Contrastive Learning}} for {{Multi-Label Classification}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Bai, Junwen and Kong, Shufeng and Gomes, Carla P.},
  year = {2022},
  month = jun,
  pages = {1383--1398},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Multi-label classification (MLC) is a prediction task where each sample can have more than one label. We propose a novel contrastive learning boosted multi-label prediction model based on a Gaussian mixture variational autoencoder (C-GMVAE), which learns a multimodal prior space and employs a contrastive loss. Many existing methods introduce extra complex neural modules like graph neural networks to capture the label correlations, in addition to the prediction modules. We find that by using contrastive learning in the supervised setting, we can exploit label information effectively in a data-driven manner, and learn meaningful feature and label embeddings which capture the label correlations and enhance the predictive power. Our method also adopts the idea of learning and aligning latent spaces for both features and labels. In contrast to previous works based on a unimodal prior, C-GMVAE imposes a Gaussian mixture structure on the latent space, to alleviate the posterior collapse and over-regularization issues. C-GMVAE outperforms existing methods on multiple public datasets and can often match other models' full performance with only 50\% of the training data. Furthermore, we show that the learnt embeddings provide insights into the interpretation of label-label interactions.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/6FD5XP9Q/Bai et al. - 2022 - Gaussian Mixture Variational Autoencoder with Cont.pdf}
}

@article{barcelosDual2021,
  title = {Dual {{Online Stein Variational Inference}} for {{Control}} and {{Dynamics}}},
  author = {Barcelos, Lucas and Lambert, Alexander and Oliveira, Rafael and Borges, Paulo and Boots, Byron and Ramos, Fabio},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.12890 [cs]},
  eprint = {2103.12890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Model predictive control (MPC) schemes have a proven track record for delivering aggressive and robust performance in many challenging control tasks, coping with nonlinear system dynamics, constraints, and observational noise. Despite their success, these methods often rely on simple control distributions, which can limit their performance in highly uncertain and complex environments. MPC frameworks must be able to accommodate changing distributions over system parameters, based on the most recent measurements. In this paper, we devise an implicit variational inference algorithm able to estimate distributions over model parameters and control inputs on-the-fly. The method incorporates Stein Variational gradient descent to approximate the target distributions as a collection of particles, and performs updates based on a Bayesian formulation. This enables the approximation of complex multi-modal posterior distributions, typically occurring in challenging and realistic robot navigation tasks. We demonstrate our approach on both simulated and real-world experiments requiring real-time execution in the face of dynamically changing environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/XEYAVZZ7/Barcelos et al. - 2021 - Dual Online Stein Variational Inference for Contro.pdf;/Users/scannea1/Zotero/storage/RYZDEYSS/2103.html}
}

@inproceedings{bauerUnderstanding2016,
  title = {Understanding {{Probabilistic Sparse Gaussian Process Approximations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bauer, Matthias and {van der Wilk}, Mark and Rasmussen, Carl Edward},
  year = {2016},
  volume = {29},
  pages = {1533--1541},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/PQCYYMZR/Bauer et al. - 2016 - Understanding Probabilistic Sparse Gaussian Proces.pdf;/Users/scannea1/Zotero/storage/FWFIKGWQ/7250eb93b3c18cc9daa29cf58af7a004-Abstract.html}
}

@inproceedings{bechtleCurious2020,
  title = {Curious {{iLQR}}: {{Resolving Uncertainty}} in {{Model-based RL}}},
  shorttitle = {Curious {{iLQR}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Bechtle, Sarah and Lin, Yixin and Rai, Akshara and Righetti, Ludovic and Meier, Franziska},
  year = {2020},
  month = may,
  pages = {162--171},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Curiosity as a means to explore during reinforcement learning problems has recently become very popular. However, very little progress has been made in utilizing curiosity for learning control. In ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JVSF3DF5/Bechtle et al. - 2020 - Curious iLQR Resolving Uncertainty in Model-based.pdf;/Users/scannea1/Zotero/storage/57HXZE89/bechtle20a.html}
}

@article{becker2022on,
  title = {On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning},
  author = {Becker, Philipp and Neumann, Gerhard},
  year = {2022},
  journal = {Transactions on Machine Learning Research}
}

@article{becker2022on,
  title = {On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning},
  author = {Becker, Philipp and Neumann, Gerhard},
  year = {2022},
  journal = {Transactions on Machine Learning Research},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Becker_Neumann-2022 On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning/Becker_Neumann_2022_On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/66KFGUAK/forum.html}
}

@article{bellemareAutonomous2020,
  title = {Autonomous Navigation of Stratospheric Balloons Using Reinforcement Learning},
  author = {Bellemare, Marc G. and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C. and Moitra, Subhodeep and Ponda, Sameera S. and Wang, Ziyu},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7836},
  pages = {77--82},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2939-8},
  abstract = {Efficiently navigating a superpressure balloon in the stratosphere1 requires the integration of a multitude of cues, such as wind speed and solar elevation, and the process is complicated by forecast errors and sparse wind measurements. Coupled with the need to make decisions in real time, these factors rule out the use of conventional control techniques2,3. Here we describe the use of reinforcement learning4,5 to create a high-performing flight controller. Our algorithm uses data augmentation6,7 and a self-correcting design to overcome the key technical challenge of reinforcement learning from imperfect data, which has proved to be a major obstacle to its application to physical systems8. We deployed our controller to station Loon superpressure balloons at multiple locations across the globe, including a 39-day controlled experiment over the Pacific Ocean. Analyses show that the controller outperforms Loon's previous algorithm and is robust to the natural diversity in stratospheric winds. These results demonstrate that reinforcement learning is an effective solution to real-world autonomous control problems in which neither conventional methods nor human intervention suffice, offering clues about what may be needed to create artificially intelligent agents that continuously interact with real, dynamic environments.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/698C2AZ7/Bellemare et al. - 2020 - Autonomous navigation of stratospheric balloons us.pdf;/Users/scannea1/Zotero/storage/5CXZNZ75/s41586-020-2939-8.html}
}

@inproceedings{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  pages = {449--458},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Bellemare et al-2017 A Distributional Perspective on Reinforcement Learning/Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/RWJRM5RZ/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf}
}

@article{bellmanDynamic1956,
  title = {Dynamic {{Programming}}},
  author = {Bellman, Richard},
  year = {1956},
  journal = {Princeton University Press},
  doi = {10.1126/science.153.3731.34}
}

@inproceedings{belznerBayesian2017b,
  title = {Bayesian Verification under Model Uncertainty},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Software Engineering}} for {{Smart Cyber-Physical Systems}}},
  author = {Belzner, Lenz and Gabor, Thomas},
  year = {2017},
  month = may,
  series = {{{SEsCPS}} '17},
  pages = {10--13},
  publisher = {{IEEE Press}},
  address = {{Buenos Aires, Argentina}},
  abstract = {Machine learning enables systems to build and update domain models based on runtime observations. In this paper, we study statistical model checking and runtime verification for systems with this ability. Two challenges arise: (1) Models built from limited runtime data yield uncertainty to be dealt with. (2) There is no definition of satisfaction w.r.t. uncertain hypotheses. We propose such a definition of subjective satisfaction based on recently introduced satisfaction functions. We also propose the BV algorithm as a Bayesian solution to runtime verification of subjective satisfaction under model uncertainty. BV provides user-definable stochastic bounds for type I and II errors. We discuss empirical results of a toy experiment.},
  isbn = {978-1-5386-4043-2},
  file = {/Users/scannea1/Zotero/storage/32XVLZ4U/Belzner and Gabor - 2017 - Bayesian Verification under Model Uncertainty.pdf}
}

@inproceedings{berkenkampSafe2015,
  title = {Safe and Robust Learning Control with {{Gaussian}} Processes},
  booktitle = {2015 {{European Control Conference}} ({{ECC}})},
  author = {Berkenkamp, Felix and Schoellig, Angela P.},
  year = {2015},
  month = jul,
  pages = {2496--2501},
  doi = {10.1109/ECC.2015.7330913},
  abstract = {This paper introduces a learning-based robust control algorithm that provides robust stability and performance guarantees during learning. The approach uses Gaussian process (GP) regression based on data gathered during operation to update an initial model of the system and to gradually decrease the uncertainty related to this model. Embedding this data-based update scheme in a robust control framework guarantees stability during the learning process. Traditional robust control approaches have not considered online adaptation of the model and its uncertainty before. As a result, their controllers do not improve performance during operation. Typical machine learning algorithms that have achieved similar high-performance behavior by adapting the model and controller online do not provide the guarantees presented in this paper. In particular, this paper considers a stabilization task, linearizes the nonlinear, GP-based model around a desired operating point, and solves a convex optimization problem to obtain a linear robust controller. The resulting performance improvements due to the learning-based controller are demonstrated in experiments on a quadrotor vehicle.},
  keywords = {Adaptation models,Data models,Robust control,Robustness,Stability analysis,Uncertainty,Vehicle dynamics},
  file = {/Users/scannea1/Zotero/storage/BBDEMG6I/Berkenkamp and Schoellig - 2015 - Safe and robust learning control with Gaussian pro.pdf}
}

@inproceedings{berkenkampSafe2016,
  title = {Safe Controller Optimization for Quadrotors with {{Gaussian}} Processes},
  booktitle = {International {{Conference}} on {{Robotics}} and {{Automation}}},
  author = {Berkenkamp, F. and Schoellig, A. P. and Krause, A.},
  year = {2016},
  month = may,
  pages = {491--496},
  publisher = {{IEEE}},
  doi = {10.1109/ICRA.2016.7487170},
  abstract = {One of the most fundamental problems when designing controllers for dynamic systems is the tuning of the controller parameters. Typically, a model of the system is used to obtain an initial controller, but ultimately the controller parameters must be tuned manually on the real system to achieve the best performance. To avoid this manual tuning step, methods from machine learning, such as Bayesian optimization, have been used. However, as these methods evaluate different controller parameters on the real system, safety-critical system failures may happen. In this paper, we overcome this problem by applying, for the first time, a recently developed safe optimization algorithm, SafeOpt, to the problem of automatic controller parameter tuning. Given an initial, low-performance controller, SafeOpt automatically optimizes the parameters of a control law while guaranteeing safety. It models the underlying performance measure as a Gaussian process and only explores new controller parameters whose performance lies above a safe performance threshold with high probability. Experimental results on a quadrotor vehicle indicate that the proposed method enables fast, automatic, and safe optimization of controller parameters without human intervention.},
  keywords = {aircraft control,automatic controller parameter tuning,Bayes methods,Bayesian optimization,Computational modeling,control system synthesis,controller design,dynamic systems,Gaussian process,Gaussian processes,helicopters,learning systems,low-performance controller,machine learning,Noise measurement,optimisation,Optimization,parameter estimation,performance threshold,probability,quadcopter,quadrotor vehicle,safe controller optimization,safe optimization algorithm,safe-exploration,SafeOpt,Safety,safety-critical system failure,Tuning,Vehicle dynamics},
  file = {/Users/scannea1/Zotero/storage/ZNFMVIUZ/Berkenkamp et al. - 2016 - Safe controller optimization for quadrotors with G.pdf;/Users/scannea1/Zotero/storage/P55WHW8T/7487170.html}
}

@inproceedings{berkenkampSafe2017,
  title = {Safe {{Model-based Reinforcement Learning}} with {{Stability Guarantees}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  year = {2017},
  volume = {30},
  pages = {908--918},
  langid = {english},
  keywords = {reinforcement-learning,safe-exploration},
  file = {/Users/scannea1/Zotero/storage/AQTLYZZW/Berkenkamp et al. - 2017 - Safe Model-based Reinforcement Learning with Stabi.pdf;/Users/scannea1/Zotero/storage/UXGE9IY2/766ebcd59621e305170616ba3d3dac32-Abstract.html}
}

@phdthesis{berkenkampSafe2019,
  type = {Doctoral {{Thesis}}},
  title = {Safe {{Exploration}} in {{Reinforcement Learning}}: {{Theory}} and {{Applications}} in {{Robotics}}},
  shorttitle = {Safe {{Exploration}} in {{Reinforcement Learning}}},
  author = {Berkenkamp, Felix},
  year = {2019},
  doi = {10.3929/ethz-b-000370833},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  langid = {english},
  school = {ETH Zurich},
  keywords = {quadcopter,reinforcement-learning,robotics,safe-exploration},
  annotation = {Accepted: 2019-10-16T10:53:26Z},
  file = {/Users/scannea1/Zotero/storage/WYNQYBJ3/Berkenkamp - 2019 - Safe Exploration in Reinforcement Learning Theory.pdf;/Users/scannea1/Zotero/storage/7XZQV7AR/370833.html;/Users/scannea1/Zotero/storage/J9XZZSNF/370833.html}
}

@article{bettsSurvey1998,
  title = {Survey of {{Numerical Methods}} for {{Trajectory Optimization}}},
  author = {Betts, John T.},
  year = {1998},
  month = mar,
  journal = {Journal of Guidance, Control, and Dynamics},
  volume = {21},
  number = {2},
  pages = {193--207},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/2.4231},
  file = {/Users/scannea1/Zotero/storage/AVKV9FTT/Betts - 1998 - Survey of Numerical Methods for Trajectory Optimiz.pdf;/Users/scannea1/Zotero/storage/UX6QF98W/2.html}
}

@inproceedings{bhardwajDifferentiable2020,
  title = {Differentiable {{Gaussian Process Motion Planning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bhardwaj, Mohak and Boots, Byron and Mukadam, Mustafa},
  year = {2020},
  month = may,
  pages = {10598--10604},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197260},
  abstract = {Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.},
  keywords = {Artificial intelligence,Automation,Conferences,Gaussian processes,Planning,Robots,Trajectory optimization},
  file = {/Users/scannea1/Zotero/storage/Z83QZZAC/Bhardwaj et al. - 2020 - Differentiable Gaussian Process Motion Planning.pdf;/Users/scannea1/Zotero/storage/CB972CMQ/9197260.html}
}

@inproceedings{blundellWeight2015,
  title = {Weight {{Uncertainty}} in {{Neural Network}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  month = jun,
  pages = {1613--1622},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/WJBLL6WY/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf}
}

@inproceedings{boedeckerApproximate2014,
  title = {Approximate Real-Time Optimal Control Based on Sparse {{Gaussian}} Process Models},
  booktitle = {2014 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  author = {Boedecker, Joschka and Springenberg, Jost Tobias and W{\"u}lfing, Jan and Riedmiller, Martin},
  year = {2014},
  month = dec,
  pages = {1--8},
  issn = {2325-1867},
  doi = {10.1109/ADPRL.2014.7010608},
  abstract = {In this paper we present a fully automated approach to (approximate) optimal control of non-linear systems. Our algorithm jointly learns a non-parametric model of the system dynamics - based on Gaussian Process Regression (GPR) - and performs receding horizon control using an adapted iterative LQR formulation. This results in an extremely data-efficient learning algorithm that can operate under real-time constraints. When combined with an exploration strategy based on GPR variance, our algorithm successfully learns to control two benchmark problems in simulation (two-link manipulator, cart-pole) as well as to swing-up and balance a real cart-pole system. For all considered problems learning from scratch, that is without prior knowledge provided by an expert, succeeds in less than 10 episodes of interaction with the system.},
  keywords = {Approximation algorithms,Approximation methods,Computational modeling,Optimal control,Optimization,Predictive models,Trajectory},
  file = {/Users/scannea1/Zotero/storage/DYM9RM8Y/Boedecker et al. - 2014 - Approximate real-time optimal control based on spa.pdf;/Users/scannea1/Zotero/storage/VCPKHUJ7/7010608.html}
}

@inproceedings{boneyRegularizing2019,
  title = {Regularizing {{Trajectory Optimization}} with {{Denoising Autoencoders}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Boney, Rinu and Di Palo, Norman and Berglund, Mathias and Ilin, Alexander and Kannala, Juho and Rasmus, Antti and Valpola, Harri},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Zotero/storage/AS2KZHRW/Boney et al. - 2019 - Regularizing Trajectory Optimization with Denoisin.pdf}
}

@article{bonyPrincipe1969,
  title = {Principe Du Maximum, In\'egalit\'e de {{Harnack}} et Unicit\'e Du Probl\`eme de {{Cauchy}} Pour Les Op\'erateurs Elliptiques D\'eg\'en\'er\'es},
  author = {Bony, Jean-Michel},
  year = {1969},
  journal = {Annales de l'Institut Fourier},
  volume = {19},
  number = {1},
  pages = {277--304},
  doi = {10.5802/aif.319},
  file = {/Users/scannea1/Zotero/storage/XNR2DYNE/Bony - 1969 - Principe du maximum, inégalité de Harnack et unici.pdf;/Users/scannea1/Zotero/storage/FLLNVGFL/item.html}
}

@article{brezisCharacterization1970,
  title = {On a Characterization of Flow-Invariant Sets},
  author = {Brezis, Haim},
  year = {1970},
  month = mar,
  journal = {Communications on Pure and Applied Mathematics},
  volume = {23},
  number = {2},
  pages = {261--263},
  issn = {00103640, 10970312},
  doi = {10.1002/cpa.3160230211},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/FL6V3IAF/on-a-characterization-of-flow-invariant-sets-FOPl7HAHdR.html}
}

@inproceedings{buisson-fenetActively2020,
  title = {Actively {{Learning Gaussian Process Dynamics}}},
  booktitle = {2nd {{Annual Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {{Buisson-Fenet}, Mona and Solowjow, Friedrich and Trimpe, Sebastian},
  year = {2020},
  volume = {120},
  pages = {1--11},
  publisher = {{Proceedings of Machine Learning Research}},
  abstract = {Despite the availability of ever more data enabled through modern sensor and computer technology, it still remains an open problem to learn dynamical systems in a sample-efficient way. We propose active learning strategies that leverage information-theoretical properties arising naturally during Gaussian process regression while respecting constraints on the sampling process imposed by the system dynamics. Sample points are selected in regions with high uncertainty, leading to exploratory behavior and data-efficient training of the model. All results are validated in an extensive numerical benchmark.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3LQUYRKQ/Buisson-Fenet et al. - Actively Learning Gaussian Process Dynamics.pdf}
}

@inproceedings{burtRates2019,
  title = {Rates of {{Convergence}} for {{Sparse Variational Gaussian Process Regression}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Burt, David and Rasmussen, Carl Edward and Wilk, Mark Van Der},
  year = {2019},
  month = may,
  pages = {862--871},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the ({$\mathsl{N}$}3)O(N3)\textbackslash mathcal\{O\}\textbackslash left(N\^3\textbackslash right) scaling with dataset size {$\mathsl{N}$}NN. They reduce the computational cost to ({$\mathsl{N}\mathsl{M}$}2)O(NM2)\textbackslash mathcal\{O\}\textbackslash left(NM\^2\textbackslash right), with {$\mathsl{M}\ll\mathsl{N}$}M{$\ll$}NM\textbackslash ll N the number of inducing variables, which summarise the process. While the computational cost seems to be linear in {$\mathsl{N}$}NN, the true complexity of the algorithm depends on how {$\mathsl{M}$}MM must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing {$\mathsl{M}$}MM more slowly than {$\mathsl{N}$}NN. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, {$\mathsl{M}$}=(log{$\mathsl{D}\mathsl{N}$})M=O(logD⁡N)M=\textbackslash mathcal\{O\}(\textbackslash log\^D N) suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase {$\mathsl{M}$}MM in continual learning scenarios.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/IVLK3YJI/Burt et al. - 2019 - Rates of Convergence for Sparse Variational Gaussi.pdf;/Users/scannea1/Zotero/storage/VLQ6WE7S/Burt et al. - 2019 - Rates of Convergence for Sparse Variational Gaussi.pdf}
}

@inproceedings{caponeLocalized2020,
  title = {Localized Active Learning of {{Gaussian}} Process State Space Models},
  booktitle = {2nd {{Annual Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Capone, Alexandre and Noske, Gerrit and Umlauft, Jonas and Beckers, Thomas and Lederer, Armin and Hirche, Sandra},
  year = {2020},
  volume = {120:1-12},
  eprint = {2005.02191},
  eprinttype = {arxiv},
  publisher = {{Proceedings of Machine Learning Research}},
  abstract = {The performance of learning-based control techniques crucially depends on how effectively the system is explored. While most exploration techniques aim to achieve a globally accurate model, such approaches are generally unsuited for systems with unbounded state spaces. Furthermore, a globally accurate model is not required to achieve good performance in many common control applications, e.g., local stabilization tasks. In this paper, we propose an active learning strategy for Gaussian process state space models that aims to obtain an accurate model on a bounded subset of the state-action space. Our approach aims to maximize the mutual information of the exploration trajectories with respect to a discretization of the region of interest. By employing model predictive control, the proposed technique integrates information collected during exploration and adaptively improves its exploration strategy. To enable computational tractability, we decouple the choice of most informative data points from the model predictive control optimization step. This yields two optimization problems that can be solved in parallel. We apply the proposed method to explore the state space of various dynamical systems and compare our approach to a commonly used entropy-based exploration strategy. In all experiments, our method yields a better model within the region of interest than the entropy-based method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/HWFMG3WZ/Capone et al. - 2020 - Localized active learning of Gaussian process stat.pdf;/Users/scannea1/Zotero/storage/J8WW4JNZ/2005.html}
}

@book{carmoRiemannian1992,
  title = {Riemannian {{Geometry}}},
  author = {do Carmo, Manfredo},
  year = {1992},
  series = {Mathematics: {{Theory}} \& {{Applications}}},
  publisher = {{Birkh\"auser Basel}},
  abstract = {Riemannian Geometry is an expanded edition of a highly acclaimed and successful textbook (originally published in Portuguese) for first-year graduate students in mathematics and physics. The author's treatment goes very directly to the basic language of Riemannian geometry and immediately presents some of its most fundamental theorems. It is elementary, assuming only a modest background from readers, making it suitable for a wide variety of students and course structures. Its selection of topics has been deemed "superb" by teachers who have used the text. A significant feature of the book is its powerful and revealing structure, beginning simply with the definition of a differentiable manifold and ending with one of the most important results in Riemannian geometry, a proof of the Sphere Theorem. The text abounds with basic definitions and theorems, examples, applications, and numerous exercises to test the student's understanding and extend knowledge and insight into the subject. Instructors and students alike will find the work to be a significant contribution to this highly applicable and stimulating subject.},
  isbn = {978-0-8176-3490-2},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/VAALLD2J/9780817634902.html}
}

@inproceedings{chengEndtoEnd2019,
  title = {End-to-{{End Safe Reinforcement Learning}} through {{Barrier Functions}} for {{Safety-Critical Continuous Control Tasks}}},
  booktitle = {{{AAAI}}},
  author = {Cheng, Richard and Orosz, G. and Murray, R. and Burdick, J.},
  year = {2019},
  doi = {10.1609/aaai.v33i01.33013387},
  abstract = {Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) on-line learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.  Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous car-following with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.},
  file = {/Users/scannea1/Zotero/storage/AAV4CC6T/Cheng et al. - 2019 - End-to-End Safe Reinforcement Learning through Bar.pdf}
}

@inproceedings{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  year = {2021},
  eprint = {2101.05982},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Chen et al-2021 Randomized Ensembled Double Q-Learning/Chen et al_2021_Randomized Ensembled Double Q-Learning.pdf;/Users/scannea1/Zotero/storage/FAXTXBT3/2101.html}
}

@inproceedings{chuaDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  year = {2018},
  volume = {31},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/K5RIDAIK/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials.pdf;/Users/scannea1/Zotero/storage/GPK99BVP/3de568f8597b94bda53149c7d7f5958c-Abstract.html}
}

@inproceedings{cohenHealing2020,
  title = {Healing {{Products}} of {{Gaussian Process Experts}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Cohen, Samuel and Mbuvha, Rendani and Marwala, Tshilidzi and Deisenroth, Marc},
  year = {2020},
  month = nov,
  pages = {2068--2077},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Gaussian processes (GPs) are nonparametric Bayesian models that have been applied to regression and classification problems. One of the approaches to alleviate their cubic training cost is the use of local GP experts trained on subsets of the data. In particular, product-of-expert models combine the predictive distributions of local experts through a tractable product operation. While these expert models allow for massively distributed computation, their predictions typically suffer from erratic behaviour of the mean or uncalibrated uncertainty quantification. By calibrating predictions via a tempered softmax weighting, we provide a solution to these problems for multiple product-of-expert models, including the generalised product of experts and the robust Bayesian committee machine. Furthermore, we leverage the optimal transport literature and propose a new product-of-expert model that combines predictions of local experts by computing their Wasserstein barycenter, which can be applied to both regression and classification.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/TYJE597B/Cohen et al. - 2020 - Healing Products of Gaussian Process Experts.pdf;/Users/scannea1/Zotero/storage/Z28D4TVC/Cohen et al. - 2020 - Healing Products of Gaussian Process Experts.pdf}
}

@book{coverElements2006,
  title = {Elements of Information Theory},
  author = {Cover, M., Thomas and Joy, A., Thomas},
  year = {2006},
  publisher = {{John Wiley \& Sons}}
}

@article{cowen-riversSAMBA2022,
  title = {{{SAMBA}}: Safe Model-Based~\& Active Reinforcement Learning},
  shorttitle = {{{SAMBA}}},
  author = {{Cowen-Rivers}, Alexander I. and Palenicek, Daniel and Moens, Vincent and Abdullah, Mohammed Amin and Sootla, Aivar and Wang, Jun and {Bou-Ammar}, Haitham},
  year = {2022},
  month = jan,
  journal = {Machine Learning},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-06103-6},
  abstract = {In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel acquisition functions for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our acquisition functions and safety constraints.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4K3U6I8V/Cowen-Rivers et al. - 2022 - SAMBA safe model-based & active reinforcement lea.pdf}
}

@inproceedings{curiCombining2021,
  title = {Combining {{Pessimism}} with {{Optimism}} for {{Robust}} and {{Efficient Model-Based Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Curi, Sebastian and Bogunovic, Ilija and Krause, Andreas},
  year = {2021},
  month = jul,
  pages = {2254--2264},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time. To ensure reliable performance, the RL agents need to exhibit robustness to such worst-case situations. The robust-RL framework addresses this challenge via a minimax optimization between an agent and an adversary. Previous robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems. We propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty and efficiently explores both the agent and the adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. Experimentally we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/56JHPA7V/Curi et al. - 2021 - Combining Pessimism with Optimism for Robust and E.pdf;/Users/scannea1/Zotero/storage/DWGYF72A/Curi et al. - 2021 - Combining Pessimism with Optimism for Robust and E.pdf}
}

@inproceedings{curiEfficient2020,
  title = {Efficient {{Model-Based Reinforcement Learning}} through {{Optimistic Policy Search}} and {{Planning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
  year = {2020},
  keywords = {model-based-reinforcement-learning,planning,policy-search},
  file = {/Users/scannea1/Zotero/storage/ID2FAF6Y/Mukherjee and Fine - 1995 - Asymptotics of Gradient-based Neural Network Train.pdf;/Users/scannea1/Zotero/storage/3WGIBLZ7/a36b598abb934e4528412e5a2127b931-Paper.html}
}

@inproceedings{curiStructured2020,
  title = {Structured {{Variational Inference}} in {{Partially Observable Unstable Gaussian Process State Space Models}}},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {Curi, Sebastian and Melchior, Silvan and Berkenkamp, Felix and Krause, Andreas},
  year = {2020},
  month = jul,
  pages = {147--157},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose a new variational inference algorithm for learning in Gaussian Process State-Space Models (GPSSMs). Our algorithm enables learning of unstable and partially observable systems, where pre...},
  langid = {english},
  keywords = {gaussian-process,state-space-model,variational-inference},
  file = {/Users/scannea1/Zotero/storage/DAEIJZM2/Curi et al. - 2020 - Structured Variational Inference in Partially Obse.pdf;/Users/scannea1/Zotero/storage/9W4SGF7V/curi20a.html}
}

@inproceedings{cutlerEfficient2015,
  title = {Efficient Reinforcement Learning for Robots Using Informative Simulated Priors},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Cutler, M. and How, J. P.},
  year = {2015},
  month = may,
  pages = {2605--2612},
  publisher = {{IEEE}},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2015.7139550},
  abstract = {Autonomous learning through interaction with the physical world is a promising approach to designing controllers and decision-making policies for robots. Unfortunately, learning on robots is often difficult due to the large number of samples needed for many learning algorithms. Simulators are one way to decrease the samples needed from the robot by incorporating prior knowledge of the dynamics into the learning algorithm. In this paper we present a novel method for transferring data from a simulator to a robot, using simulated data as a prior for real-world learning. A Bayesian nonparametric prior is learned from a potentially black-box simulator. The mean of this function is used as a prior for the Probabilistic Inference for Learning Control (PILCO) algorithm. The simulated prior improves the convergence rate and performance of PILCO by directing the policy search in areas of the state-space that have not yet been observed by the robot. Simulated and hardware results show the benefits of using the prior knowledge in the learning framework.},
  keywords = {autonomous learning,Bayes methods,Bayesian nonparametric prior,black-box simulator,controller design,convergence rate,Data models,decision-making policy,Gaussian processes,Hardware,Heuristic algorithms,informative simulated priors,learning (artificial intelligence),learning systems,Mathematical model,nonparametric statistics,PILCO algorithm,Prediction algorithms,probabilistic inference for learning control algorithm,reinforcement learning algorithm,robots,Robots},
  file = {/Users/scannea1/Zotero/storage/8TMVDYFN/Cutler and How - 2015 - Efficient reinforcement learning for robots using .pdf;/Users/scannea1/Zotero/storage/8868YSWR/7139550.html}
}

@phdthesis{damianouDeep2015,
  title = {Deep {{Gaussian Processes}} and {{Variational Propagation}} of {{Uncertainty}}},
  author = {Damianou, Andreas},
  year = {2015},
  month = jul,
  abstract = {Uncertainty propagation across components of complex probabilistic models is vital for improving regularisation. Unfortunately, for many interesting models based on non-linear Gaussian processes (GPs), straightforward propagation of uncertainty is computationally and mathematically intractable. This thesis is concerned with solving this problem through developing novel variational inference approaches.  From a modelling perspective, a key contribution of the thesis is the development of deep Gaussian processes (deep GPs). Deep GPs generalise several interesting GP-based models and, hence, motivate the development of uncertainty propagation techniques. In a deep GP, each layer is modelled as the output of a multivariate GP, whose inputs are governed by another GP. The resulting model is no longer a GP but, instead, can learn much more complex interactions between data. In contrast to other deep models, all the uncertainty in parameters and latent variables is marginalised out and both supervised and unsupervised learning is handled. Two important special cases of a deep GP can equivalently be seen as its building components and, historically, were developed as such. Firstly, the variational GP-LVM is concerned with propagating uncertainty in Gaussian process latent variable models. Any observed inputs (e.g. temporal) can also be used to correlate the latent space posteriors. Secondly, this thesis develops manifold relevance determination (MRD) which considers a common latent space for multiple views. An adapted variational framework allows for strong model regularisation, resulting in rich latent space representations to be learned. The developed models are also equipped with algorithms that maximise the information communicated between their different stages using uncertainty propagation, to achieve improved learning when partially observed values are present.  The developed methods are demonstrated in experiments with simulated and real data. The results show that the developed variational methodologies improve practical applicability by enabling automatic capacity control in the models, even when data are scarce.},
  copyright = {cc\_by\_nc\_nd},
  langid = {english},
  school = {University of Sheffield},
  file = {/Users/scannea1/Zotero/storage/6UA38AQH/Damianou - 2015 - Deep Gaussian Processes and Variational Propagatio.pdf;/Users/scannea1/Zotero/storage/GFC4KFMI/Damianou_Thesis.pdf;/Users/scannea1/Zotero/storage/KMT7ZCPG/9968.html}
}

@inproceedings{dasguptaAnalysis2005,
  title = {Analysis of a Greedy Active Learning Strategy},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dasgupta, Sanjoy},
  year = {2005},
  volume = {17},
  publisher = {{MIT Press}},
  file = {/Users/scannea1/Zotero/storage/V7PKLZJS/Dasgupta - 2005 - Analysis of a greedy active learning strategy.pdf}
}

@inproceedings{daxbergerLaplace2021,
  title = {Laplace {{Redux}} - {{Effortless Bayesian Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Daxberger, Erik and Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Bauer, Matthias and Hennig, Philipp},
  year = {2021},
  volume = {34},
  pages = {20089--20103},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Bayesian formulations of deep learning have been shown to have compelling theoretical properties and offer practical functional benefits, such as improved predictive uncertainty quantification and model selection. The Laplace approximation (LA) is a classic, and arguably the simplest family of approximations for the intractable posteriors of deep neural networks. Yet, despite its simplicity, the LA is not as popular as alternatives like variational Bayes or deep ensembles. This may be due to assumptions that the LA is expensive due to the involved Hessian computation, that it is difficult to implement, or that it yields inferior results. In this work we show that these are misconceptions: we (i) review the range of variants of the LA including versions with minimal cost overhead; (ii) introduce "laplace", an easy-to-use software library for PyTorch offering user-friendly access to all major flavors of the LA; and (iii) demonstrate through extensive experiments that the LA is competitive with more popular alternatives in terms of performance, while excelling in terms of computational cost. We hope that this work will serve as a catalyst to a wider adoption of the LA in practical deep learning, including in domains where Bayesian approaches are not typically considered at the moment.},
  file = {/Users/scannea1/Zotero/storage/YYKB5MPQ/Daxberger et al. - 2021 - Laplace Redux - Effortless Bayesian Deep Learning.pdf}
}

@inproceedings{deisenrothApproximate2008,
  title = {Approximate Dynamic Programming with {{Gaussian}} Processes},
  booktitle = {American {{Control Conference}}},
  author = {Deisenroth, Marc P. and Peters, Jan and Rasmussen, Carl E.},
  year = {2008},
  month = jun,
  pages = {4480--4485},
  publisher = {{IEEE}},
  issn = {2378-5861},
  doi = {10.1109/ACC.2008.4587201},
  abstract = {In general, it is difficult to determine an optimal closed-loop policy in nonlinear control problems with continuous-valued state and control domains. Hence, approximations are often inevitable. The standard method of discretizing states and controls suffers from the curse of dimensionality and strongly depends on the chosen temporal sampling rate. In this paper, we introduce Gaussian process dynamic programming (GPDP) and determine an approximate globally optimal closed-loop policy. In GPDP, value functions in the Bellman recursion of the dynamic programming algorithm are modeled using Gaussian processes. GPDP returns an optimal state- feedback for a finite set of states. Based on these outcomes, we learn a possibly discontinuous closed-loop policy on the entire state space by switching between two independently trained Gaussian processes. A binary classifier selects one Gaussian process to predict the optimal control signal. We show that GPDP is able to yield an almost optimal solution to an LQ problem using few sample points. Moreover, we successfully apply GPDP to the underpowered pendulum swing up, a complex nonlinear control problem.},
  keywords = {Bayesian methods,Control systems,Dynamic programming,Function approximation,Gaussian processes,Machine learning,Nonlinear dynamical systems,Optimal control,Sampling methods,State-space methods},
  file = {/Users/scannea1/Zotero/storage/YT5V4IBN/Deisenroth et al. - 2008 - Approximate dynamic programming with Gaussian proc.pdf;/Users/scannea1/Zotero/storage/EM9NQUE4/4587201.html}
}

@phdthesis{deisenrothEfficient2010,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc},
  year = {2010},
  month = nov,
  langid = {american},
  file = {/Users/scannea1/Zotero/storage/8K7J9N2Q/Deisenroth - 2019 - Efficient Reinforcement Learning using Gaussian Pr.pdf}
}

@phdthesis{deisenrothEfficient2010a,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  year = {2010},
  langid = {english},
  keywords = {gaussian-processes,model-based-rl},
  file = {/Users/scannea1/Zotero/storage/8XMSD6YK/Deisenroth - Efficient Reinforcement Learning using Gaussian Pr.pdf}
}

@article{deisenrothGaussian2009,
  title = {Gaussian Process Dynamic Programming},
  author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward and Peters, Jan},
  year = {2009},
  month = mar,
  journal = {Neurocomputing},
  series = {Advances in {{Machine Learning}} and {{Computational Intelligence}}},
  volume = {72},
  number = {7},
  pages = {1508--1524},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2008.12.019},
  abstract = {Reinforcement learning (RL) and optimal control of systems with continuous states and actions require approximation techniques in most interesting cases. In this article, we introduce Gaussian process dynamic programming (GPDP), an approximate value function-based RL algorithm. We consider both a classic optimal control problem, where problem-specific prior knowledge is available, and a classic RL problem, where only very general priors can be used. For the classic optimal control problem, GPDP models the unknown value functions with Gaussian processes and generalizes dynamic programming to continuous-valued states and actions. For the RL problem, GPDP starts from a given initial state and explores the state space using Bayesian active learning. To design a fast learner, available data have to be used efficiently. Hence, we propose to learn probabilistic models of the a priori unknown transition dynamics and the value functions on the fly. In both cases, we successfully apply the resulting continuous-valued controllers to the under-actuated pendulum swing up and analyze the performances of the suggested algorithms. It turns out that GPDP uses data very efficiently and can be applied to problems, where classic dynamic programming would be cumbersome.},
  langid = {english},
  keywords = {Bayesian active learning,Dynamic programming,Gaussian processes,Optimal control,Policy learning,Reinforcement learning},
  file = {/Users/scannea1/Zotero/storage/WSCK8AP2/Deisenroth et al. - 2009 - Gaussian process dynamic programming.pdf;/Users/scannea1/Zotero/storage/LC9VDCYH/S0925231209000162.html}
}

@article{deisenrothGaussian2015,
  title = {Gaussian {{Processes}} for {{Data-Efficient Learning}} in {{Robotics}} and {{Control}}},
  author = {Deisenroth, M. P. and Fox, D. and Rasmussen, C. E.},
  year = {2015},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {2},
  pages = {408--423},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.218},
  abstract = {Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.},
  keywords = {Approximation methods,Bayesian inference,Computational modeling,control,Data models,Gaussian processes,Policy search,Predictive models,Probabilistic logic,reinforcement learning,robotics,Robots,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/EKCHX8MG/Deisenroth et al. - 2015 - Gaussian Processes for Data-Efficient Learning in .pdf;/Users/scannea1/Zotero/storage/ETJFH6IF/6654139.html}
}

@inproceedings{deisenrothPILCO2011,
  title = {{{PILCO}}: {{A Model-Based}} and {{Data-Efficient Approach}} to {{Policy Search}}.},
  shorttitle = {{{PILCO}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Deisenroth, Marc and Rasmussen, Carl},
  year = {2011},
  month = jan,
  volume = {28},
  pages = {465--472},
  abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
  file = {/Users/scannea1/Zotero/storage/9Y6HDXE3/Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf}
}

@inproceedings{depewegLearning2017,
  title = {Learning and Policy Search in Stochastic Dynamical Systems with {{Bayesian}} Neural Networks},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2017 - {{Conference Track Proceedings}}},
  author = {Depeweg, S. and {Hern{\'a}ndez-Lobato}, J. M. and {Doshi-Velez}, F. and Udluft, S.},
  year = {2017},
  file = {/Users/scannea1/Zotero/storage/XDBN68FW/1195629.html}
}

@inproceedings{doerrOptimizing2017,
  title = {Optimizing {{Long-term Predictions}} for {{Model-based Policy Search}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Doerr, Andreas and Daniel, CHristian and {Nguyen-Tuong}, Duy and Marco, Alonso and Schaal, Stefan and Marc, Toussaint and Trimpe, Sebastian},
  year = {2017},
  month = oct,
  pages = {227--238},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose a novel long-term optimization criterion to improve the robustness of model-based reinforcement learning in real-world scenarios. Learning a dynamics model to derive a solution promises...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/T23MBV4T/Doerr et al. - 2017 - Optimizing Long-term Predictions for Model-based P.pdf;/Users/scannea1/Zotero/storage/TUM2S5EH/doerr17a.html}
}

@inproceedings{doerrProbabilistic2018,
  title = {Probabilistic {{Recurrent State-Space Models}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Doerr, A. and Daniel, C. and Schiegg, Martin and {Nguyen-Tuong}, D. and Schaal, S. and Toussaint, Marie-Eve and Trimpe, Sebastian},
  year = {2018},
  abstract = {State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. To overcome this limitation, we propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. In contrast to existing work, the proposed variational approximation allows one to fully capture the latent state temporal correlations. These correlations are the key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.},
  keywords = {gaussian-processes,state-space-model,variational-inference},
  file = {/Users/scannea1/Zotero/storage/JZL52M9S/Doerr et al. - 2018 - Probabilistic Recurrent State-Space Models.pdf}
}

@inproceedings{dongMotion2016,
  title = {Motion {{Planning}} as {{Probabilistic Inference}} Using {{Gaussian Processes}} and {{Factor Graphs}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Dong, Jing and Mukadam, Mustafa and Dellaert, F. and Boots, Byron},
  year = {2016},
  doi = {10.15607/RSS.2016.XII.001},
  abstract = {With the increased use of high degree-of-freedom robots that must perform tasks in real-time, there is a need for fast algorithms for motion planning. In this work, we view motion planning from a probabilistic perspective. We consider smooth continuous-time trajectories as samples from a Gaussian process (GP) and formulate the planning problem as probabilistic inference. We use factor graphs and numerical optimization to perform inference quickly, and we show how GP interpolation can further increase the speed of the algorithm. Our framework also allows us to incrementally update the solution of the planning problem to contend with changing conditions. We benchmark our algorithm against several recent trajectory optimization algorithms on planning problems in multiple environments. Our evaluation reveals that our approach is several times faster than previous algorithms while retaining robustness. Finally, we demonstrate the incremental version of our algorithm on replanning problems, and show that it often can find successful solutions in a fraction of the time required to replan from scratch.},
  file = {/Users/scannea1/Zotero/storage/TYZB7HUM/Dong et al. - 2016 - Motion Planning as Probabilistic Inference using G.pdf}
}

@article{duffieOverview1997a,
  title = {An {{Overview}} of {{Value}} at {{Risk}}},
  author = {Duffie, Darrell and Pan, Jun},
  year = {1997},
  month = feb,
  journal = {The Journal of Derivatives},
  volume = {4},
  number = {3},
  pages = {7--49},
  publisher = {{Institutional Investor Journals Umbrella}},
  issn = {1074-1240, 2168-8524},
  doi = {10.3905/jod.1997.407971},
  chapter = {Primary Article},
  copyright = {\textcopyright{} 1997 Pageant Media Ltd},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/PUPCSZR8/7.html}
}

@book{eduardof.Model2007,
  title = {Model {{Predictive Control}}},
  author = {Eduardo F., Camacho and Carlos, Bordons},
  year = {2007},
  publisher = {{Springer}}
}

@inproceedings{eeckmanSigmoid1988,
  title = {The {{Sigmoid Nonlinearity}} in {{Prepyriform Cortex}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Eeckman, Frank},
  editor = {Anderson, D.},
  year = {1988},
  publisher = {{American Institute of Physics}},
  file = {/Users/scannea1/Zotero/storage/HXJWREHX/60a70bb05b08d6cd95deb3bdb750dce8-Abstract.html}
}

@inproceedings{eleftheriadisIdentification2017,
  title = {Identification of {{Gaussian Process State Space Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eleftheriadis, Stefanos and Nicholson, Tom and Deisenroth, Marc and Hensman, James},
  year = {2017},
  volume = {30},
  pages = {5309--5319},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/5YAC3Z5R/Eleftheriadis et al. - 2017 - Identification of Gaussian Process State Space Mod.pdf;/Users/scannea1/Zotero/storage/EKVSHK87/1006ff12c465532f8c574aeaa4461b16-Abstract.html;/Users/scannea1/Zotero/storage/Y3DYE9YC/1006ff12c465532f8c574aeaa4461b16-Abstract.html}
}

@inproceedings{ertinMaximum2003,
  title = {Maximum {{Mutual Information Principle}} for {{Dynamic Sensor Query Problems}}},
  booktitle = {Information {{Processing}} in {{Sensor Networks}}},
  author = {Ertin, Emre and Fisher, John W. and Potter, Lee C.},
  editor = {Zhao, Feng and Guibas, Leonidas},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {405--416},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36978-3_27},
  abstract = {In this paper we study a dynamic sensor selection method for Bayesian filtering problems. In particular we consider the distributed Bayesian Filtering strategy given in [1] and show that the principle of mutual information maximization follows naturally from the expected uncertainty minimization criterion in a Bayesian filtering framework. This equivalence results in a computationally feasible approach to state estimation in sensor networks. We illustrate the application of the proposed dynamic sensor selection method to both discrete and linear Gaussian models for distributed tracking as well as to stationary target localization using acoustic arrays.},
  isbn = {978-3-540-36978-3},
  langid = {english},
  keywords = {Mutual Information,Sensor Measurement,Sensor Network,Sensor Node,State Space Model},
  file = {/Users/scannea1/Zotero/storage/R5MFG3MW/Ertin et al. - 2003 - Maximum Mutual Information Principle for Dynamic S.pdf}
}

@misc{eschenhagenMixturesLaplaceApproximations2021,
  title = {Mixtures of {{Laplace Approximations}} for {{Improved Post-Hoc Uncertainty}} in {{Deep Learning}}},
  author = {Eschenhagen, Runa and Daxberger, Erik and Hennig, Philipp and Kristiadi, Agustinus},
  year = {2021},
  month = nov,
  number = {arXiv:2111.03577},
  eprint = {2111.03577},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.03577},
  abstract = {Deep neural networks are prone to overconfident predictions on outliers. Bayesian neural networks and deep ensembles have both been shown to mitigate this problem to some extent. In this work, we aim to combine the benefits of the two approaches by proposing to predict with a Gaussian mixture model posterior that consists of a weighted sum of Laplace approximations of independently trained deep neural networks. The method can be used post hoc with any set of pre-trained networks and only requires a small computational and memory overhead compared to regular ensembles. We theoretically validate that our approach mitigates overconfidence "far away" from the training data and empirically compare against state-of-the-art baselines on standard uncertainty quantification benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Eschenhagen et al-2021 Mixtures of Laplace Approximations for Improved Post-Hoc Uncertainty in Deep/Eschenhagen et al_2021_Mixtures of Laplace Approximations for Improved Post-Hoc Uncertainty in Deep.pdf;/Users/scannea1/Zotero/storage/VBCB9Q32/2111.html}
}

@inproceedings{eysenbachRobust2021,
  title = {Robust {{Predictable Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey},
  year = {2021},
  volume = {34},
  pages = {27813--27825},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Zotero/storage/AJVL6SIE/Eysenbach et al. - 2021 - Robust Predictable Control.pdf}
}

@article{faesslerDifferential2018,
  title = {Differential {{Flatness}} of {{Quadrotor Dynamics Subject}} to {{Rotor Drag}} for {{Accurate Tracking}} of {{High-Speed Trajectories}}},
  author = {Faessler, Matthias and Franchi, Antonio and Scaramuzza, Davide},
  year = {2018},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {2},
  pages = {620--626},
  issn = {2377-3766},
  doi = {10.1109/LRA.2017.2776353},
  abstract = {In this letter, we prove that the dynamical model of a quadrotor subject to linear rotor drag effects is differentially flat in its position and heading. We use this property to compute feedforward control terms directly from a reference trajectory to be tracked. The obtained feedforward terms are then used in a cascaded, nonlinear feedback control law that enables accurate agile flight with quadrotors. Compared to the state-of-the-art control methods, which treat the rotor drag as an unknown disturbance, our method reduces the trajectory tracking error significantly. Finally, we present a method based on a gradient-free optimization to identify the rotor drag coefficients, which are required to compute the feedforward control terms. The new theoretical results are thoroughly validated trough extensive comparative experiments.},
  keywords = {Acceleration,Aerial systems,Aerodynamics,Computational modeling,differential flatness,Drag,dynamics,mechanics and control,quadrotor control,Rotors,Trajectory,Trajectory tracking},
  file = {/Users/scannea1/Zotero/storage/4JIJS2IU/Faessler et al. - 2018 - Differential Flatness of Quadrotor Dynamics Subjec.pdf}
}

@inproceedings{fahrooDirect2000,
  title = {Direct Trajectory Optimization by a {{Chebyshev}} Pseudospectral Method},
  booktitle = {Proceedings of the 2000 {{American Control Conference}}},
  author = {Fahroo, F. and Ross, I. M.},
  year = {2000},
  month = jun,
  volume = {6},
  pages = {3860--3864},
  issn = {0743-1619},
  doi = {10.1109/ACC.2000.876945},
  abstract = {A Chebyshev pseudospectral method is presented in this paper for directly solving a generic optimal control problem with state and control constraints. This method employs Nth degree Lagrange polynomial approximations for the state and control variables with the values of these variables at the Chebyshev-Gauss-Lobatto (CGL) points as the expansion coefficients. This process yields a nonlinear programming problem (NLP) with the state and control values at the CGL points as unknown NLP parameters. Numerical examples demonstrate this method yields more accurate results than those obtained from the traditional collocation methods.},
  keywords = {CGL points,Chebyshev approximation,Chebyshev pseudospectral method,Chebyshev-Gauss-Lobatto points,control constraints,Cost function,Differential equations,direct trajectory optimization,Gaussian processes,high-degree Lagrange polynomial approximations,Lagrangian functions,Mathematics,Nonlinear equations,nonlinear programming,nonlinear programming problem,optimal control,Optimal control,optimal control problem,Optimization methods,path planning,Polynomials,spectral analysis,state constraints,unknown NLP parameters},
  file = {/Users/scannea1/Zotero/storage/9RXR2VPM/Fahroo and Ross - 2000 - Direct trajectory optimization by a Chebyshev pseu.pdf;/Users/scannea1/Zotero/storage/CQN2YN69/876945.html}
}

@book{ferberGames1958,
  title = {Games and {{Decisions}}: {{Introduction}} and {{Critical Survey}}},
  shorttitle = {Games and {{Decisions}}},
  author = {Ferber, R. and Luce, R. and Raiffa, H.},
  year = {1958},
  publisher = {{Wiley New York}},
  abstract = {Semantic Scholar extracted view of "Games and Decisions: Introduction and Critical Survey" by R. Ferber et al.}
}

@book{freemanRobust1996,
  title = {Robust {{Nonlinear Control Design}}: {{State-Space}} and {{Lyapunov Techniques}}},
  shorttitle = {Robust {{Nonlinear Control Design}}},
  author = {Freeman, Randy and Kokotovic, Petar V.},
  year = {1996},
  series = {Modern {{Birkh\"auser Classics}}},
  publisher = {{Birkh\"auser Basel}},
  doi = {10.1007/978-0-8176-4759-9},
  abstract = {This book presents advances in the theory and design of robust nonlinear control systems. In the first part of the book, the authors provide a unified framework for state-space and Lyapunov techniques by combining concepts from set-valued analysis, Lyapunov stability theory, and game theory. Within this unified framework, the authors then develop a variety of control design methods suitable for systems described by low-order nonlinear ordinary differential equations. Emphasis is placed on global controller designs, that is, designs for the entire region of model validity. Because linear theory deals well with local system behavior (except for critical cases in which Jacobian linearization fails), the authors focus on achieving robustness and performance for large deviations from a given operation condition. The purpose of the book is to summarize Lyapunov design techniques for nonlinear systems and to raise important issues concerning large-signal robustness and performance. The authors have been the first to address some of these issues, and they report their findings in this text. For example, they identify two potential sources of excessive control effort in Lyapunov design techniques and show how such effort can be greatly reduced. The researcher who wishes to enter the field of robust nonlinear control could use this book as a source of new research topics. For those already active in the field, the book may serve as a reference to a recent body of significant work. Finally, the design engineer faced with a nonlinear control problem will benefit from the techniques presented here. "The text is practically self-contained. The authors offer all necessary definitions and give a comprehensive introduction. Only the most basic knowledge of nonlinear analysis and design tools is required, including Lyapunov stability theory and optimal control. The authors also provide a review of set-valued maps for those readers who are not familiar with set-valued analysis. The book is intended for graduate students and researchers in control theory, serving as both a summary of recent results and a source of new research problems. In the opinion of this reviewer the authors do succeed in attaining these objectives." \textemdash{} Mathematical Reviews},
  isbn = {978-0-8176-4758-2},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/GXRPXABP/9780817647582.html}
}

@book{freemanRobust2009,
  title = {Robust {{Nonlinear Control Design}}: {{State-Space}} and {{Lyapunov Techniques}}},
  shorttitle = {Robust {{Nonlinear Control Design}}},
  author = {Freeman, Randy A. and Kokotovic, Petar V.},
  year = {2009},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book presents advances in the theory and design of robust nonlinear control systems. In the first part of the book, the authors provide a unified framework for state-space and Lyapunov techniques by combining concepts from set-valued analysis, Lyapunov stability theory, and game theory. Within this unified framework, the authors then develop a variety of control design methods suitable for systems described by low-order nonlinear ordinary differential equations. Emphasis is placed on global controller designs, that is, designs for the entire region of model validity. Because linear theory deals well with local system behavior (except for critical cases in which Jacobian linearization fails), the authors focus on achieving robustness and performance for large deviations from a given operation condition. The purpose of the book is to summarize Lyapunov design techniques for nonlinear systems and to raise important issues concerning large-signal robustness and performance. The authors have been the first to address some of these issues, and they report their findings in this text. For example, they identify two potential sources of excessive control effort in Lyapunov design techniques and show how such effort can be greatly reduced. The researcher who wishes to enter the field of robust nonlinear control could use this book as a source of new research topics. For those already active in the field, the book may serve as a reference to a recent body of significant work. Finally, the design engineer faced with a nonlinear control problem will benefit from the techniques presented here. "The text is practically self-contained. The authors offer all necessary definitions and give a comprehensive introduction. Only the most basic knowledge of nonlinear analysis and design tools is required, including Lyapunov stability theory and optimal control. The authors also provide a review of set-valued maps for those readers who are not familiar with set-valued analysis. The book is intended for graduate students and researchers in control theory, serving as both a summary of recent results and a source of new research problems. In the opinion of this reviewer the authors do succeed in attaining these objectives." \textemdash{} Mathematical Reviews},
  googlebooks = {vb\_cBwAAQBAJ},
  isbn = {978-0-8176-4759-9},
  langid = {english},
  keywords = {Language Arts \& Disciplines / Library \& Information Science / General,Mathematics / Differential Equations / General,Mathematics / General,Mathematics / Linear \& Nonlinear Programming,Mathematics / Mathematical Analysis,Science / System Theory}
}

@article{frohlichCautious2020,
  title = {Cautious {{Bayesian Optimization}} for {{Efficient}} and {{Scalable Policy Search}}},
  author = {Fr{\"o}hlich, Lukas P. and Zeilinger, M. and Klenske, Edgar D.},
  year = {2020},
  journal = {undefined},
  abstract = {Sample efficiency is one of the key factors when applying policy search to real-world problems. In recent years, Bayesian Optimization (BO) has become prominent in the field of robotics due to its sample efficiency and little prior knowledge needed. However, one drawback of BO is its poor performance on high-dimensional search spaces as it focuses on global search. In the policy search setting, local optimization is typically sufficient as initial policies are often available, e.g., via meta-learning, kinesthetic demonstrations or sim-to-real approaches. In this paper, we propose to constrain the policy search space to a sublevel-set of the Bayesian surrogate model\&\#39;s predictive uncertainty. This simple yet effective way of constraining the policy update enables BO to scale to high-dimensional spaces (\&gt;100) as well as reduces the risk of damaging the system. We demonstrate the effectiveness of our approach on a wide range of problems, including a motor skills task, adapting deep RL agents to new reward signals and a sim-to-real task for an inverted pendulum system.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/A3PDNMG4/Fröhlich et al. - 2020 - Cautious Bayesian Optimization for Efficient and S.pdf;/Users/scannea1/Zotero/storage/XRS7FCMY/a6b6de5bcb1f609aed6d78d735622ab196455700.html}
}

@inproceedings{gaddEnriched2020,
  title = {Enriched Mixtures of Generalised {{Gaussian}} Process Experts},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gadd, Charles and Wade, Sara and Boukouvalas, Alexis},
  year = {2020},
  month = jun,
  pages = {3144--3154},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Mixtures of experts probabilistically divide the input space into regions, where the assumptions of each expert, or conditional model, need only hold locally. Combined with Gaussian process (GP) experts, this results in a powerful and highly flexible model. We focus on alternative mixtures of GP experts, which  model the joint distribution of the inputs and targets explicitly. We highlight issues of this approach in multi-dimensional input spaces, namely,  poor scalability and the need for an unnecessarily large number of experts, degrading the predictive performance and increasing uncertainty. We construct a novel model to address these issues through a nested partitioning scheme that automatically infers the number of components at both levels. Multiple response types are accommodated through a generalised GP framework, while multiple input types are included through a factorised exponential family structure. We show the effectiveness of our approach in estimating a parsimonious probabilistic description of both  synthetic data of increasing dimension and an Alzheimer's challenge dataset.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/77MNIVHX/Gadd et al. - 2020 - Enriched mixtures of generalised Gaussian process .pdf;/Users/scannea1/Zotero/storage/GXCW2FXW/Gadd et al. - 2020 - Enriched mixtures of generalised Gaussian process .pdf}
}

@inproceedings{galImproving2016,
  title = {Improving {{PILCO}} with {{Bayesian Neural Network Dynamics Models}}},
  booktitle = {{{ICML Workshop}} on {{Data-Efficient Machine Learning}}},
  author = {Gal, Yarin and McAllister, Rowan and Rasmussen, Carl},
  year = {2016},
  abstract = {PILCO's framework is extended to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality, and it is shown that moment matching is a crucial simplifying assumption made by the model. Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efficiency can be further improved with a probabilistic model of the agent's ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO [1] being a prominent example, achieving state-of-theart data efficiency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps [2]. In this paper we extend PILCO's framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/7CC35VP4/127d856c8b74d3e54a2f7da7b11b784014832ed9.html}
}

@article{gargUnified2010,
  title = {A Unified Framework for the Numerical Solution of Optimal Control Problems Using Pseudospectral Methods},
  author = {Garg, Divya and Patterson, Michael and Hager, William W. and Rao, Anil V. and Benson, David A. and Huntington, Geoffrey T.},
  year = {2010},
  month = nov,
  journal = {Automatica},
  volume = {46},
  number = {11},
  pages = {1843--1851},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.06.048},
  abstract = {A unified framework is presented for the numerical solution of optimal control problems using collocation at Legendre\textendash Gauss (LG), Legendre\textendash Gauss\textendash Radau (LGR), and Legendre\textendash Gauss\textendash Lobatto (LGL) points. It is shown that the LG and LGR differentiation matrices are rectangular and full rank whereas the LGL differentiation matrix is square and singular. Consequently, the LG and LGR schemes can be expressed equivalently in either differential or integral form, while the LGL differential and integral forms are not equivalent. Transformations are developed that relate the Lagrange multipliers of the discrete nonlinear programming problem to the costates of the continuous optimal control problem. The LG and LGR discrete costate systems are full rank while the LGL discrete costate system is rank-deficient. The LGL costate approximation is found to have an error that oscillates about the true solution and this error is shown by example to be due to the null space in the LGL discrete costate system. An example is considered to assess the accuracy and features of each collocation scheme.},
  langid = {english},
  keywords = {Nonlinear programming,Optimal control,Pseudospectral methods},
  file = {/Users/scannea1/Zotero/storage/T9YSQEZM/Garg et al. - 2010 - A unified framework for the numerical solution of .pdf;/Users/scannea1/Zotero/storage/NSLYL44L/S0005109810002980.html}
}

@inproceedings{gelbartBayesian2014,
  title = {{Bayesian optimization with unknown constraints}},
  booktitle = {{Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014}},
  author = {Gelbart, Michael A. and Snoek, Jasper and Adams, Ryan P.},
  year = {2014},
  pages = {250--259},
  publisher = {{AUAI Press}},
  langid = {English (US)},
  file = {/Users/scannea1/Zotero/storage/VG2LGR3L/bayesian-optimization-with-unknown-constraints.html}
}

@inproceedings{ghahramaniLearning1999,
  title = {Learning {{Nonlinear Dynamical Systems}} Using an {{EM Algorithm}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 11},
  author = {Ghahramani, Zoubin and Roweis, Sam T.},
  year = {1999},
  pages = {599--605},
  publisher = {{MIT Press}},
  abstract = {The Expectation Maximization (EM) algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables[2]. It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9]. We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems. The "expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the "maximization" step re-estimates the parameters using these uncertain state estimates. In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states. However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.},
  file = {/Users/scannea1/Zotero/storage/72TJ6IPJ/Ghahramani and Roweis - 1999 - Learning Nonlinear Dynamical Systems using an EM A.pdf;/Users/scannea1/Zotero/storage/MX5NPPSJ/summary.html}
}

@inproceedings{ghahramaniSwitching1996,
  title = {Switching {{State-Space Models}}},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {1996},
  doi = {10.1007/978-0-387-35768-3_13},
  abstract = {We introduce a statistical model for times series data with nonlinear dynamics which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of those regimes. This model combines and generalizes two of the most widely used stochastic time series models|the hidden Markov model and the linear dynamical system|and is related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied. However, we present a variational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward\{backward recursions for hidden Markov models and the Kalman lter recursions for linear dynamical systems.\vphantom\}},
  file = {/Users/scannea1/Zotero/storage/QDGJI8XS/11-1999.pdf}
}

@techreport{ghahramaniSwitching1996a,
  title = {Switching {{State-Space Models}}},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {1996},
  institution = {{King's College Road, Toronto M5S 3H5}},
  abstract = {We introduce a statistical model for non-linear time series which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time series models---the hidden Markov model and the linear dynamical system---and is related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied. However, we present a variational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward--backward recursions for hidden Markov mo...},
  file = {/Users/scannea1/Zotero/storage/6FJPB3J2/Ghahramani and Hinton - 1996 - Switching State-Space Models.pdf;/Users/scannea1/Zotero/storage/8GP9X52Q/download.html}
}

@article{ghahramaniVariational1998,
  title = {Variational Learning for Switching State-Space Models},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {1998},
  journal = {Neural Computation},
  volume = {12},
  pages = {963--996},
  abstract = {We introduce a new statistical model for time series which iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time series models -- hidden Markov models and linear dynamical systems -- and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log likelihood and makes use of both the forward-backward recursions for hidden Markov models and the Kalman lter recursions for linear dynamical systems. We tested the algorithm both on artificial data sets and on a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
  file = {/Users/scannea1/Zotero/storage/D9J5HK5G/Ghahramani and Hinton - 1998 - Variational learning for switching state-space mod.pdf;/Users/scannea1/Zotero/storage/P9IELXW9/summary.html}
}

@article{ghahramaniVariational2000,
  title = {Variational {{Learning}} for {{Switching State-Space Models}}},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {2000},
  month = apr,
  journal = {Neural Computation},
  volume = {12},
  number = {4},
  pages = {831--864},
  issn = {0899-7667},
  doi = {10.1162/089976600300015619},
  abstract = {We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models\textemdash hidden Markov models and linear dynamical systems\textemdash and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, \& Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
  file = {/Users/scannea1/Zotero/storage/RW4JB25F/6789465.html}
}

@misc{ghugareSimplifyingModelbasedRL2022,
  title = {Simplifying {{Model-based RL}}: {{Learning Representations}}, {{Latent-space Models}}, and {{Policies}} with {{One Objective}}},
  shorttitle = {Simplifying {{Model-based RL}}},
  author = {Ghugare, Raj and Bharadhwaj, Homanga and Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  year = {2022},
  month = sep,
  number = {arXiv:2209.08466},
  eprint = {2209.08466},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.08466},
  abstract = {While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-free RL methods. While such sample efficient methods typically are computationally demanding, our method attains the performance of SAC in about 50\textbackslash\% less wall-clock time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ghugare et al-2022 Simplifying Model-based RL/Ghugare et al_2022_Simplifying Model-based RL.pdf;/Users/scannea1/Zotero/storage/VITWFA8M/2209.html}
}

@phdthesis{girardApproximate2004,
  title = {Approximate {{Methods}} for {{Propagation}} of {{Uncertainty}} with {{Gaussian Process Models}}},
  author = {Girard, Agathe},
  year = {2004},
  abstract = {This thesis presents extensions of the Gaussian Process (GP) model, based on approximate methods allowing the model to deal with input uncertainty. Zero-mean GPs with Gaussian covariance function are of particular interest, as they allow to carry out many derivations exactly, as well as having been shown to have modelling abilities and predictive performance comparable to that of neural networks (Rasmussen, 1996a). With this model, given observed data and a new input, making a prediction corresponds to computing the (Gaussian) predictive distribution of the associated output, whose mean can be used as an estimate. This way, the predictive variance provides error-bars or confidence intervals on this estimate: It quantifies the model's degree of belief in its `best guess'. Using the knowledge of the predictive variance in an informative manner is at the centre of this thesis, as the problems of how to propagate it in the model, how to account for it when derivative observations are available, and how to derive a control law with a cautious behaviour are addressed.},
  langid = {english},
  school = {University of Glasgow},
  file = {/Users/scannea1/Zotero/storage/HQFVDXPM/Girard - Approximate Methods for Propagation of Uncertainty.pdf}
}

@incollection{girardGaussian2003,
  title = {Gaussian {{Process}} Priors with Uncertain Inputs? {{Application}} to Multiple-Step Ahead Time Series Forecasting},
  booktitle = {Becker, {{S}}},
  author = {Girard, Agathe and Rasmussen, Carl Edward and {Quinonero-Candela}, Joaquin and {Murray-Smith}, Roderick},
  year = {2003},
  publisher = {{MIT Press}},
  address = {{Vancouver, Canada}},
  abstract = {We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form y t = f(Yt-1 ,..., Yt-L ), the prediction of y at time t + k is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.},
  isbn = {978-0-262-02550-8},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/2RIMIIAU/Girard et al. - 2003 - Gaussian Process priors with uncertain inputs App.pdf;/Users/scannea1/Zotero/storage/36RXX8DX/3117.html}
}

@inproceedings{gonzalezGLASSES2015,
  title = {{{GLASSES}}: {{Relieving The Myopia Of Bayesian Optimisation}}},
  shorttitle = {{{GLASSES}}},
  booktitle = {Proceedings of the {{Nineteenth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gonz{\'a}lez, Javier and Osborne, Michael and Lawrence, Neil D.},
  year = {2015},
  month = oct,
  eprint = {1510.06299},
  eprinttype = {arxiv},
  abstract = {We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss.We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/QFYN2BAH/González et al. - 2015 - GLASSES Relieving The Myopia Of Bayesian Optimisa.pdf;/Users/scannea1/Zotero/storage/LVQ7D5AY/1510.html}
}

@article{GPflow2017,
  title = {{{GPflow}}: {{A Gaussian}} Process Library Using {{TensorFlow}}},
  author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke. and Boukouvalas, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
  year = {2017},
  month = apr,
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {40},
  pages = {1--6}
}

@inproceedings{haarnojaSoft2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = jul,
  pages = {1861--1870},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major cha...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/8HLAHQGR/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf;/Users/scannea1/Zotero/storage/SVVM4X8W/haarnoja18b.html}
}

@inproceedings{haesaerts.Datadriven2015,
  title = {Data-Driven and Model-Based Verification: A {{Bayesian}} Identification Approach},
  shorttitle = {Data-Driven and Model-Based Verification},
  booktitle = {Proceedings of the {{Conference}} on {{Decision}} and {{Control}}, 15-18 {{December}} 2015, {{Osaka}}, {{Japan}}},
  author = {{Haesaert, S.} and {van den Hof, P.M.J.} and {Abate, A.} and {Control Systems} and {Dynamic Networks: Data-Driven Modeling and Control} and {Formal methods for control of cyber-physical systems}},
  year = {2015},
  month = sep,
  pages = {6830--6835},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  doi = {10.1109/cdc.2015.7403295},
  abstract = {This work develops a measurement-driven and model-based formal verification approach, applicable to systems with partly unknown dynamics. We provide a principled method, grounded on reachability analysis and on Bayesian inference, to compute the confidence that a physical system driven by external inputs and accessed under noisy measurements, verifies a temporal logic property. A case study is discussed, where we investigate the bounded- and unbounded-time safety of a partly unknown linear time invariant system.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/DML32MZN/Haesaert, S. et al. - 2015 - Data-driven and model-based verification a Bayesi.pdf}
}

@inproceedings{hafnerLearning2019,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  year = {2019},
  month = may,
  pages = {2555--2565},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the w...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/36E8ER7E/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf;/Users/scannea1/Zotero/storage/HB9RTJLG/hafner19a.html}
}

@inproceedings{hafnerMasteringAtariDiscrete2022,
  title = {Mastering {{Atari}} with {{Discrete World Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hafner, Danijar and Lillicrap, Timothy P. and Norouzi, Mohammad and Ba, Jimmy},
  year = {2022},
  month = feb,
  abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hafner et al-2022 Mastering Atari with Discrete World Models/Hafner et al_2022_Mastering Atari with Discrete World Models.pdf;/Users/scannea1/Zotero/storage/S86WW9BM/forum.html}
}

@misc{hansenTemporal2022,
  title = {Temporal {{Difference Learning}} for {{Model Predictive Control}}},
  author = {Hansen, Nicklas and Wang, Xiaolong and Su, Hao},
  year = {2022},
  month = mar,
  number = {arXiv:2203.04955},
  eprint = {2203.04955},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Data-driven model predictive control has two key advantages over model-free methods: a potential for improved sample efficiency through model learning, and better performance as computational budget for planning increases. However, it is both costly to plan over long horizons and challenging to obtain an accurate model of the environment. In this work, we combine the strengths of model-free and model-based methods. We use a learned task-oriented latent dynamics model for local trajectory optimization over a short horizon, and use a learned terminal value function to estimate long-term return, both of which are learned jointly by temporal difference learning. Our method, TD-MPC, achieves superior sample efficiency and asymptotic performance over prior work on both state and image-based continuous control tasks from DMControl and Meta-World. Code and video results are available at https://nicklashansen.github.io/td-mpc.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/FMLK6LRN/Hansen et al. - 2022 - Temporal Difference Learning for Model Predictive .pdf;/Users/scannea1/Zotero/storage/LXLBZERU/2203.html}
}

@misc{haoRegret2022,
  title = {Regret {{Bounds}} for {{Information-Directed Reinforcement Learning}}},
  author = {Hao, Botao and Lattimore, Tor},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04640},
  eprint = {2206.04640},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  abstract = {Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning (RL). However, theoretical understanding of IDS for Markov Decision Processes (MDPs) is still limited. We develop novel information-theoretic tools to bound the information ratio and cumulative information gain about the learning target. Our theoretical results shed light on the importance of choosing the learning target such that the practitioners can balance the computation and regret bounds. As a consequence, we derive prior-free Bayesian regret bounds for vanilla-IDS which learns the whole environment under tabular finite-horizon MDPs. In addition, we propose a computationally-efficient regularized-IDS that maximizes an additive form rather than the ratio form and show that it enjoys the same regret bound as vanilla-IDS. With the aid of rate-distortion theory, we improve the regret bound by learning a surrogate, less informative environment. Furthermore, we extend our analysis to linear MDPs and prove similar regret bounds for Thompson sampling as a by-product.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/scannea1/Zotero/storage/4CL6EINE/Hao and Lattimore - 2022 - Regret Bounds for Information-Directed Reinforceme.pdf;/Users/scannea1/Zotero/storage/8435UCVT/2206.html}
}

@inproceedings{haubergGeometric2012,
  title = {A {{Geometric}} Take on {{Metric Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hauberg, S{\o}ren and Freifeld, Oren and Black, Michael J},
  year = {2012},
  pages = {9},
  abstract = {Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.},
  langid = {english},
  keywords = {geodesics,geometric-learning,riemannian},
  file = {/Users/scannea1/Zotero/storage/JSWHM5S9/Hauberg et al. - A Geometric take on Metric Learning.pdf}
}

@inproceedings{heDeep2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {/Users/scannea1/Zotero/storage/6HAE7E65/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/Users/scannea1/Zotero/storage/YV6ZAGER/citations.html}
}

@inproceedings{heessLearningContinuousControl2015,
  title = {Learning {{Continuous Control Policies}} by {{Stochastic Value Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We present a unified framework for learning continuous control policies usingbackpropagation. It supports stochastic control by treating stochasticity in theBellman equation as a deterministic function of exogenous noise. The productis a spectrum of general policy gradient algorithms that range from model-freemethods with value functions to model-based methods without value functions.We use learned models but only require observations from the environment insteadof observations from model-predicted trajectories, minimizing the impactof compounded model errors. We apply these algorithms first to a toy stochasticcontrol problem and then to several physics-based control problems in simulation.One of these variants, SVG(1), shows the effectiveness of learning models, valuefunctions, and policies simultaneously in continuous domains.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Heess et al-2015 Learning Continuous Control Policies by Stochastic Value Gradients/Heess et al_2015_Learning Continuous Control Policies by Stochastic Value Gradients.pdf}
}

@article{hehnRealTime2015,
  title = {Real-{{Time Trajectory Generation}} for {{Quadrocopters}}},
  author = {Hehn, Markus and D'Andrea, Raffaello},
  year = {2015},
  month = aug,
  journal = {IEEE Transactions on Robotics},
  volume = {31},
  number = {4},
  pages = {877--892},
  issn = {1941-0468},
  doi = {10.1109/TRO.2015.2432611},
  abstract = {This paper presents a trajectory generation algorithm that efficiently computes high-performance flight trajectories that are capable of moving a quadrocopter from a large class of initial states to a given target point that will be reached at rest. The approach consists of planning separate trajectories in each of the three translational degrees of freedom, and ensuring feasibility by deriving decoupled constraints for each degree of freedom through approximations that preserve feasibility. The presented algorithm can compute a feasible trajectory within tens of microseconds on a laptop computer; remaining computation time can be used to iteratively improve the trajectory. By replanning the trajectory at a high rate, the trajectory generator can be used as an implicit feedback law similar to model predictive control. The solutions generated by the algorithm are analyzed by comparing them with time-optimal motions, and experimental results validate the approach.},
  keywords = {Acceleration,Heuristic algorithms,Motion control,optimal control,quadrocopter,Real-time systems,Robots,Trajectory,trajectory generation,unmanned aerial vehicles,Vehicle dynamics,Vehicles},
  file = {/Users/scannea1/Zotero/storage/XM4Y3DUK/Hehn and D’Andrea - 2015 - Real-Time Trajectory Generation for Quadrocopters.pdf;/Users/scannea1/Zotero/storage/LUYRASXX/7128399.html}
}

@article{hennigEntropy2012,
  title = {Entropy {{Search}} for {{Information-Efficient Global Optimization}}},
  author = {Hennig, Philipp and Schuler, Christian J.},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  pages = {1809--1837},
  abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
  file = {/Users/scannea1/Zotero/storage/9HLX5WWP/Hennig and Schuler - 2012 - Entropy Search for Information-Efficient Global Op.pdf}
}

@article{hennigProbabilistic2015,
  title = {Probabilistic Numerics and Uncertainty in Computations},
  author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
  year = {2015},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {471},
  number = {2179},
  pages = {20150142},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2015.0142},
  abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data have led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimizers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.},
  keywords = {inference,numerical methods,probability,statistics},
  file = {/Users/scannea1/Zotero/storage/3PDGNY4K/Hennig et al. - 2015 - Probabilistic numerics and uncertainty in computat.pdf}
}

@inproceedings{hensmanGaussian2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},
  year = {2013},
  volume = {29},
  pages = {282--290},
  abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  langid = {english},
  keywords = {gaussian-processes,sparse-gaussian-processes,variational-inference},
  file = {/Users/scannea1/Zotero/storage/KLY9PMZH/Hensman et al. - Gaussian Processes for Big Data.pdf}
}

@inproceedings{hensmanScalable2015,
  title = {Scalable {{Variational Gaussian Process Classification}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Hensman, James and Matthews, Alexander and Ghahramani, Zoubin},
  year = {2015},
  month = feb,
  pages = {351--360},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, out-performing the state of ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/9HDJVCTJ/Hensman et al. - 2015 - Scalable Variational Gaussian Process Classificati.pdf;/Users/scannea1/Zotero/storage/FD6TZPEI/hensman15.html}
}

@article{herzallahPMAC2020,
  title = {{{PMAC}}: Probabilistic Multimodality Adaptive Control},
  shorttitle = {{{PMAC}}},
  author = {Herzallah, Randa and Lowe, David},
  year = {2020},
  month = jul,
  journal = {International Journal of Control},
  volume = {93},
  number = {7},
  pages = {1637--1650},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207179.2018.1523567},
  abstract = {This paper develops a probabilistic multimodal adaptive control approach for systems that are characterised by temporal multimodality where the system dynamics are subject to abrupt mode switching at arbitrary times. In this framework, the control objective is redefined such that it utilises the complete probability distribution of the system dynamics. The derived probabilistic control law is thus of a dual type that incorporates the functional uncertainty of the controlled system. A multi-modal density model with prediction error-dependent mixing coefficients is introduced to effect the mode switching. This approach can deal with arbitrary noise distributions, nonlinear plant dynamics and arbitrary mode switching. For the affine systems focussed upon for illustration in this paper the approach has global stability. The theoretical architecture constructs are verified by validation on a simulation example.},
  keywords = {Multi-modal density model,multiobjective probabilistic control,operator Riccati equation,probabilistic multimodal adaptive control,switching control,temporal multimodality},
  annotation = {\_eprint: https://doi.org/10.1080/00207179.2018.1523567},
  file = {/Users/scannea1/Zotero/storage/SKYIVLD6/Herzallah and Lowe - 2020 - PMAC probabilistic multimodality adaptive control.pdf;/Users/scannea1/Zotero/storage/4Z7VFDS9/00207179.2018.html}
}

@article{hewingCautious2020,
  title = {Cautious {{Model Predictive Control Using Gaussian Process Regression}}},
  author = {Hewing, Lukas and Kabzan, Juraj and Zeilinger, Melanie N.},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {28},
  number = {6},
  pages = {2736--2743},
  issn = {1558-0865},
  doi = {10.1109/TCST.2019.2949757},
  abstract = {Gaussian process (GP) regression has been widely used in supervised machine learning due to its flexibility and inherent ability to describe uncertainty in function estimation. In the context of control, it is seeing increasing use for modeling of nonlinear dynamical systems from data, as it allows the direct assessment of residual model uncertainty. We present a model predictive control (MPC) approach that integrates a nominal system with an additive nonlinear part of the dynamics modeled as a GP. We describe a principled way of formulating the chance-constrained MPC problem, which takes into account residual uncertainties provided by the GP model to enable cautious control. Using additional approximations for efficient computation, we finally demonstrate the approach in a simulation example, as well as in a hardware implementation for autonomous racing of remote-controlled race cars with fast sampling times of 20 ms, highlighting improvements with regard to both performance and safety over a nominal controller.},
  keywords = {Autonomous racing,Computational modeling,Data models,Gaussian processes,Gaussian processes (GPs),Kernel,learning-based control,model learning,model predictive control (MPC),Predictive control,Predictive models,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/5NP9WH9Y/Hewing et al. - 2020 - Cautious Model Predictive Control Using Gaussian P.pdf;/Users/scannea1/Zotero/storage/5LZ4583Z/8909368.html}
}

@inproceedings{hewingCorrespondence2018,
  title = {On a {{Correspondence}} between {{Probabilistic}} and {{Robust Invariant Sets}} for {{Linear Systems}}},
  booktitle = {2018 {{European Control Conference}} ({{ECC}})},
  author = {Hewing, Lukas and Carron, Andrea and Wabersich, Kim P. and Zeilinger, Melanie N.},
  year = {2018},
  month = jun,
  pages = {1642--1647},
  doi = {10.23919/ECC.2018.8550160},
  abstract = {Dynamical systems with stochastic uncertainties are ubiquitous in the field of control, with linear systems under additive Gaussian disturbances a most prominent example. The concept of probabilistic invariance was introduced to extend the widely applied concept of invariance to this class of problems. Computational methods for their synthesis, however, are limited. In this paper we present a relationship between probabilistic and robust invariant sets for linear systems, which enables the use of well-studied robust design methods. Conditions are shown, under which a robust invariant set, designed with a confidence region of the disturbance, results in a probabilistic invariant set. We furthermore show that this condition holds for common box and ellipsoidal confidence regions, generalizing and improving existing results for probabilistic invariant set computation. We finally exemplify the synthesis for an ellipsoidal probabilistic invariant set. Two numerical examples demonstrate the approach and the advantages to be gained from exploiting robust computations for probabilistic invariant sets.},
  keywords = {Additives,Control systems,Gaussian distribution,Linear systems,Probabilistic logic,Random variables,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/A7RQGY5L/Hewing et al. - 2018 - On a Correspondence between Probabilistic and Robu.pdf;/Users/scannea1/Zotero/storage/L4TEPLYJ/8550160.html}
}

@article{hewingLearningBased2020,
  ids = {hewingLearningBased2020a},
  title = {Learning-{{Based Model Predictive Control}}: {{Toward Safe Learning}} in {{Control}}},
  shorttitle = {Learning-{{Based Model Predictive Control}}},
  author = {Hewing, Lukas and Wabersich, Kim P. and Menner, Marcel and Zeilinger, Melanie N.},
  year = {2020},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {3},
  number = {1},
  pages = {269--296},
  doi = {10.1146/annurev-control-090419-075625},
  abstract = {Recent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive control (MPC), as the prime methodology for constrained control, offers a significant opportunity to exploit the abundance of data in a reliable manner, particularly while taking safety constraints into account. This review aims at summarizing and categorizing previous research on learning-based MPC, i.e., the integration or combination of MPC with learning methods, for which we consider three main categories. Most of the research addresses learning for automatic improvement of the prediction model from recorded data. There is, however, also an increasing interest in techniques to infer the parameterization of the MPC controller, i.e., the cost and constraints, that lead to the best closed-loop performance. Finally, we discuss concepts that leverage MPC to augment learning-based controllers with constraint satisfaction properties.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-control-090419-075625},
  file = {/Users/scannea1/Zotero/storage/5M3WTY63/Hewing et al. - 2020 - Learning-Based Model Predictive Control Toward Sa.pdf}
}

@inproceedings{hewingSimulation2020,
  title = {On {{Simulation}} and {{Trajectory Prediction}} with {{Gaussian Process Dynamics}}},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {Hewing, Lukas and Arcari, Elena and Fr{\"o}hlich, Lukas P. and Zeilinger, Melanie N.},
  year = {2020},
  month = jul,
  pages = {424--434},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Established techniques for simulation and prediction with Gaussian process (GP) dynamics implicitly make use of an independence assumption on successive function evaluations of the dynamics model. ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/SLW3YVJJ/Hewing et al. - 2020 - On Simulation and Trajectory Prediction with Gauss.pdf;/Users/scannea1/Zotero/storage/PM5RAFJM/hewing20a.html}
}

@inproceedings{hewingStochastic2018,
  title = {Stochastic {{Model Predictive Control}} for {{Linear Systems Using Probabilistic Reachable Sets}}},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Hewing, Lukas and Zeilinger, Melanie N.},
  year = {2018},
  month = dec,
  pages = {5182--5188},
  issn = {2576-2370},
  doi = {10.1109/CDC.2018.8619554},
  abstract = {In this paper, we propose a stochastic model predictive control (MPC) algorithm for linear discrete-time systems affected by possibly unbounded additive disturbances and subject to probabilistic constraints. Constraints are treated in analogy to robust MPC using a constraint tightening based on the concept of probabilistic reachable sets, which is shown to provide closed-loop fulfillment of chance constraints under a unimodality assumption on the disturbance distribution. A control scheme reverting to a backup solution from a previous time step in case of infeasibility is proposed, for which an asymptotic average performance bound is derived. Two examples illustrate the approach, highlighting closed-loop chance constraint satisfaction and the benefits of the proposed controller in the presence of unmodeled disturbances.},
  keywords = {Additives,Chebyshev approximation,Linear systems,Predictive control,Probabilistic logic,Random variables,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/D74JVUVJ/Hewing and Zeilinger - 2018 - Stochastic Model Predictive Control for Linear Sys.pdf;/Users/scannea1/Zotero/storage/KCQRLML6/8619554.html}
}

@article{hoBayesian1964,
  title = {A {{Bayesian}} Approach to Problems in Stochastic Estimation and Control},
  author = {Ho, Y. and Lee, R.},
  year = {1964},
  journal = {IEEE Transactions on Automatic Control},
  volume = {9},
  pages = {382--387},
  doi = {10.1109/JACC.1964.4168717},
  abstract = {In this paper, a general class of stochastic estimation and control problems is formulated from the Bayesian Decision-Theoretic viewpoint. A discussion as to how these problems can be solved step by step in principle and practice from this approach is presented. As a specific example, the closed form Wiener-Kalman solution for linear estimation in Gaussian noise is derived. The purpose of the paper is to show that the Bayesian approach provides; 1) a general unifying framework within which to pursue further researches in stochastic estimation and control problems, and 2) the necessary computations and difficulties that must be overcome for these problems. An example of a nonlinear, non-Gaussian estimation problem is also solved.}
}

@article{hoffmanStochastic2013,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {4},
  pages = {1303--1347},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  file = {/Users/scannea1/Zotero/storage/9MQ4G5JD/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf}
}

@article{houlsbyBayesian2011,
  title = {Bayesian {{Active Learning}} for {{Classification}} and {{Preference Learning}}},
  author = {Houlsby, Neil and Husz{\'a}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'a}t{\'e}},
  year = {2011},
  month = dec,
  journal = {arXiv:1112.5745 [cs, stat]},
  eprint = {1112.5745},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/NWQLADHZ/Houlsby et al. - 2011 - Bayesian Active Learning for Classification and Pr.pdf;/Users/scannea1/Zotero/storage/JGWGTBSD/1112.html}
}

@article{houthooftVIME2017,
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  shorttitle = {{{VIME}}},
  author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  year = {2017},
  month = jan,
  journal = {arXiv:1605.09674 [cs, stat]},
  eprint = {1605.09674},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/JLYSSI3G/Houthooft et al. - 2017 - VIME Variational Information Maximizing Explorati.pdf;/Users/scannea1/Zotero/storage/PTVKGQUZ/1605.html}
}

@inproceedings{huActiveUncertaintyReduction2023,
  title = {Active {{Uncertainty Reduction}} for {{Human-Robot Interaction}}: {{An Implicit Dual Control Approach}}},
  shorttitle = {Active {{Uncertainty Reduction}} for {{Human-Robot Interaction}}},
  booktitle = {Algorithmic {{Foundations}} of {{Robotics XV}}},
  author = {Hu, Haimin and Fisac, Jaime F.},
  editor = {LaValle, Steven M. and O'Kane, Jason M. and Otte, Michael and Sadigh, Dorsa and Tokekar, Pratap},
  year = {2023},
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}},
  pages = {385--401},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-21090-7_23},
  abstract = {The ability to accurately predict human behavior is central to the safety and efficiency of robot autonomy in interactive settings. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as people's goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning, mainly due to the fundamental coupling between robot trajectory optimization and human intent inference. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by real-time gradient-based optimization methods. The resulting policy is shown to preserve the dual control effect for a broad class of predictive human models with both continuous and categorical uncertainty. The efficacy of our approach is demonstrated with simulated driving examples.},
  isbn = {978-3-031-21090-7},
  langid = {english},
  keywords = {Dual control theory,Human-robot interaction,Stochastic MPC},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hu_Fisac-2023 Active Uncertainty Reduction for Human-Robot Interaction/Hu_Fisac_2023_Active Uncertainty Reduction for Human-Robot Interaction.pdf}
}

@inproceedings{immerScalable2021,
  title = {Scalable {{Marginal Likelihood Estimation}} for {{Model Selection}} in {{Deep Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Immer, Alexander and Bauer, Matthias and Fortuin, Vincent and R{\"a}tsch, Gunnar and Emtiyaz, Khan Mohammad},
  year = {2021},
  month = jul,
  pages = {4563--4573},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Marginal-likelihood based model-selection, even though promising, is rarely used in deep learning due to estimation difficulties. Instead, most approaches rely on validation data, which may not be readily available. In this work, we present a scalable marginal-likelihood estimation method to select both hyperparameters and network architectures, based on the training data alone. Some hyperparameters can be estimated online during training, simplifying the procedure. Our marginal-likelihood estimate is based on Laplace's method and Gauss-Newton approximations to the Hessian, and it outperforms cross-validation and manual tuning on standard regression and image classification datasets, especially in terms of calibration and out-of-distribution detection. Our work shows that marginal likelihoods can improve generalization and be useful when validation data is unavailable (e.g., in nonstationary settings).},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/FGT3XJXL/Immer et al. - 2021 - Scalable Marginal Likelihood Estimation for Model .pdf;/Users/scannea1/Zotero/storage/ZQZQAGS7/Immer et al. - 2021 - Scalable Marginal Likelihood Estimation for Model .pdf}
}

@inproceedings{izmailovWhat2021,
  title = {What {{Are Bayesian Neural Network Posteriors Really Like}}?},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D. and Wilson, Andrew Gordon Gordon},
  year = {2021},
  month = jul,
  pages = {4629--4640},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a ``cold posterior'' effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC can provide good generalization, their predictive distributions are distinct from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4BHLCR25/Izmailov et al. - 2021 - What Are Bayesian Neural Network Posteriors Really.pdf;/Users/scannea1/Zotero/storage/E45S87FZ/Izmailov et al. - 2021 - What Are Bayesian Neural Network Posteriors Really.pdf}
}

@article{jacobsAdaptive1991,
  title = {Adaptive {{Mixtures}} of {{Local Experts}}},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = {1991},
  month = mar,
  journal = {Neural Computation},
  volume = {3},
  number = {1},
  pages = {79--87},
  issn = {0899-7667},
  doi = {10.1162/neco.1991.3.1.79},
  abstract = {We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.},
  file = {/Users/scannea1/Zotero/storage/85CFX29F/Jacobs et al. - 1991 - Adaptive Mixtures of Local Experts.pdf;/Users/scannea1/Zotero/storage/EYULD8LB/Adaptive-Mixtures-of-Local-Experts.html}
}

@article{jacobsonDifferential1970,
  title = {Differential Dynamic Programming},
  author = {Jacobson, David H. and Mayne, David Q.},
  year = {1970},
  publisher = {{North-Holland}}
}

@inproceedings{jagtapControl2020,
  title = {Control {{Barrier Functions}} for {{Unknown Nonlinear Systems}} Using {{Gaussian Processes}}*},
  booktitle = {{{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Jagtap, Pushpak and Pappas, George J. and Zamani, M.},
  year = {2020},
  volume = {59},
  publisher = {{IEEE}},
  doi = {10.1109/CDC42340.2020.9303847},
  abstract = {This paper focuses on the controller synthesis for unknown, nonlinear systems while ensuring safety constraints. Our approach consists of two steps, a learning step that uses Gaussian processes and a controller synthesis step that is based on control barrier functions. In the learning step, we use a data-driven approach utilizing Gaussian processes to learn the unknown control affine nonlinear dynamics together with a statistical bound on the accuracy of the learned model. In the second controller synthesis steps, we develop a systematic approach to compute control barrier functions that explicitly take into consideration the uncertainty of the learned model. The control barrier function not only results in a safe controller by construction but also provides a rigorous lower bound on the probability of satisfaction of the safety specification. Finally, we illustrate the effectiveness of the proposed results by synthesizing a safety controller for a jet engine example.},
  file = {/Users/scannea1/Zotero/storage/URCCH3KQ/Jagtap et al. - 2020 - Control Barrier Functions for Unknown Nonlinear Sy.pdf}
}

@article{jakschNearoptimal2010,
  title = {Near-Optimal {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {51},
  pages = {1563--1600},
  issn = {1533-7928},
  abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret \~O(DS{$\surd$}AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of {$\Omega$}({$\surd$}DSAT) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. Finally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of \~O(l1/3T2/3DS{$\surd$}A).},
  file = {/Users/scannea1/Zotero/storage/4Z9LMIWU/Jaksch et al. - 2010 - Near-optimal Regret Bounds for Reinforcement Learn.pdf}
}

@inproceedings{jannerPlanning2022,
  title = {Planning with {{Diffusion}} for {{Flexible Behavior Synthesis}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua and Levine, Sergey},
  year = {2022},
  month = jun,
  pages = {9902--9915},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Model-based reinforcement learning methods often use learning only for the purpose of recovering an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/EMR4UP3J/Janner et al. - 2022 - Planning with Diffusion for Flexible Behavior Synt.pdf}
}

@inproceedings{jannerWhen2019,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  year = {2019},
  volume = {32},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/CIG5RUDI/Janner et al. - 2019 - When to Trust Your Model Model-Based Policy Optim.pdf;/Users/scannea1/Zotero/storage/Q5UZUMN8/mbpo_2019_supp.pdf;/Users/scannea1/Zotero/storage/RUBIH4IV/5faf461eff3099671ad63c6f3f094f7f-Abstract.html}
}

@misc{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@article{julierUnscented2004,
  title = {Unscented Filtering and Nonlinear Estimation},
  author = {Julier, S.J. and Uhlmann, J.K.},
  year = {2004},
  month = mar,
  journal = {Proceedings of the IEEE},
  volume = {92},
  number = {3},
  pages = {401--422},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2003.823141},
  abstract = {The extended Kalman filter (EKF) is probably the most widely used estimation algorithm for nonlinear systems. However, more than 35 years of experience in the estimation community has shown that is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the updates. Many of these difficulties arise from its use of linearization. To overcome this limitation, the unscented transformation (UT) was developed as a method to propagate mean and covariance information through nonlinear transformations. It is more accurate, easier to implement, and uses the same order of calculations as linearization. This paper reviews the motivation, development, use, and implications of the UT.},
  keywords = {Chemical processes,Control systems,Filtering,Kalman filters,Navigation,Nonlinear control systems,Nonlinear systems,Particle tracking,Target tracking,Vehicles},
  file = {/Users/scannea1/Zotero/storage/6TYL885C/Julier and Uhlmann - 2004 - Unscented filtering and nonlinear estimation.pdf;/Users/scannea1/Zotero/storage/IGQM2C8D/1271397.html}
}

@inproceedings{kaiserBayesian2018,
  title = {Bayesian {{Alignments}} of {{Warped Multi-Output Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  year = {2018},
  volume = {31},
  pages = {6995--7004},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JMCYFREV/Kaiser et al. - 2018 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;/Users/scannea1/Zotero/storage/RM276IC3/2974788b53f73e7950e8aa49f3a306db-Abstract.html}
}

@article{kaiserBayesian2020,
  title = {Bayesian Decomposition of Multi-Modal Dynamical Systems for Reinforcement Learning},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas A. and Ek, Carl Henrik},
  year = {2020},
  month = nov,
  journal = {Neurocomputing},
  volume = {416},
  pages = {352--359},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.12.132},
  abstract = {In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner. The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task. The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty. To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise. Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.},
  langid = {english},
  keywords = {Bayesian machine learning,Data-efficiency,Gaussian processes,Hierarchical gaussian processes,Model-based reinforcement learning,Reinforcement learning,Stochastic policy search},
  file = {/Users/scannea1/Zotero/storage/HR34GBG6/Kaiser et al. - 2020 - Bayesian decomposition of multi-modal dynamical sy.pdf;/Users/scannea1/Zotero/storage/EJE7Q7YF/S0925231220305026.html}
}

@inproceedings{kaiserData2019,
  title = {Data {{Association}} with {{Gaussian Processes}}},
  booktitle = {Joint {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  year = {2019},
  eprint = {1810.07158},
  eprinttype = {arxiv},
  abstract = {The data association problem is concerned with separating data coming from different generating processes, for example when data come from different data sources, contain significant noise, or exhibit multimodality. We present a fully Bayesian approach to this problem. Our model is capable of simultaneously solving the data association problem and the induced supervised learning problems. Underpinning our approach is the use of Gaussian process priors to encode the structure of both the data and the data associations. We present an efficient learning scheme based on doubly stochastic variational inference and discuss how it can be applied to deep Gaussian process priors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/2QN8CB7D/Kaiser et al. - 2019 - Data Association with Gaussian Processes.pdf;/Users/scannea1/Zotero/storage/PPAGW99R/1810.html}
}

@inproceedings{kaiserModelBasedReinforcement2020,
  title = {Model {{Based Reinforcement Learning}} for {{Atari}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kaiser, {\L}ukasz and Babaeizadeh, Mohammad and Mi{\l}os, Piotr and Osi{\'n}ski, B{\l}a{\.z}ej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2020},
  month = mar,
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kaiser et al-2020 Model Based Reinforcement Learning for Atari/Kaiser et al_2020_Model Based Reinforcement Learning for Atari.pdf;/Users/scannea1/Zotero/storage/YIHPAPDY/forum.html}
}

@article{kalmanNew1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R. E.},
  year = {1960},
  month = mar,
  journal = {Journal of Basic Engineering},
  volume = {82},
  number = {1},
  pages = {35--45},
  issn = {0021-9223},
  doi = {10.1115/1.3662552},
  abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the ``state-transition'' method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
  file = {/Users/scannea1/Zotero/storage/S93L7H82/A-New-Approach-to-Linear-Filtering-and-Prediction.html}
}

@inproceedings{kamtheDataEfficient2018,
  title = {Data-{{Efficient Reinforcement Learning}} with {{Probabilistic Model Predictive Control}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kamthe, Sanket and Deisenroth, Marc},
  year = {2018},
  month = mar,
  pages = {1701--1710},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Trial-and-error based reinforcement learning (RL) has seen rapid advancements in recent times, especially with the advent of deep neural networks. However, the majority of autonomous RL algorithms...},
  langid = {english},
  keywords = {model-based-rl,mpc},
  file = {/Users/scannea1/Zotero/storage/E96H6LF8/Kamthe and Deisenroth - 2018 - Data-Efficient Reinforcement Learning with Probabi.pdf;/Users/scannea1/Zotero/storage/UGM5DYC9/kamthe18a.html}
}

@inproceedings{kappenIntroduction2007,
  title = {An Introduction to Stochastic Control Theory, Path Integrals and Reinforcement Learning},
  booktitle = {{{AIP Conference Proceedings}}},
  author = {Kappen, Hilbert J},
  year = {2007},
  pages = {34},
  abstract = {Control theory is a mathematical description of how to act optimally to gain future rewards. In this paper I give an introduction to deterministic and stochastic control theory and I give an overview of the possible application of control theory to the modeling of animal behavior and learning. I discuss a class of non-linear stochastic control problems that can be efficiently solved using a path integral or by MC sampling. In this control formalism the central concept of cost-to-go becomes a free energy and methods and concepts from statistical physics can be readily applied.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/T5W7HD97/Kappen - An introduction to stochastic control theory, path.pdf}
}

@inproceedings{kappenOptimal2013,
  ids = {kappenOptimal2013a},
  title = {Optimal Control as a Graphical Model Inference Problem},
  booktitle = {Proceedings of the {{Twenty-Third International Conference}} on {{International Conference}} on {{Automated Planning}} and {{Scheduling}}},
  author = {Kappen, Hilbert J. and G{\'o}mez, Vicen{\c c} and Opper, Manfred},
  year = {2013},
  month = jun,
  series = {{{ICAPS}}'13},
  pages = {472--473},
  publisher = {{AAAI Press}},
  address = {{Rome, Italy}},
  abstract = {In this paper we show the identification between stochastic optimal control computation and probabilistic inference on a graphical model for certain class of control problems. We refer to these problems as Kullback-Leibler (KL) control problems. We illustrate how KL control can be used to model a multi-agent cooperative game for which optimal control can be approximated using belief propagation when exact inference is unfeasible.},
  file = {/Users/scannea1/Zotero/storage/C4NPGFTC/Kappen et al. - 2012 - Optimal control as a graphical model inference pro.pdf}
}

@misc{kattBayesian2018,
  title = {Bayesian {{Reinforcement Learning}} in {{Factored POMDPs}}},
  author = {Katt, Sammie and Oliehoek, Frans and Amato, Christopher},
  year = {2018},
  month = nov,
  number = {arXiv:1811.05612},
  eprint = {1811.05612},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Bayesian approaches provide a principled solution to the exploration-exploitation trade-off in Reinforcement Learning. Typical approaches, however, either assume a fully observable environment or scale poorly. This work introduces the Factored Bayes-Adaptive POMDP model, a framework that is able to exploit the underlying structure while learning the dynamics in partially observable systems. We also present a belief tracking method to approximate the joint posterior over state and model variables, and an adaptation of the Monte-Carlo Tree Search solution method, which together are capable of solving the underlying problem near-optimally. Our method is able to learn efficiently given a known factorization or also learn the factorization and the model parameters at the same time. We demonstrate that this approach is able to outperform current methods and tackle problems that were previously infeasible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/scannea1/Zotero/storage/9X454S6J/Katt et al. - 2018 - Bayesian Reinforcement Learning in Factored POMDPs.pdf;/Users/scannea1/Zotero/storage/D6R7SLPX/1811.html}
}

@article{kellyIntroduction2017,
  title = {An {{Introduction}} to {{Trajectory Optimization}}: {{How}} to {{Do Your Own Direct Collocation}}},
  shorttitle = {An {{Introduction}} to {{Trajectory Optimization}}},
  author = {Kelly, Matthew},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {4},
  pages = {849--904},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/16M1062569},
  abstract = {This paper is an introductory tutorial for numerical trajectory optimization with a focus on direct collocation methods. These methods are relatively simple to understand and effectively solve a wide variety of trajectory optimization problems. Throughout the paper we illustrate each new set of concepts by working through a sequence of four example problems. We start by using trapezoidal collocation to solve a simple one-dimensional toy problem and work up to using Hermite\textendash Simpson collocation to compute the optimal gait for a bipedal walking robot. Along the way, we cover basic debugging strategies and guidelines for posing well-behaved optimization problems. The paper concludes with a short overview of other methods for trajectory optimization. We also provide an electronic supplement that contains well-documented MATLAB code for all examples and methods presented. Our primary goal is to provide the reader with the resources necessary to understand and successfully implement their own direct collocation methods.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/ERJYVAY8/Kelly - 2017 - An Introduction to Trajectory Optimization How to.pdf}
}

@inproceedings{khanApproximate2019,
  title = {Approximate {{Inference Turns Deep Networks}} into {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Khan, Mohammad Emtiyaz E and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. This enables us to relate solutions and iterations of a deep-learning algorithm to GP inference. As a result, we can obtain a GP kernel and a nonlinear feature map while training a DNN. Surprisingly, the resulting kernel is the neural tangent kernel. We show kernels obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.},
  file = {/Users/scannea1/Zotero/storage/ESU7S5H7/Khan et al. - 2019 - Approximate Inference Turns Deep Networks into Gau.pdf}
}

@misc{khanBayesian2022,
  title = {The {{Bayesian Learning Rule}}},
  author = {Khan, Mohammad Emtiyaz and Rue, H{\aa}vard},
  year = {2022},
  month = mar,
  number = {arXiv:2107.04562},
  eprint = {2107.04562},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.04562},
  abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/4CNHZC3R/Khan and Rue - 2022 - The Bayesian Learning Rule.pdf;/Users/scannea1/Zotero/storage/A9MGRBDG/2107.html}
}

@inproceedings{khanKnowledgeAdaptation2021,
  title = {Knowledge-{{Adaptation Priors}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Khan, Mohammad Emtiyaz E and Swaroop, Siddharth},
  year = {2021},
  volume = {34},
  pages = {19757--19770},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Humans and animals have a natural ability to quickly adapt to their surroundings, but machine-learning models, when subjected to changes, often require a complete retraining from scratch. We present Knowledge-adaptation priors (K-priors) to reduce the cost of retraining by enabling quick and accurate adaptation for a wide-variety of tasks and models. This is made possible by a combination of weight and function-space priors to reconstruct the gradients of the past, which recovers and generalizes many existing, but seemingly-unrelated, adaptation strategies. Training with simple first-order gradient methods can often recover the exact retrained model to an arbitrary accuracy by choosing a sufficiently large memory of the past data. Empirical results show that adaptation with K-priors achieves performance similar to full retraining, but only requires training on a handful of past examples.},
  file = {/Users/scannea1/Zotero/storage/KSM9YXVA/Khan and Swaroop - 2021 - Knowledge-Adaptation Priors.pdf}
}

@inproceedings{khojastehProbabilistic2020,
  title = {Probabilistic {{Safety Constraints}} for {{Learned High Relative Degree System Dynamics}}},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {Khojasteh, Mohammad Javad and Dhiman, Vikas and Franceschetti, Massimo and Atanasov, Nikolay},
  year = {2020},
  month = jul,
  pages = {781--792},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {This paper focuses on learning a model of system dynamics online while satisfying safety constraints. Our motivation is to avoid offline system identification or hand-specified dynamics models and ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/8RWK9LT6/Khojasteh et al. - 2020 - Probabilistic Safety Constraints for Learned High .pdf;/Users/scannea1/Zotero/storage/YA9Z3G8C/khojasteh20a.html}
}

@inproceedings{kidambiMOReLModelBasedOffline2020,
  title = {{{MOReL}}: {{Model-Based Offline Reinforcement Learning}}},
  shorttitle = {{{MOReL}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  year = {2020},
  volume = {33},
  pages = {21810--21823},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. This serves as an extreme test for an agent's ability to effectively use historical data which is known to be critical for efficient RL. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP using the offline dataset; (b) learning a near-optimal policy in this pessimistic MDP. The design of the pessimistic MDP is such that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the pessimistic MDP. This enables the pessimistic MDP to serve as a good surrogate for purposes of policy evaluation and learning. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Empirically, MOReL matches or exceeds state-of-the-art results on widely used offline RL benchmarks. Overall, the modular design of MOReL enables translating advances in its components (for e.g., in model learning, planning etc.) to improvements in offline RL.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kidambi et al-2020 MOReL/Kidambi et al_2020_MOReL.pdf}
}

@misc{kidgerNeural2022,
  title = {On {{Neural Differential Equations}}},
  author = {Kidger, Patrick},
  year = {2022},
  month = feb,
  number = {arXiv:2202.02435},
  eprint = {2202.02435},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.02435},
  abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/W446TT3L/Kidger - 2022 - On Neural Differential Equations.pdf;/Users/scannea1/Zotero/storage/PMTBJNK3/2202.html}
}

@article{kimAnalyzing2005,
  title = {Analyzing {{Nonstationary Spatial Data Using Piecewise Gaussian Processes}}},
  author = {Kim, Hyoung-Moon and Mallick, Bani K and Holmes, C. C},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {653--668},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214504000002014},
  abstract = {In many problems in geostatistics the response variable of interest is strongly related to the underlying geology of the spatial location. In these situations there is often little correlation in the responses found in different rock strata, so the underlying covariance structure shows sharp changes at the boundaries of the rock types. Conventional stationary and nonstationary spatial methods are inappropriate, because they typically assume that the covariance between points is a smooth function of distance. In this article we propose a generic method for the analysis of spatial data with sharp changes in the underlying covariance structure. Our method works by automatically decomposing the spatial domain into disjoint regions within which the process is assumed to be stationary, but the data are assumed independent across regions. Uncertainty in the number of disjoint regions, their shapes, and the model within regions is dealt with in a fully Bayesian fashion. We illustrate our approach on a previously unpublished dataset relating to soil permeability of the Schneider Buda oil field in Wood County, Texas.},
  keywords = {Bayes factor,Kriging,Model averaging,Reversible-jump Markov chain Monte Carlo,Voronoi tessellation},
  annotation = {\_eprint: https://doi.org/10.1198/016214504000002014},
  file = {/Users/scannea1/Zotero/storage/YPI64BPC/016214504000002014.html}
}

@article{kingmaAdam2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/BA9JP6IL/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/scannea1/Zotero/storage/R2QSRLK6/1412.html}
}

@article{kingmaAutoEncoding2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, M.},
  year = {2014},
  journal = {ICLR},
  abstract = {A stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case is introduced. Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  file = {/Users/scannea1/Zotero/storage/STWNYJNX/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf}
}

@book{kirkOptimal2004,
  title = {Optimal Control Theory: An Introduction},
  author = {Kirk, Donald},
  year = {2004},
  publisher = {{Courier Corporation}}
}

@inproceedings{kirschnerAdaptiveSafeBayesian2019,
  title = {Adaptive and {{Safe Bayesian Optimization}} in {{High Dimensions}} via {{One-Dimensional Subspaces}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and Ischebeck, Rasmus and Krause, Andreas},
  year = {2019},
  month = may,
  pages = {3429--3438},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kirschner et al-2019 Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional/Kirschner et al_2019_Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional.pdf;/Users/scannea1/Zotero/storage/EICNB4Y8/Kirschner et al. - 2019 - Adaptive and Safe Bayesian Optimization in High Di.pdf}
}

@article{klenskeDual2016,
  title = {Dual {{Control}} for {{Approximate Bayesian Reinforcement Learning}}},
  author = {Klenske, Edgar D. and Hennig, Philipp},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {127},
  pages = {1--30},
  issn = {1533-7928},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/CG3X7PCY/Klenske and Hennig - 2016 - Dual Control for Approximate Bayesian Reinforcemen.pdf;/Users/scannea1/Zotero/storage/6CPR4MF6/15-162.html}
}

@article{koExact1995,
  title = {An {{Exact Algorithm}} for {{Maximum Entropy Sampling}}},
  author = {Ko, Chun-Wa and Lee, Jon and Queyranne, Maurice},
  year = {1995},
  month = aug,
  journal = {Operations Research},
  volume = {43},
  number = {4},
  pages = {684--691},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.43.4.684},
  abstract = {We study the experimental design problem of selecting a most informative subset, having prespecified size, from a set of correlated random variables. The problem arises in many applied domains, such as meteorology, environmental statistics, and statistical geology. In these applications, observations can be collected at different locations, and possibly, at different times. Information is measured by ``entropy.'' In the Gaussian case, the problem is recast as that of maximizing the determinant of the covariance matrix of the chosen subset. We demonstrate that this problem is NP-hard. We establish an upper bound for the entropy, based on the eigenvalue interlacing property, and we incorporate this bound in a branch-and-bound algorithm for the exact solution of the problem. We present computational results for estimated covariance matrices that correspond to sets of environmental monitoring stations in the United States.},
  keywords = {facilities/equipment planning,location of environmental monitoring stations,maximum entropy sampling,nonlinear combinatorial optimization,programming,statistics}
}

@article{kofmanProbabilistic2012,
  title = {Probabilistic Set Invariance and Ultimate Boundedness},
  author = {Kofman, Ernesto and De Don{\'a}, Jos{\'e} A. and Seron, Maria M.},
  year = {2012},
  month = oct,
  journal = {Automatica},
  volume = {48},
  number = {10},
  pages = {2670--2676},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2012.06.074},
  abstract = {The notions of invariant sets and ultimate bounds are important concepts in the analysis of dynamical systems and very useful tools for the design of control systems. Several approaches have been reported for the characterisation of these sets, including constructive methods for their computation and procedures to obtain different approximations. However, there are shortcomings in those concepts, in the sense that no general probability distributions can be considered for the disturbances affecting the system (which, for example, precludes the assumption of Gaussian distributions insofar as they are not bounded). Motivated by those shortcomings, we propose in this paper the novel concepts of probabilistic ultimate bounds and probabilistic invariant sets, which extend the notions of invariant sets and ultimate bounds to consider `containment in probability', and have the important feature of allowing stochastic noises with more general distributions, including the ubiquitous Gaussian distribution, to be considered. We introduce some key definitions for these sets, establish their main properties and develop methods for their computation. A numerical example illustrates the main ideas.},
  langid = {english},
  keywords = {Invariant sets,Linear systems,Probabilistic methods,Ultimate bounds},
  file = {/Users/scannea1/Zotero/storage/6UFJUVQ6/Kofman et al. - 2012 - Probabilistic set invariance and ultimate boundedn.pdf;/Users/scannea1/Zotero/storage/BYUCB2YS/S0005109812003408.html}
}

@inproceedings{kollerLearningBased2018,
  title = {Learning-{{Based Model Predictive Control}} for {{Safe Exploration}}},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Koller, T. and Berkenkamp, F. and Turchetta, M. and Krause, A.},
  year = {2018},
  month = dec,
  pages = {6059--6066},
  issn = {2576-2370},
  doi = {10.1109/CDC.2018.8619572},
  abstract = {Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.},
  keywords = {complex control tasks,Data models,dynamic systems,Ellipsoids,Gaussian process,Gaussian processes,high-probability safety guarantees,Kernel,learning (artificial intelligence),learning-based model predictive control scheme,model uncertainties,mpc,predicted trajectories,predictive control,Predictive models,probability,provably accurate confidence intervals,safe-exploration,Safety,safety constraints,safety-critical,Trajectory,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/XQVMNHZC/Koller et al. - 2018 - Learning-Based Model Predictive Control for Safe E.pdf;/Users/scannea1/Zotero/storage/9DEXJKB7/8619572.html}
}

@article{krauseNearOptimal2008,
  title = {Near-{{Optimal Sensor Placements}} in {{Gaussian Processes}}: {{Theory}}, {{Efficient Algorithms}} and {{Empirical Studies}}},
  shorttitle = {Near-{{Optimal Sensor Placements}} in {{Gaussian Processes}}},
  author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {8},
  pages = {235--284},
  abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
  file = {/Users/scannea1/Zotero/storage/LRZX27GX/Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf}
}

@inproceedings{krauseNonmyopic2007,
  title = {Nonmyopic Active Learning of {{Gaussian}} Processes: An Exploration-Exploitation Approach},
  shorttitle = {Nonmyopic Active Learning of {{Gaussian}} Processes},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning},
  author = {Krause, Andreas and Guestrin, Carlos},
  year = {2007},
  month = jun,
  series = {{{ICML}} '07},
  pages = {449--456},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1273496.1273553},
  abstract = {When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations are selected based on previous measurements, will perform significantly better than sensing at an a priori specified set of locations. For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an analysis and efficient algorithms that address this question. Central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies. We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading off exploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e., near-optimally selecting observations when the parameters are (approximately) known. We discuss several exploration strategies, and present logarithmic sample complexity bounds for the exploration phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the model. We also present extensive empirical evaluation on several real-world problems.},
  isbn = {978-1-59593-793-3},
  file = {/Users/scannea1/Zotero/storage/74BTCB6D/Krause and Guestrin - 2007 - Nonmyopic active learning of Gaussian processes a.pdf}
}

@inproceedings{krizhevskyImageNet2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\textbackslash\% and 18.9\textbackslash\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Users/scannea1/Zotero/storage/RHQ56BCC/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@article{kurutachModelEnsemble2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, I. and Duan, Yan and Tamar, Aviv and Abbeel, P.},
  year = {2018},
  journal = {ICLR},
  abstract = {This paper analyzes the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and shows that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  file = {/Users/scannea1/Zotero/storage/BGUTUYR2/Kurutach et al. - 2018 - Model-Ensemble Trust-Region Policy Optimization.pdf}
}

@phdthesis{kussGaussian2006,
  title = {Gaussian {{Process Models}} for {{Robust Regression}}, {{Classification}}, and {{Reinforcement Learning}}},
  author = {Kuss, Malte},
  year = {2006},
  abstract = {Our goal is to understand the principles of Perception, Action and Learning in autonomous systems that successfully interact with complex environments and to use this understanding to design future systems.},
  langid = {english},
  school = {Technische Universit\"at Darmstadt, Darmstadt, Germany},
  file = {/Users/scannea1/Zotero/storage/PA5ZCPBF/Kuss - 2006 - Gaussian Process Models for Robust Regression, Cla.pdf;/Users/scannea1/Zotero/storage/CE36ZCDW/4050.html}
}

@article{lambertLowLevel2019,
  title = {Low-{{Level Control}} of a {{Quadrotor With Deep Model-Based Reinforcement Learning}}},
  author = {Lambert, Nathan O. and Drew, Daniel S. and Yaconelli, Joseph and Levine, Sergey and Calandra, Roberto and Pister, Kristofer S. J.},
  year = {2019},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {4},
  pages = {4224--4230},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2930489},
  abstract = {Designing effective low-level robot controllers often entail platform-specific implementations that require manual heuristic parameter tuning, significant system knowledge, or long design times. With the rising number of robotic and mechatronic systems deployed across areas ranging from industrial automation to intelligent toys, the need for a general approach to generating low-level controllers is increasing. To address the challenge of rapidly generating low-level controllers, we argue for using model-based reinforcement learning (MBRL) trained on relatively small amounts of automatically generated (i.e., without system simulation) data. In this letter, we explore the capabilities of MBRL on a Crazyflie centimeter-scale quadrotor with rapid dynamics to predict and control at {$\leq$}50 Hz. To our knowledge, this is the first use of MBRL for controlled hover of a quadrotor using only on-board sensors, direct motor input signals, and no initial dynamics knowledge. Our controller leverages rapid simulation of a neural network forward dynamics model on a graphic processing unit enabled base station, which then transmits the best current action to the quadrotor firmware via radio. In our experiments, the quadrotor achieved hovering capability of up to 6 s with 3 min of experimental training data.},
  keywords = {aerial systems: mechanics and control,Attitude control,Data models,Deep learning in robotics and automation,Predictive models,Pulse width modulation,Robots,Trajectory,Vehicle dynamics},
  file = {/Users/scannea1/Zotero/storage/NU9R9BNE/Lambert et al. - 2019 - Low-Level Control of a Quadrotor With Deep Model-B.pdf;/Users/scannea1/Zotero/storage/DMLF4SDJ/8769882.html}
}

@misc{lambertObjectiveMismatchModelbased2021,
  title = {Objective {{Mismatch}} in {{Model-based Reinforcement Learning}}},
  author = {Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
  year = {2021},
  month = apr,
  number = {arXiv:2002.04523},
  eprint = {2002.04523},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.04523},
  abstract = {Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t.\textasciitilde the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lambert et al-2021 Objective Mismatch in Model-based Reinforcement Learning/Lambert et al_2021_Objective Mismatch in Model-based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/TTWIQRAE/2002.html}
}

@phdthesis{lambertSynergy2022,
  title = {Synergy of {{Prediction}} and {{Control}} in {{Model-based Reinforcement Learning}}},
  author = {Lambert, Nathan},
  year = {2022},
  month = may,
  file = {/Users/scannea1/Zotero/storage/ZE9E4ZKP/EECS-2022-65.pdf;/Users/scannea1/Zotero/storage/UWRG3272/EECS-2022-65.html}
}

@article{lecunGradientbased1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/scannea1/Zotero/storage/6GIQW4J5/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/scannea1/Zotero/storage/LMPT8P2U/726791.html}
}

@article{leeGPILQG2017,
  title = {{{GP-ILQG}}: {{Data-driven Robust Optimal Control}} for {{Uncertain Nonlinear Dynamical Systems}}},
  shorttitle = {{{GP-ILQG}}},
  author = {Lee, Gilwoo and Srinivasa, Siddhartha S. and Mason, Matthew T.},
  year = {2017},
  month = may,
  journal = {arXiv:1705.05344 [cs]},
  eprint = {1705.05344},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {As we aim to control complex systems, use of a simulator in model-based reinforcement learning is becoming more common. However, it has been challenging to overcome the Reality Gap, which comes from nonlinear model bias and susceptibility to disturbance. To address these problems, we propose a novel algorithm that combines data-driven system identification approach (Gaussian Process) with a Differential-Dynamic-Programming-based robust optimal control method (Iterative Linear Quadratic Control). Our algorithm uses the simulator's model as the mean function for a Gaussian Process and learns only the difference between the simulator's prediction and actual observations, making it a natural hybrid of simulation and real-world observation. We show that our approach quickly corrects incorrect models, comes up with robust optimal controllers, and transfers its acquired model knowledge to new tasks efficiently.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/KEQGYL7T/Lee et al. - 2017 - GP-ILQG Data-driven Robust Optimal Control for Un.pdf;/Users/scannea1/Zotero/storage/2NUXIVH4/1705.html}
}

@inproceedings{leeStochasticLatentActorCritic2020,
  title = {Stochastic {{Latent Actor-Critic}}: {{Deep Reinforcement Learning}} with a {{Latent Variable Model}}},
  shorttitle = {Stochastic {{Latent Actor-Critic}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Alex X. and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  year = {2020},
  volume = {33},
  pages = {741--752},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of  challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lee et al-2020 Stochastic Latent Actor-Critic/Lee et al_2020_Stochastic Latent Actor-Critic.pdf}
}

@article{leibfriedTutorial2021,
  title = {A {{Tutorial}} on {{Sparse Gaussian Processes}} and {{Variational Inference}}},
  author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
  year = {2021},
  month = feb,
  journal = {arXiv:2012.13962 [cs, stat]},
  eprint = {2012.13962},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as inderdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/P96DJSJL/Leibfried et al. - 2021 - A Tutorial on Sparse Gaussian Processes and Variat.pdf;/Users/scannea1/Zotero/storage/Z55ZREHB/2012.html}
}

@misc{leibfriedVariationalInferenceModelFree2022,
  title = {Variational {{Inference}} for {{Model-Free}} and {{Model-Based Reinforcement Learning}}},
  author = {Leibfried, Felix},
  year = {2022},
  month = sep,
  number = {arXiv:2209.01693},
  eprint = {2209.01693},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.01693},
  abstract = {Variational inference (VI) is a specific type of approximate Bayesian inference that approximates an intractable posterior distribution with a tractable one. VI casts the inference problem as an optimization problem, more specifically, the goal is to maximize a lower bound of the logarithm of the marginal likelihood with respect to the parameters of the approximate posterior. Reinforcement learning (RL) on the other hand deals with autonomous agents and how to make them act optimally such as to maximize some notion of expected future cumulative reward. In the non-sequential setting where agents' actions do not have an impact on future states of the environment, RL is covered by contextual bandits and Bayesian optimization. In a proper sequential scenario, however, where agents' actions affect future states, instantaneous rewards need to be carefully traded off against potential long-term rewards. This manuscript shows how the apparently different subjects of VI and RL are linked in two fundamental ways. First, the optimization objective of RL to maximize future cumulative rewards can be recovered via a VI objective under a soft policy constraint in both the non-sequential and the sequential setting. This policy constraint is not just merely artificial but has proven as a useful regularizer in many RL tasks yielding significant improvements in agent performance. And second, in model-based RL where agents aim to learn about the environment they are operating in, the model-learning part can be naturally phrased as an inference problem over the process that governs environment dynamics. We are going to distinguish between two scenarios for the latter: VI when environment states are fully observable by the agent and VI when they are only partially observable through an observation distribution.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Leibfried-2022 Variational Inference for Model-Free and Model-Based Reinforcement Learning/Leibfried_2022_Variational Inference for Model-Free and Model-Based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/DDVDJY48/2209.html}
}

@inproceedings{levineGuided2013,
  title = {Guided {{Policy Search}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  year = {2013},
  month = may,
  pages = {1--9},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and o...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3256U5QT/Levine and Koltun - 2013 - Guided Policy Search.pdf;/Users/scannea1/Zotero/storage/VKD6XSNY/levine13.html}
}

@inproceedings{levineLearning2014,
  title = {Learning {{Neural Network Policies}} with {{Guided Policy Search}} under {{Unknown Dynamics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levine, Sergey and Abbeel, Pieter},
  year = {2014},
  volume = {27},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/M7QQLZRR/Levine and Abbeel - 2014 - Learning Neural Network Policies with Guided Polic.pdf;/Users/scannea1/Zotero/storage/RUZWAZFU/6766aa2750c19aad2fa1b32f36ed4aee-Abstract.html}
}

@article{levineReinforcement2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  year = {2018},
  journal = {ArXiv},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  file = {/Users/scannea1/Zotero/storage/74K38XL7/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf}
}

@inproceedings{levineVariational2013,
  title = {Variational {{Policy Search}} via {{Trajectory Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  year = {2013},
  volume = {26},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/LHGXIJIL/Levine and Koltun - 2013 - Variational Policy Search via Trajectory Optimizat.pdf;/Users/scannea1/Zotero/storage/RSGHGXI9/38af86134b65d0f10fe33d30dd76442e-Abstract.html}
}

@techreport{liaoAdvantages1992,
  title = {Advantages of {{Differential Dynamic Programming Over Newton}}''s {{Method}} for {{Discrete-time Optimal Control Problems}}},
  author = {Liao, L. and Shoemaker, C.},
  year = {1992},
  institution = {{Cornell University, Ithaca, NY}},
  abstract = {Differential Dynamic Programming (DDP) and stagewise Newton\&\#39;\&\#39;s method are both quadratically convergent algorithms for solving discrete time optimal control problems. Although these two algorithms share many theoretical similarities, they demonstrate significantly different numerical performance. In this paper, we will compare and analyze these two algorithms in detail and derive another quadratically convergent algorithm which is a combination of the DDP algorithm and Newton\&\#39;\&\#39;s method. This new second-order algorithm plays a key role in the explanation of the numerical differences between the DDP algorithm and Newton\&\#39;\&\#39;s method. The detailed algorithmic and structural differences for these three algorithms and their impact on numerical performance will be discussed and explored. Two test problems with various dimensions solved by these three algorithms will be presented. One nonlinear test problem demonstrates that the DDP algorithm can be as much as 28 times faster than the stagewise Newton\&\#39;\&\#39;s method. The numerical comparsion indicates that the DDP algorithm is numerically superior to the stagewise Newton\&\#39;\&\#39;s method.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/9GC5A94M/787b49f4ad64f175d6afa47fd16068d2281be2c5.html}
}

@inproceedings{lillicrapContinuousControlDeep2016,
  title = {Continuous Control with Deep Reinforcement Learning},
  booktitle = {4th International Conference on Learning Representations, {{ICLR}} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/LillicrapHPHETS15.bib},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lillicrap et al-2019 Continuous control with deep reinforcement learning/Lillicrap et al_2019_Continuous control with deep reinforcement learning2.pdf;/Users/scannea1/Zotero/storage/HWS4YW6U/1509.html}
}

@inproceedings{lindingerMeanField2020,
  title = {Beyond the {{Mean-Field}}: {{Structured Deep Gaussian Processes Improve}} the {{Predictive Uncertainties}}},
  shorttitle = {Beyond the {{Mean-Field}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Lindinger, J. and Reeb, D. and Lippert, C. and Rakitsch, Barbara},
  year = {2020},
  volume = {34},
  abstract = {Deep Gaussian Processes learn probabilistic data representations for supervised learning by cascading multiple Gaussian Processes. While this model family promises flexible predictive distributions, exact inference is not tractable. Approximate inference techniques trade off the ability to closely resemble the posterior distribution against speed of convergence and computational efficiency. We propose a novel Gaussian variational family that allows for retaining covariances between latent processes while achieving fast convergence by marginalising out all global latent variables. After providing a proof of how this marginalisation can be done for general covariances, we restrict them to the ones we empirically found to be most important in order to also achieve computational efficiency. We provide an efficient implementation of our new approach and apply it to several regression benchmark datasets. We find that it yields more accurate predictive distributions, in particular for test data points that are distant from the training set.},
  file = {/Users/scannea1/Zotero/storage/5FZWEGC3/Lindinger et al. - 2020 - Beyond the Mean-Field Structured Deep Gaussian Pr.pdf}
}

@article{liuConstrained2021,
  title = {Constrained {{Model-based Reinforcement Learning}} with {{Robust Cross-Entropy Method}}},
  author = {Liu, Zuxin and Zhou, Hongyi and Chen, Baiming and Zhong, Sicheng and Hebert, Martial and Zhao, Ding},
  year = {2021},
  month = mar,
  journal = {arXiv:2010.07968 [cs]},
  eprint = {2010.07968},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper studies the constrained/safe reinforcement learning (RL) problem with sparse indicator signals for constraint violations. We propose a model-based approach to enable RL agents to effectively explore the environment with unknown system dynamics and environment constraints given a significantly small number of violation budgets. We employ the neural network ensemble model to estimate the prediction uncertainty and use model predictive control as the basic control framework. We propose the robust cross-entropy method to optimize the control sequence considering the model uncertainty and constraints. We evaluate our methods in the Safety Gym environment. The results show that our approach learns to complete the tasks with a much smaller number of constraint violations than state-of-the-art baselines. Additionally, we are able to achieve several orders of magnitude better sample efficiency when compared with constrained model-free RL approaches. The code is available at \textbackslash url\{https://github.com/liuzuxin/safe-mbrl\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/DSGIGWHF/Liu et al. - 2021 - Constrained Model-based Reinforcement Learning wit.pdf;/Users/scannea1/Zotero/storage/MKP3BWF3/2010.html}
}

@inproceedings{liuSimplePrincipledUncertainty2020,
  title = {Simple and {{Principled Uncertainty Estimation}} with {{Deterministic Deep Learning}} via {{Distance Awareness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Jeremiah and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax Weiss, Tania and Lakshminarayanan, Balaji},
  year = {2020},
  volume = {33},
  pages = {7498--7512},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model.  However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer.  On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Liu et al-2020 Simple and Principled Uncertainty Estimation with Deterministic Deep Learning/Liu et al_2020_Simple and Principled Uncertainty Estimation with Deterministic Deep Learning.pdf}
}

@book{ljungSystem1999,
  title = {System {{Identification}}: {{Theory}} for the {{User}}},
  author = {Ljung, Lennart},
  year = {1999},
  series = {Prentice {{Hall Information}} and {{System Sciences Series}}},
  edition = {Second},
  publisher = {{Pearson}}
}

@article{loeligerFactor2007,
  title = {The {{Factor Graph Approach}} to {{Model-Based Signal Processing}}},
  author = {Loeliger, Hans-Andrea and Dauwels, Justin and Hu, Junli and Korl, Sascha and Ping, Li and Kschischang, Frank R.},
  year = {2007},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {6},
  pages = {1295--1322},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2007.896497},
  abstract = {The message-passing approach to model-based signal processing is developed with a focus on Gaussian message passing in linear state-space models, which includes recursive least squares, linear minimum-mean-squared-error estimation, and Kalman filtering algorithms. Tabulated message computation rules for the building blocks of linear models allow us to compose a variety of such algorithms without additional derivations or computations. Beyond the Gaussian case, it is emphasized that the message-passing approach encourages us to mix and match different algorithmic techniques, which is exemplified by two different approaches - steepest descent and expectation maximization - to message passing through a multiplier node.},
  keywords = {Algorithm design and analysis,Estimation,factor graphs,graphical models,Graphical models,Information technology,Kalman filtering,Kalman filters,Least squares approximation,Machine learning algorithms,message passing,Message passing,Signal design,signal processing,Signal processing,Signal processing algorithms},
  file = {/Users/scannea1/Zotero/storage/JBWKUUSB/Loeliger et al. - 2007 - The Factor Graph Approach to Model-Based Signal Pr.pdf;/Users/scannea1/Zotero/storage/6ARVVJ8G/4282128.html}
}

@inproceedings{lotfiBayesian2022,
  title = {Bayesian {{Model Selection}}, the {{Marginal Likelihood}}, and {{Generalization}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  year = {2022},
  month = jun,
  pages = {14223--14247},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4ZKFR66N/Lotfi et al. - 2022 - Bayesian Model Selection, the Marginal Likelihood,.pdf}
}

@article{lowreyPlan2019,
  title = {Plan {{Online}}, {{Learn Offline}}: {{Efficient Learning}} and {{Exploration}} via {{Model-Based Control}}},
  shorttitle = {Plan {{Online}}, {{Learn Offline}}},
  author = {Lowrey, Kendall and Rajeswaran, A. and Kakade, S. and Todorov, E. and Mordatch, Igor},
  year = {2019},
  journal = {ICLR},
  abstract = {A plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world and how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.},
  file = {/Users/scannea1/Zotero/storage/AW7BCGGU/Lowrey et al. - 2019 - Plan Online, Learn Offline Efficient Learning and.pdf}
}

@article{lyapunovGeneral1992a,
  title = {The General Problem of the Stability of Motion},
  author = {Lyapunov, A. M.},
  year = {1992},
  month = mar,
  journal = {International Journal of Control},
  volume = {55},
  number = {3},
  pages = {531--534},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207179208934253},
  annotation = {\_eprint: https://doi.org/10.1080/00207179208934253},
  file = {/Users/scannea1/Zotero/storage/L73PVNMP/00207179208934253.html}
}

@article{mackayBayesian1992,
  title = {Bayesian {{Interpolation}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {415--447},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.415},
  abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. ``Occam's razor'' is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
  file = {/Users/scannea1/Zotero/storage/3GJG8PUA/MacKay - 1992 - Bayesian Interpolation.pdf;/Users/scannea1/Zotero/storage/H5INT59A/Bayesian-Interpolation.html}
}

@article{mackayInformationBased1992,
  title = {Information-{{Based Objective Functions}} for {{Active Data Selection}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = jul,
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {590--604},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.4.590},
  abstract = {Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.},
  file = {/Users/scannea1/Zotero/storage/NAF2ZXPE/MacKay - 1992 - Information-Based Objective Functions for Active D.pdf;/Users/scannea1/Zotero/storage/GL9H3DEV/Information-Based-Objective-Functions-for-Active.html}
}

@article{mackayPractical1992,
  title = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {448--472},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.448},
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
  file = {/Users/scannea1/Zotero/storage/FME5SXC4/MacKay - 1992 - A Practical Bayesian Framework for Backpropagation.pdf;/Users/scannea1/Zotero/storage/2BPFDD26/A-Practical-Bayesian-Framework-for-Backpropagation.html}
}

@article{mackayProbable1995,
  title = {Probable Networks and Plausible Predictions \textemdash{} a Review of Practical {{Bayesian}} Methods for Supervised Neural Networks},
  author = {Mackay, David J. C.},
  year = {1995},
  month = jan,
  journal = {Network: Computation in Neural Systems},
  volume = {6},
  number = {3},
  pages = {469--505},
  publisher = {{Informa UK Limited}},
  issn = {0954-898X},
  doi = {10.1088/0954-898X/6/3/011},
  abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. The article describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.},
  langid = {english}
}

@inproceedings{maddoxConditioningSparseVariational2021,
  title = {Conditioning {{Sparse Variational Gaussian Processes}} for {{Online Decision-making}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maddox, Wesley J and Stanton, Samuel and Wilson, Andrew G},
  year = {2021},
  volume = {34},
  pages = {6365--6379},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Maddox et al-2021 Conditioning Sparse Variational Gaussian Processes for Online Decision-making/Maddox et al_2021_Conditioning Sparse Variational Gaussian Processes for Online Decision-making.pdf}
}

@inproceedings{marcoAutomatic2016,
  ids = {marcoAutomatic2016a},
  title = {Automatic {{LQR}} Tuning Based on {{Gaussian}} Process Global Optimization},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Marco, Alonso and Hennig, Philipp and Bohg, Jeannette and Schaal, Stefan and Trimpe, Sebastian},
  year = {2016},
  month = may,
  pages = {270--277},
  doi = {10.1109/ICRA.2016.7487144},
  abstract = {This paper proposes an automatic controller tuning framework based on linear optimal control combined with Bayesian optimization. With this framework, an initial set of controller gains is automatically improved according to a pre-defined performance objective evaluated from experimental data. The underlying Bayesian optimization algorithm is Entropy Search, which represents the latent objective as a Gaussian process and constructs an explicit belief over the location of the objective minimum. This is used to maximize the information gain from each experimental evaluation. Thus, this framework shall yield improved controllers with fewer evaluations compared to alternative approaches. A seven-degree-of-freedom robot arm balancing an inverted pole is used as the experimental demonstrator. Results of two- and four-dimensional tuning problems highlight the method's potential for automatic controller tuning on robotic platforms.},
  keywords = {Bayes methods,Computational modeling,Cost function,Gaussian processes,Robots,Tuning},
  file = {/Users/scannea1/Zotero/storage/7H6NWTCX/Marco et al. - 2016 - Automatic LQR tuning based on Gaussian process glo.pdf;/Users/scannea1/Zotero/storage/IS625MU9/Marco et al. - 2016 - Automatic LQR tuning based on Gaussian process glo.pdf;/Users/scannea1/Zotero/storage/2MZEI9XT/7487144.html;/Users/scannea1/Zotero/storage/GLH4RV2S/7487144.html}
}

@inproceedings{martensOptimizing2015,
  title = {Optimizing {{Neural Networks}} with {{Kronecker-factored Approximate Curvature}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James and Grosse, Roger},
  year = {2015},
  month = jun,
  pages = {2408--2417},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JYW3T4NX/Martens and Grosse - 2015 - Optimizing Neural Networks with Kronecker-factored.pdf}
}

@article{matthewsGPflow2017,
  title = {\{\vphantom\}{{GP}}\vphantom\{\}flow: {{A}} \{\vphantom\}{{G}}\vphantom\{\}aussian Process Library Using \{\vphantom\}{{T}}\vphantom\{\}ensor\{\vphantom\}{{F}}\vphantom\{\}low\vphantom\{\}},
  author = {Matthews, Alexander G de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke and Boukouvalas, Alexis and {\{Le\{{\textbackslash}'o\}n-Villagr\{{\textbackslash}'a\}\}}, Pablo and Ghahramani, Zoubin},
  year = {2017},
  month = apr,
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--6}
}

@phdthesis{mcallisterBayesian2017,
  type = {Thesis},
  title = {Bayesian {{Learning}} for {{Data-Efficient Control}}},
  author = {McAllister, Rowan},
  year = {2017},
  month = apr,
  doi = {10.17863/CAM.16688},
  abstract = {Applications to learn control of unfamiliar dynamical systems with increasing autonomy are ubiquitous. From robotics, to finance, to industrial processing, autonomous learning helps obviate a heavy reliance on experts for system identification and controller design. Often real world systems are nonlinear, stochastic, and expensive to operate (e.g. slow, energy intensive, prone to wear and tear). Ideally therefore, nonlinear systems can be identified with minimal system interaction. This thesis considers data efficient autonomous learning of control of nonlinear, stochastic systems. Data efficient learning critically requires probabilistic modelling of dynamics. Traditional control approaches use deterministic models, which easily overfit data, especially small datasets. We use probabilistic Bayesian modelling to learn systems from scratch, similar to the PILCO algorithm, which achieved unprecedented data efficiency in learning control of several benchmarks. We extend PILCO in three principle ways. First, we learn control under significant observation noise by simulating a filtered control process using a tractably analytic framework of Gaussian distributions. In addition, we develop the `latent variable belief Markov decision process' when filters must predict under real-time constraints. Second, we improve PILCO's data efficiency by directing exploration with predictive loss uncertainty and Bayesian optimisation, including a novel approximation to the Gittins index. Third, we take a step towards data efficient learning of high-dimensional control using Bayesian neural networks (BNN). Experimentally we show although filtering mitigates adverse effects of observation noise, much greater performance is achieved when optimising controllers with evaluations faithful to reality: by simulating closed-loop filtered control if executing closed-loop filtered control. Thus, controllers are optimised w.r.t. how they are used, outperforming filters applied to systems optimised by unfiltered simulations. We show directed exploration improves data efficiency. Lastly, we show BNN dynamics models are almost as data efficient as Gaussian process models. Results show data efficient learning of high-dimensional control is possible as BNNs scale to high-dimensional state inputs.},
  copyright = {All Rights Reserved},
  langid = {english},
  school = {Department of Engineering, University of Cambridge},
  annotation = {Accepted: 2017-11-28T16:41:59Z},
  file = {/Users/scannea1/Zotero/storage/JPLABBYW/McAllister - 2017 - Bayesian Learning for Data-Efficient Control.pdf;/Users/scannea1/Zotero/storage/DM29JAFZ/269779.html}
}

@inproceedings{mckinnonLearning2017,
  ids = {mckinnonLearning2017b},
  title = {Learning Multimodal Models for Robot Dynamics Online with a Mixture of {{Gaussian}} Process Experts},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {McKinnon, C. D. and Schoellig, A. P.},
  year = {2017},
  month = may,
  pages = {322--328},
  publisher = {{IEEE}},
  doi = {10.1109/ICRA.2017.7989041},
  abstract = {For decades, robots have been essential allies alongside humans in controlled industrial environments like heavy manufacturing facilities. However, without the guidance of a trusted human operator to shepherd a robot safely through a wide range of conditions, they have been barred from the complex, ever changing environments that we live in from day to day. Safe learning control has emerged as a promising way to start bridging algorithms based on first principles to complex real-world scenarios by using data to adapt, and improve performance over time. Safe learning methods rely on a good estimate of the robot dynamics and of the bounds on modelling error in order to be effective. Current methods focus on either a single adaptive model, or a fixed, known set of models for the robot dynamics. This limits them to static or slowly changing environments. This paper presents a method using Gaussian Processes in a Dirichlet Process mixture model to learn an increasing number of non-linear models for the robot dynamics. We show that this approach enables a robot to re-use past experience from an arbitrary number of previously visited operating conditions, and to automatically learn a new model when a new and distinct operating condition is encountered. This approach improves the robustness of existing Gaussian Process-based models to large changes in dynamics that do not have to be specified ahead of time.},
  keywords = {Aerodynamics,controlled industrial environment,Data models,Dirichlet process mixture model,Gaussian processes,Heuristic algorithms,human operator,learning (artificial intelligence),mixture models,mixture of Gaussian process experts,multimodal model learning,robot dynamics,Robots,Safety,System dynamics},
  file = {/Users/scannea1/Zotero/storage/6XLZFBSE/McKinnon and Schoellig - 2017 - Learning multimodal models for robot dynamics onli.pdf;/Users/scannea1/Zotero/storage/9JHYDX6T/7989041.html;/Users/scannea1/Zotero/storage/Y27Y938R/7989041.html}
}

@inproceedings{milamNew2000,
  title = {A New Computational Approach to Real-Time Trajectory Generation for Constrained Mechanical Systems},
  booktitle = {Proceedings of the 39th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Milam, M. B. and Mushambi, K. and Murray, R. M.},
  year = {2000},
  month = dec,
  volume = {1},
  pages = {845--851},
  publisher = {{IEEE}},
  issn = {0191-2216},
  doi = {10.1109/CDC.2000.912875},
  abstract = {Preliminary results of a new computational approach to generate aggressive trajectories in real-time for constrained mechanical systems are presented. The algorithm is based on a combination of the nonlinear control theory, spline theory, and sequential quadratic programming. It is demonstrated that real-time trajectory generation for constrained mechanical systems is possible by mapping the problem to one of finding trajectory curves in a lower dimensional space. Performance of the algorithm is compared with existing optimal trajectory generation techniques. Numerical results are reported using the nonlinear trajectory generation software package.},
  keywords = {Adaptive control,constrained mechanical systems,control system analysis computing,Control systems,Databases,Mechanical systems,nonlinear control,nonlinear control systems,Nonlinear control systems,optimal control,Optimal control,optimisation,quadratic programming,Real time systems,real-time systems,sequential quadratic programming,spline,splines (mathematics),Stability,Target tracking,tracking,Trajectory,trajectory generation},
  file = {/Users/scannea1/Zotero/storage/NUTQUYQH/Milam et al. - 2000 - A new computational approach to real-time trajecto.pdf;/Users/scannea1/Zotero/storage/C2U83PCJ/912875.html}
}

@inproceedings{mitrovicAdaptive2010,
  title = {Adaptive {{Optimal Feedback Control}} with {{Learned Internal Dynamics Models}}},
  booktitle = {From {{Motor Learning}} to {{Interaction Learning}} in {{Robots}}},
  author = {Mitrovic, Djordje and Klanke, Stefan and Vijayakumar, S.},
  year = {2010},
  doi = {10.1007/978-3-642-05181-4_4},
  abstract = {Optimal Feedback Control (OFC) has been proposed as an attractive movement generation strategy in goal reaching tasks for anthropomorphic manipulator systems. Recent developments, such as the Iterative Linear Quadratic Gaussian (ILQG) algorithm, have focused on the case of non-linear, but still analytically available, dynamics. For realistic control systems, however, the dynamics may often be unknown, difficult to estimate, or subject to frequent systematic changes. In this chapter, we combine the ILQG framework with learning the forward dynamics for simulated arms, which exhibit large redundancies, both, in kinematics and in the actuation. We demonstrate how our approach can compensate for complex dynamic perturbations in an online fashion. The specific adaptive framework introduced lends itself to a computationally more efficient implementation of the ILQG optimisation without sacrificing control accuracy \textendash{} allowing the method to scale to large DoF systems.},
  file = {/Users/scannea1/Zotero/storage/Q9BM2T8Y/Mitrovic et al. - 2010 - Adaptive Optimal Feedback Control with Learned Int.pdf}
}

@inproceedings{moerlandEfficient2017,
  title = {Efficient Exploration with {{Double Uncertain Value Networks}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Moerland, Thomas and Broekens, Joost and Jonker, Catholijn},
  year = {2017},
  abstract = {This paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action. We identify two sources of uncertainty that are relevant for exploration. The first originates from limited data (parametric uncertainty), while the second originates from the distribution of the returns (return uncertainty). We identify methods to learn these distributions with deep neural networks, where we estimate parametric uncertainty with Bayesian drop-out, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. Then, we identify that both can be jointly estimated in one network, which we call the Double Uncertain Value Network. The policy is directly derived from the learned distributions based on Thompson sampling. Experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge.},
  file = {/Users/scannea1/Zotero/storage/Q94JFI6T/Moerland et al. - 2017 - Efficient exploration with Double Uncertain Value .pdf}
}

@article{moerlandLearning2017,
  title = {Learning {{Multimodal Transition Dynamics}} for {{Model-Based Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Jonker, Catholijn M.},
  year = {2017},
  month = aug,
  journal = {arXiv:1705.00470},
  eprint = {1705.00470},
  eprinttype = {arxiv},
  abstract = {In this paper we study how to learn stochastic, multimodal transition dynamics in reinforcement learning (RL) tasks. We focus on evaluating transition function estimation, while we defer planning over this model to future work. Stochasticity is a fundamental property of many task environments. However, discriminative function approximators have difficulty estimating multimodal stochasticity. In contrast, deep generative models do capture complex high-dimensional outcome distributions. First we discuss why, amongst such models, conditional variational inference (VI) is theoretically most appealing for model-based RL. Subsequently, we compare different VI models on their ability to learn complex stochasticity on simulated functions, as well as on a typical RL gridworld with multimodal dynamics. Results show VI successfully predicts multimodal outcomes, but also robustly ignores these for deterministic parts of the transition dynamics. In summary, we show a robust method to learn multimodal transitions using function approximation, which is a key preliminary for model-based RL in stochastic domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/8BGSA3IV/Moerland et al. - 2017 - Learning Multimodal Transition Dynamics for Model-.pdf;/Users/scannea1/Zotero/storage/6PAULW4H/1705.html}
}

@misc{moldovanSafeExplorationMarkov2012,
  title = {Safe {{Exploration}} in {{Markov Decision Processes}}},
  author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4810},
  eprint = {1205.4810},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Moldovan_Abbeel-2012 Safe Exploration in Markov Decision Processes/Moldovan_Abbeel_2012_Safe Exploration in Markov Decision Processes.pdf;/Users/scannea1/Zotero/storage/BQ5H8I3Y/1205.html}
}

@article{mukadamContinuoustime2018,
  title = {Continuous-Time {{Gaussian}} Process Motion Planning via Probabilistic Inference},
  author = {Mukadam, Mustafa and Dong, Jing and Yan, Xinyan and Dellaert, Frank and Boots, Byron},
  year = {2018},
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {11},
  pages = {1319--1340},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364918790369},
  abstract = {We introduce a novel formulation of motion planning, for continuous-time trajectories, as probabilistic inference. We first show how smooth continuous-time trajectories can be represented by a small number of states using sparse Gaussian process (GP) models. We next develop an efficient gradient-based optimization algorithm that exploits this sparsity and GP interpolation. We call this algorithm the Gaussian Process Motion Planner (GPMP). We then detail how motion planning problems can be formulated as probabilistic inference on a factor graph. This forms the basis for GPMP2, a very efficient algorithm that combines GP representations of trajectories with fast, structure-exploiting inference via numerical optimization. Finally, we extend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replan when conditions change. We benchmark our algorithms against several sampling-based and trajectory optimization-based motion planning algorithms on planning problems in multiple environments. Our evaluation reveals that GPMP2 is several times faster than previous algorithms while retaining robustness. We also benchmark iGPMP2 on replanning problems, and show that it can find successful solutions in a fraction of the time required by GPMP2 to replan from scratch.},
  langid = {english},
  keywords = {factor graphs,Gaussian processes,Motion planning,probabilistic inference,trajectory optimization},
  file = {/Users/scannea1/Zotero/storage/GB3VYF5E/Mukadam et al. - 2018 - Continuous-time Gaussian process motion planning v.pdf}
}

@techreport{murrayNote2005,
  title = {A Note on the Evidence and {{Bayesian Occam}}'s Razor},
  author = {Murray, Iain and Ghahramani, Zoubin},
  year = {2005},
  abstract = {Abstract\textemdash In his thesis, MacKay (1991) introduced figure 1, explaining how Bayes rule provides an automatic ``Occam's razor '' effect, penalizing unnecessarily complex models. This figure has been adopted by several authors in the same schematic form. Here, after briefly reviewing necessary material, we compute a realization of the plot for a toy data modeling problem. We discuss interesting aspects of this plot and their implications for understanding model complexity. I.},
  file = {/Users/scannea1/Zotero/storage/M5MRVENH/Murray and Ghahramani - 2005 - A note on the evidence and Bayesian Occam’s razor.pdf;/Users/scannea1/Zotero/storage/RVURUM52/summary.html}
}

@inproceedings{nagabandiDeep2020,
  title = {Deep {{Dynamics Models}} for {{Learning Dexterous Manipulation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
  year = {2020},
  month = may,
  pages = {1101--1112},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in on-line model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills \textendash{} and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at https://sites.google.com/view/pddm/.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/NU3ZJ22M/Nagabandi et al. - 2020 - Deep Dynamics Models for Learning Dexterous Manipu.pdf}
}

@inproceedings{nagumoUber1942,
  title = {\"Uber Die {{Lage}} Der {{Integralkurven}} Gew\"ohnlicher {{Differentialgleichungen}}},
  booktitle = {Proceedings of the {{Physico-Mathematical Society}} of {{Japan}}. 3rd {{Series}}},
  author = {Nagumo, Mitio},
  year = {1942},
  volume = {24},
  pages = {551--559},
  doi = {10.11429/PPMSJ1919.24.0_551},
  abstract = {Semantic Scholar extracted view of "\"Uber die Lage der Integralkurven gew\"ohnlicher Differentialgleichungen" by Mitio Nagumo}
}

@inproceedings{naish-guzmanGeneralized2008,
  title = {The {{Generalized FITC Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Naish-guzman}, Andrew and Holden, Sean},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Zotero/storage/F98CE2B4/Naish-guzman and Holden - 2008 - The Generalized FITC Approximation.pdf}
}

@article{nakkaChanceConstrained2021,
  title = {Chance-{{Constrained Trajectory Optimization}} for {{Safe Exploration}} and {{Learning}} of {{Nonlinear Systems}}},
  author = {Nakka, Yashwanth Kumar and Liu, Anqi and Shi, Guanya and Anandkumar, Anima and Yue, Yisong and Chung, Soon-Jo},
  year = {2021},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {389--396},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3044033},
  abstract = {Learning-based control algorithms require data collection with abundant supervision for training. Safe exploration algorithms ensure the safety of this data collection process even when only partial knowledge is available. We present a new approach for optimal motion planning with safe exploration that integrates chance-constrained stochastic optimal control with dynamics learning and feedback control. We derive an iterative convex optimization algorithm that solves an Information-cost Stochastic Nonlinear Optimal Control problem (Info-SNOC). The optimization objective encodes control cost for performance and exploration cost for learning, and the safety is incorporated as distributionally robust chance constraints. The dynamics are predicted from a robust regression model that is learned from data. The Info-SNOC algorithm is used to compute a sub-optimal pool of safe motion plans that aid in exploration for learning unknown residual dynamics under safety constraints. A stable feedback controller is used to execute the motion plan and collect data for model learning. We prove the safety of rollout from our exploration method and reduction in uncertainty over epochs, thereby guaranteeing the consistency of our learning method. We validate the effectiveness of Info-SNOC by designing and implementing a pool of safe trajectories for a planar robot. We demonstrate that our approach has higher success rate in ensuring safety when compared to a deterministic trajectory optimization approach.},
  keywords = {Chance constraints,Computational modeling,Data models,Dynamics,machine learning for robot control,model learning for control,motion and path planning,Optimal control,Planning,Safety,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/IDB5STY5/Nakka et al. - 2021 - Chance-Constrained Trajectory Optimization for Saf.pdf}
}

@inproceedings{nazaretVariational2022,
  title = {Variational {{Inference}} for {{Infinitely Deep Neural Networks}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Nazaret, Achille and Blei, David},
  year = {2022},
  month = jun,
  pages = {16447--16461},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We introduce the unbounded depth neural network (UDN), an infinitely deep probabilistic model that adapts its complexity to the training data. The UDN contains an infinite sequence of hidden layers and places an unbounded prior on a truncation L, the layer from which it produces its data. Given a dataset of observations, the posterior UDN provides a conditional distribution of both the parameters of the infinite neural network and its truncation. We develop a novel variational inference algorithm to approximate this posterior, optimizing a distribution of the neural network weights and of the truncation depth L, and without any upper limit on L. To this end, the variational family has a special structure: it models neural network weights of arbitrary depth, and it dynamically creates or removes free variational parameters as its distribution of the truncation is optimized. (Unlike heuristic approaches to model search, it is solely through gradient-based optimization that this algorithm explores the space of truncations.) We study the UDN on real and synthetic data. We find that the UDN adapts its posterior depth to the dataset complexity; it outperforms standard neural networks of similar computational complexity; and it outperforms other approaches to infinite-depth neural networks.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/GK2E2BWK/Nazaret and Blei - 2022 - Variational Inference for Infinitely Deep Neural N.pdf}
}

@article{nguyen-tuongModel2009,
  title = {Model Learning with Local Gaussian Process Regression},
  author = {{Nguyen-Tuong}, Duy and Seeger, Matthias and Peters, Jan},
  year = {2009},
  journal = {Advanced Robotics},
  volume = {23},
  number = {15},
  eprint = {https://doi.org/10.1163/016918609X12529286896877},
  pages = {2015--2034},
  publisher = {{Taylor \& Francis}},
  doi = {10.1163/016918609X12529286896877}
}

@inproceedings{nguyenFast2014,
  title = {Fast {{Allocation}} of {{Gaussian Process Experts}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Nguyen, Trung and Bonilla, Edwin},
  year = {2014},
  month = jan,
  pages = {145--153},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We propose a scalable nonparametric Bayesian regression model based on a mixture of Gaussian process (GP) experts  and the inducing points formalism underpinning sparse GP approximations. Each expert is augmented with a set of inducing points, and the allocation of data points to experts is defined probabilistically based on their proximity to the experts. This allocation mechanism enables a fast variational inference procedure for learning of the inducing inputs and hyperparameters of the experts. When using K experts, our method can  run K\^2 times faster and use K\^2 times less memory than popular sparse methods such as the FITC approximation. Furthermore, it is easy to parallelize and handles non-stationarity  straightforwardly. Our experiments show that on medium-sized datasets (of around 10\^4 training points) it  trains up to 5 times faster than FITC while achieving comparable accuracy. On a large dataset  of 10\^5 training points, our method significantly outperforms six  competitive baselines while requiring only a few hours of training.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3M2SHR5F/Nguyen and Bonilla - 2014 - Fast Allocation of Gaussian Process Experts.pdf}
}

@article{nguyenModel2020,
  title = {Model {{Predictive Control}} for {{Micro Aerial Vehicles}}: {{A Survey}}},
  shorttitle = {Model {{Predictive Control}} for {{Micro Aerial Vehicles}}},
  author = {Nguyen, Huan and Kamel, Mina and Alexis, Kostas and Siegwart, Roland},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.11104 [cs]},
  eprint = {2011.11104},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper presents a review of the design and application of model predictive control strategies for Micro Aerial Vehicles and specifically multirotor configurations such as quadrotors. The diverse set of works in the domain is organized based on the control law being optimized over linear or nonlinear dynamics, the integration of state and input constraints, possible fault-tolerant design, if reinforcement learning methods have been utilized and if the controller refers to free-flight or other tasks such as physical interaction or load transportation. A selected set of comparison results are also presented and serve to provide insight for the selection between linear and nonlinear schemes, the tuning of the prediction horizon, the importance of disturbance observer-based offset-free tracking and the intrinsic robustness of such methods to parameter uncertainty. Furthermore, an overview of recent research trends on the combined application of modern deep reinforcement learning techniques and model predictive control for multirotor vehicles is presented. Finally, this review concludes with explicit discussion regarding selected open-source software packages that deliver off-the-shelf model predictive control functionality applicable to a wide variety of Micro Aerial Vehicle configurations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/SASSEGG9/Nguyen et al. - 2020 - Model Predictive Control for Micro Aerial Vehicles.pdf;/Users/scannea1/Zotero/storage/P8F5EY7X/2011.html}
}

@article{nguyenStochastic2018,
  title = {Stochastic Variational Hierarchical Mixture of Sparse {{Gaussian}} Processes for Regression},
  author = {Nguyen, Thi Nhat Anh and Bouzerdoum, Abdesselam and Phung, Son Lam},
  year = {2018},
  month = dec,
  journal = {Machine Learning},
  volume = {107},
  number = {12},
  pages = {1947--1986},
  issn = {1573-0565},
  doi = {10.1007/s10994-018-5721-5},
  abstract = {In this article, we propose a scalable Gaussian process (GP) regression method that combines the advantages of both global and local GP approximations through a two-layer hierarchical model using a variational inference framework. The upper layer consists of a global sparse GP to coarsely model the entire data set, whereas the lower layer comprises a mixture of sparse GP experts which exploit local information to learn a fine-grained model. A two-step variational inference algorithm is developed to learn the global GP, the GP experts and the gating network simultaneously. Stochastic optimization can be employed to allow the application of the model to large-scale problems. Experiments on a wide range of benchmark data sets demonstrate the flexibility, scalability and predictive power of the proposed method.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/LA4BFTY4/Nguyen et al. - 2018 - Stochastic variational hierarchical mixture of spa.pdf}
}

@article{nickischApproximations2008,
  title = {Approximations for {{Binary Gaussian Process Classification}}},
  author = {Nickisch, Hannes and Rasmussen, Carl Edward},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  pages = {2035--2078},
  abstract = {We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/R7ZVMTFH/Nickisch and Rasmussen - Approximations for Binary Gaussian Process Classiﬁ.pdf}
}

@inproceedings{NIPS2005_f499d34b,
  ids = {meedsAlternative2006},
  title = {An Alternative Infinite Mixture of Gaussian Process Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Meeds, Edward and Osindero, Simon},
  editor = {Weiss, Y. and Sch{\"o}lkopf, B. and Platt, J.},
  year = {2006},
  volume = {18},
  publisher = {{MIT Press}},
  file = {/Users/scannea1/Zotero/storage/BQWNWLFY/Meeds and Osindero - 2006 - An Alternative Infinite Mixture Of Gaussian Proces.pdf}
}

@inproceedings{NIPS2008_f4b9ec30,
  ids = {yuanVariational2009},
  title = {Variational Mixture of Gaussian Process Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yuan, Chao and Neubauer, Claus},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  volume = {21},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Zotero/storage/TVYKP872/Yuan and Neubauer - 2009 - Variational Mixture of Gaussian Process Experts.pdf}
}

@inproceedings{okadaVariational2020,
  title = {Variational {{Inference MPC}} for {{Bayesian Model-based Reinforcement Learning}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Okada, Masashi and Taniguchi, Tadahiro},
  year = {2020},
  month = may,
  pages = {258--272},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In recent studies on model-based reinforcement learning (MBRL), incorporating uncertainty in forward dynamics is a state-of-the-art strategy to enhance learning performance, making MBRLs competitiv...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4R3LXT8H/Okada and Taniguchi - 2020 - Variational Inference MPC for Bayesian Model-based.pdf;/Users/scannea1/Zotero/storage/XWZNGU6G/okada20a.html}
}

@inproceedings{osawaPractical2019,
  title = {Practical {{Deep Learning}} with {{Bayesian Principles}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osawa, Kazuki and Swaroop, Siddharth and Khan, Mohammad Emtiyaz E and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E and Yokota, Rio},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on out-of-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation is available as a plug-and-play optimiser.},
  file = {/Users/scannea1/Zotero/storage/8CF9CNB4/Osawa et al. - 2019 - Practical Deep Learning with Bayesian Principles.pdf}
}

@inproceedings{osbandMoreEfficientReinforcement2013,
  title = {({{More}}) {{Efficient Reinforcement Learning}} via {{Posterior Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  keywords = {reading-list},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Osband et al-2013 (More) Efficient Reinforcement Learning via Posterior Sampling/Osband et al_2013_(More) Efficient Reinforcement Learning via Posterior Sampling.pdf}
}

@inproceedings{osbandWhyPosteriorSampling2017,
  title = {Why Is {{Posterior Sampling Better}} than {{Optimism}} for {{Reinforcement Learning}}?},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Osband, Ian and Roy, Benjamin Van},
  year = {2017},
  month = jul,
  pages = {2701--2710},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an {$\mathsl{O}\tilde~$}({$\mathsl{H}\mathsl{S}\mathsl{A}\mathsl{T}$}-----{$\surd$})O\textasciitilde (HSAT)\textbackslash tilde\{O\}(H\textbackslash sqrt\{SAT\}) Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of {$\mathsl{O}\tilde~$}({$\mathsl{H}\mathsl{S}\mathsl{A}\mathsl{T}$}---{$\surd$})O\textasciitilde (HSAT)\textbackslash tilde\{O\}(H S \textbackslash sqrt\{AT\}) for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.},
  langid = {english},
  keywords = {reading-list},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Osband_Roy-2017 Why is Posterior Sampling Better than Optimism for Reinforcement Learning/Osband_Roy_2017_Why is Posterior Sampling Better than Optimism for Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/4A5HQH39/Osband and Roy - 2017 - Why is Posterior Sampling Better than Optimism for.pdf}
}

@article{panEfficient2018,
  title = {Efficient {{Reinforcement Learning}} via {{Probabilistic Trajectory Optimization}}},
  author = {Pan, Yunpeng and Boutselis, George I. and Theodorou, Evangelos A.},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {11},
  pages = {5459--5474},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2017.2764499},
  abstract = {We present a trajectory optimization approach to reinforcement learning in continuous state and action spaces, called probabilistic differential dynamic programming (PDDP). Our method represents systems dynamics using Gaussian processes (GPs), and performs local dynamic programming iteratively around a nominal trajectory in Gaussian belief spaces. Different from model-based policy search methods, PDDP does not require a policy parameterization and learns a time-varying control policy via successive forward-backward sweeps. A convergence analysis of the iterative scheme is given, showing that our algorithm converges to a stationary point globally under certain conditions. We show that prior model knowledge can be incorporated into the proposed framework to speed up learning, and a generalized optimization criterion based on the predicted cost distribution can be employed to enable risk-sensitive learning. We demonstrate the effectiveness and efficiency of the proposed algorithm using nontrivial tasks. Compared with a state-of-the-art GP-based policy search method, PDDP offers a superior combination of learning speed, data efficiency, and applicability.},
  keywords = {Aerospace electronics,Data models,Dynamic programming,Gaussian processes (GPs),Heuristic algorithms,optimal control,Optimal control,Predictive models,Probabilistic logic,reinforcement learning (RL),trajectory optimization,Trajectory optimization},
  file = {/Users/scannea1/Zotero/storage/UIRUIIA8/Pan et al. - 2018 - Efficient Reinforcement Learning via Probabilistic.pdf;/Users/scannea1/Zotero/storage/P62DM6XT/8306829.html}
}

@inproceedings{panProbabilistic2014,
  title = {Probabilistic {{Differential Dynamic Programming}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pan, Yunpeng and Theodorou, Evangelos},
  year = {2014},
  volume = {27},
  pages = {1907--1915},
  abstract = {We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradientbased policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/D4XA2UIZ/Pan and Theodorou - Probabilistic Differential Dynamic Programming.pdf}
}

@inproceedings{panSample2015,
  title = {Sample {{Efficient Path Integral Control}} under {{Uncertainty}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pan, Yunpeng and Theodorou, Evangelos and Kontitsis, Michail},
  year = {2015},
  volume = {28},
  file = {/Users/scannea1/Zotero/storage/J6YZYW7T/Fleisher - 1988 - The Hopfield Model with Multi-Level Neurons.pdf;/Users/scannea1/Zotero/storage/RDCXT48K/81ca0262c82e712e50c580c032d99b60-Paper.html}
}

@inproceedings{parmasPIPPS2018,
  title = {{{PIPPS}}: {{Flexible Model-Based Policy Search Robust}} to the {{Curse}} of {{Chaos}}},
  shorttitle = {{{PIPPS}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  year = {2018},
  month = jul,
  pages = {4065--4074},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimizati...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/XGUGY4P8/Parmas et al. - 2018 - PIPPS Flexible Model-Based Policy Search Robust t.pdf;/Users/scannea1/Zotero/storage/QXP623BU/parmas18a.html}
}

@inproceedings{pathakCuriositydrivenExplorationSelfsupervised2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {2778--2787},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Pathak et al-2017 Curiosity-driven Exploration by Self-supervised Prediction/Pathak et al_2017_Curiosity-driven Exploration by Self-supervised Prediction.pdf}
}

@incollection{patilScaling2015,
  title = {Scaling up {{Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization}} and {{Automatic Differentiation}}},
  booktitle = {Algorithmic {{Foundations}} of {{Robotics XI}}: {{Selected Contributions}} of the {{Eleventh International Workshop}} on the {{Algorithmic Foundations}} of {{Robotics}}},
  author = {Patil, Sachin and Kahn, Gregory and Laskey, Michael and Schulman, John and Goldberg, Ken and Abbeel, Pieter},
  editor = {Akin, H. Levent and Amato, Nancy M. and Isler, Volkan and {van der Stappen}, A. Frank},
  year = {2015},
  series = {Springer {{Tracts}} in {{Advanced Robotics}}},
  pages = {515--533},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-16595-0_30},
  abstract = {Belief space planning provides a principled framework to compute motion plans that explicitly gather information from sensing, as necessary, to reduce uncertainty about the robot and the environment. We consider the problem of planning in Gaussian belief spaces, which are parameterized in terms of mean states and covariances describing the uncertainty. In this work, we show that it is possible to compute locally optimal plans without including the covariance in direct trajectory optimization formulations of the problem. As a result, the dimensionality of the problem scales linearly in the state dimension instead of quadratically, as would be the case if we were to include the covariance in the optimization. We accomplish this by taking advantage of recent advances in numerical optimal control that include automatic differentiation and state of the art convex solvers. We show that the running time of each optimization step of the covariance-free trajectory optimization is {$\mathsl{O}$}({$\mathsl{n}$}3{$\mathsl{T}$})O(n3T)O(n\^3T), where {$\mathsl{n}$}nn is the dimension of the state space and {$\mathsl{T}$}TT is the number of time steps in the trajectory. We present experiments in simulation on a variety of planning problems under uncertainty including manipulator planning, estimating unknown model parameters for dynamical systems, and active simultaneous localization and mapping (active SLAM). Our experiments suggest that our method can solve planning problems in 100100100 dimensional state spaces and obtain computational speedups of 400\texttimes 400\texttimes 400\textbackslash times over related trajectory optimization methods .},
  isbn = {978-3-319-16595-0},
  langid = {english},
  keywords = {Automatic Differentiation,Model Predictive Control,Optimal Control Method,Sequential Quadratic Programming,Trajectory Optimization},
  file = {/Users/scannea1/Zotero/storage/5T8GHMQX/Patil et al. - 2015 - Scaling up Gaussian Belief Space Planning Through .pdf}
}

@inproceedings{polymenakosSafe2019,
  ids = {polymenakosSafe2019a},
  title = {Safe {{Policy Search Using Gaussian Process Models}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  author = {Polymenakos, Kyriakos and Abate, Alessandro and Roberts, Stephen},
  year = {2019},
  month = may,
  series = {{{AAMAS}} '19},
  pages = {1565--1573},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {{Richland, SC}},
  abstract = {We propose a method to optimise the parameters of a policy which will be used to safely perform a given task in a data-efficient manner. We train a Gaussian process model to capture the system dynamics, based on the PILCO framework. The model has useful analytic properties, which allow closed form computation of error gradients and the probability of violating given state space constraints. Even during training, only policies that are deemed safe are implemented on the real system, minimising the risk of catastrophic failure.},
  isbn = {978-1-4503-6309-9},
  keywords = {gaussian processes,model-based reinforcement learning,safety critical systems},
  file = {/Users/scannea1/Zotero/storage/LEEH5ULN/Polymenakos et al. - 2019 - Safe Policy Search Using Gaussian Process Models.pdf}
}

@book{pontryagin1987mathematical,
  title = {Mathematical Theory of Optimal Processes},
  author = {Pontryagin, Lev Semenovich},
  year = {1987},
  publisher = {{CRC press}}
}

@article{prajnaBarrier2006,
  title = {Barrier Certificates for Nonlinear Model Validation},
  author = {Prajna, Stephen},
  year = {2006},
  month = jan,
  journal = {Automatica},
  volume = {42},
  number = {1},
  pages = {117--126},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2005.08.007},
  abstract = {Methods for model validation of continuous-time nonlinear systems with uncertain parameters are presented in this paper. The methods employ functions of state-parameter-time, termed barrier certificates, whose existence proves that a model and a feasible parameter set are inconsistent with some time-domain experimental data. A very large class of models can be treated within this framework; this includes differential-algebraic models, models with memoryless/dynamic uncertainties, and hybrid models. Construction of barrier certificates can be performed by convex optimization, utilizing recent results on the sum of squares decomposition of multivariate polynomials.},
  langid = {english},
  keywords = {Barrier certificates,Model validation,Nonlinear systems,Semidefinite programming relaxations},
  file = {/Users/scannea1/Zotero/storage/2GNGMD4X/Prajna - 2006 - Barrier certificates for nonlinear model validatio.pdf;/Users/scannea1/Zotero/storage/JE8FZWCV/S0005109805002839.html}
}

@inproceedings{prajnaSafety2004,
  title = {Safety {{Verification}} of {{Hybrid Systems Using Barrier Certificates}}},
  booktitle = {In {{Hybrid Systems}}: {{Computation}} and {{Control}}},
  author = {Prajna, Stephen and Jadbabaie, Ali},
  year = {2004},
  pages = {477--492},
  publisher = {{Springer}},
  abstract = {This paper presents a novel methodology for safety verification  of hybrid systems. For proving that all trajectories of a hybrid  system do not enter an unsafe region, the proposed method uses a function  of state termed a barrier certificate. The zero level set of a barrier  certificate separates the unsafe region from all possible trajectories starting  from a given set of initial conditions, hence providing an exact proof  of system safety. No explicit computation of reachable sets is required in  the construction of barrier certificates, which makes nonlinearity, uncertainty,  and constraints can be handled directly within this framework.},
  file = {/Users/scannea1/Zotero/storage/DQXKMT8U/Prajna and Jadbabaie - 2004 - Safety Verification of Hybrid Systems Using Barrie.pdf;/Users/scannea1/Zotero/storage/5F4IM4ZH/summary.html}
}

@inproceedings{quinonero-candelaPropagation2003,
  title = {Propagation of Uncertainty in {{Bayesian}} Kernel Models - Application to Multiple-Step Ahead Forecasting},
  booktitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}.},
  author = {{Quinonero-Candela}, Joaquin and Girard, A. and Larsen, J. and Rasmussen, C.E.},
  year = {2003},
  month = apr,
  volume = {2},
  pages = {II-701},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2003.1202463},
  abstract = {The object of Bayesian modelling is predictive distribution, which, in a forecasting scenario, enables evaluation of forecasted values and their uncertainties. We focus on reliably estimating the predictive mean and variance of forecasted values using Bayesian kernel based models such as the Gaussian process and the relevance vector machine. We derive novel analytic expressions for the predictive mean and variance for Gaussian kernel shapes under the assumption of a Gaussian input distribution in the static case, and of a recursive Gaussian predictive density in iterative forecasting. The capability of the method is demonstrated for forecasting of time-series and compared to approximate methods.},
  keywords = {Analysis of variance,Bayesian methods,Covariance matrix,Equations,Intelligent networks,Kernel,Predictive models,Taylor series,Testing,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/MIKCW6A4/Candela et al. - 2003 - Propagation of uncertainty in Bayesian kernel mode.pdf;/Users/scannea1/Zotero/storage/8E99C732/1202463.html}
}

@article{quinonero-candelaUnifying2005,
  title = {A {{Unifying View}} of {{Sparse Approximate Gaussian Process Regression}}},
  author = {{Qui{\~n}onero-Candela}, Joaquin and Rasmussen, Carl Edward},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {65},
  pages = {1939--1959},
  abstract = {We provide a new unifying view, including all existing proper probabilistic   sparse approximations for Gaussian process regression. Our approach relies on   expressing the effective prior which the methods are using. This   allows new insights to be gained, and highlights the relationship between   existing methods. It also allows for a clear theoretically justified ranking   of the closeness of the known approximations to the corresponding full GPs.   Finally we point directly to designs of new better sparse approximations,   combining the best of the existing strategies, within attractive   computational constraints.},
  file = {/Users/scannea1/Zotero/storage/H6HSMVQS/Quiñonero-Candela and Rasmussen - 2005 - A Unifying View of Sparse Approximate Gaussian Pro.pdf}
}

@inproceedings{ranaEuclideanizing2020a,
  title = {Euclideanizing {{Flows}}: {{Diffeomorphic Reduction}} for {{Learning Stable Dynamical Systems}}},
  shorttitle = {Euclideanizing {{Flows}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Rana, Muhammad Asif and Li, Anqi and Fox, Dieter and Boots, Byron and Ramos, Fabio and Ratliff, Nathan},
  year = {2020},
  month = jul,
  pages = {630--639},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Execution of complex tasks in robotics requires motions that have complex geometric structure. We present an approach which allows robots to learn such motions from a few human demonstrations. The motions are encoded as rollouts of a dynamical system on a Riemannian manifold. Additional structure is imposed which guarantees smooth convergent motions to a goal location. The aforementioned structure involves viewing motions on an observed Riemannian manifold as deformations of straight lines on a latent Euclidean space. The observed and latent spaces are related through a diffeomorphism. Thus, this paper presents an approach for learning flexible diffeomorphisms, resulting in a stable dynamical system.  The efficacy of this approach is demonstrated through validation on an established benchmark as well demonstrations collected on a real-world robotic system.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3TZFTDTQ/Rana et al. - 2020 - Euclideanizing Flows Diffeomorphic Reduction for .pdf}
}

@book{rasmussenGaussian2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/scannea1/Zotero/storage/9KYKSDXH/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@inproceedings{rasmussenInfinite2001,
  title = {Infinite {{Mixtures}} of {{Gaussian Process Experts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rasmussen, Carl and Ghahramani, Zoubin},
  year = {2001},
  volume = {14},
  pages = {881--888},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/K55Z2K8S/Rasmussen and Ghahramani - 2001 - Infinite Mixtures of Gaussian Process Experts.pdf;/Users/scannea1/Zotero/storage/X9FGAZH2/9afefc52942cb83c7c1f14b2139b09ba-Abstract.html}
}

@inproceedings{rasmussenOccam2001,
  title = {Occam' s {{Razor}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rasmussen, Carl and Ghahramani, Zoubin},
  year = {2001},
  volume = {13},
  publisher = {{MIT Press}},
  file = {/Users/scannea1/Zotero/storage/SYEYLMMN/Rasmussen and Ghahramani - 2001 - Occam' s Razor.pdf}
}

@phdthesis{rawlikProbabilistic2013,
  title = {On {{Probabilistic Inference Approaches}} to {{Stochastic Optimal Control}}},
  author = {Rawlik, Konrad C},
  year = {2013},
  abstract = {While stochastic optimal control, together with associate formulations like Reinforcement Learning, provides a formal approach to, amongst other, motor control, it remains computationally challenging for most practical problems. This thesis is concerned with the study of relations between stochastic optimal control and probabilistic inference. Such dualities \textendash{} exemplified by the classical Kalman Duality between the Linear-Quadratic-Gaussian control problem and the filtering problem in Linear-Gaussian dynamical systems \textendash{} make it possible to exploit advances made within the separate fields. In this context, the emphasis in this work lies with utilisation of approximate inference methods for the control problem.},
  langid = {english},
  school = {University of Edinburgh},
  file = {/Users/scannea1/Zotero/storage/H74IMGYX/Rawlik - On Probabilistic Inference Approaches to Stochasti.pdf}
}

@inproceedings{rawlikStochastic2013,
  title = {On {{Stochastic Optimal Control}} and {{Reinforcement Learning}} by {{Approximate Inference}} ({{Extended Abstract}})},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Artificial Intelligence}}},
  author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
  year = {2013},
  pages = {5},
  abstract = {We present a reformulation of the stochastic optimal control problem in terms of KL divergence minimisation, not only providing a unifying perspective of previous approaches in this area, but also demonstrating that the formalism leads to novel practical approaches to the control problem. Specifically, a natural relaxation of the dual formulation gives rise to exact iterative solutions to the finite and infinite horizon stochastic optimal control problem, while direct application of Bayesian inference methods yields instances of risk sensitive control.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/XRQ3NLZ6/Rawlik et al. - On Stochastic Optimal Control and Reinforcement Le.pdf}
}

@article{rockafellarConditional2001,
  title = {Conditional {{Value-at-Risk}} for {{General Loss Distributions}}},
  author = {Rockafellar, R. and Uryasev, S.},
  year = {2001},
  journal = {Journal of Banking \& Finance},
  pages = {1443--1471},
  abstract = {Fundamental properties of conditional value-at-risk, as a measure of risk with significant advantages over value-at-risk, are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. Conditional value-at-risk is able to quantify dangers beyond value-at-risk, and moreover it is coherent. It provides optimization shortcuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/2837SYH3/d243a33c209f41453927fe969949288abb4c3382.html}
}

@inproceedings{rohrProbabilistic2021,
  title = {Probabilistic Robust Linear Quadratic Regulators with {{Gaussian}} Processes},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {von Rohr, Alexander and {Neumann-Brosig}, Matthias and Trimpe, Sebastian},
  year = {2021},
  month = may,
  pages = {324--335},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Probabilistic models such as Gaussian processes (GPs) are powerful tools to learn unknown dynamical systems from data for subsequent use in control design. While learning-based control has the pote...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/49R9IA7G/Rohr et al. - 2021 - Probabilistic robust linear quadratic regulators w.pdf;/Users/scannea1/Zotero/storage/BX9VVXBP/rohr21a.html}
}

@article{rosenbrockDifferential1972,
  title = {Differential {{Dynamic Programming}}. {{By D}}.{{H}}. {{Jacobson}} and {{D}}. {{Q}}. {{Mayne}}. {{Pp}}. Viii, 208. 1970. ({{Elsevier}}.)},
  author = {Rosenbrock, H. H.},
  year = {1972},
  month = feb,
  journal = {The Mathematical Gazette},
  volume = {56},
  number = {395},
  pages = {78--78},
  publisher = {{Cambridge University Press}},
  issn = {0025-5572, 2056-6328},
  doi = {10.2307/3613752},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0025557200188072/resource/name/firstPage-S0025557200188072a.jpg},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/BDJAYGT9/CB6EA5B3191A305CFB1D2CDF9CC5BA1D.html}
}

@inproceedings{rossiSparse2021,
  title = {Sparse {{Gaussian Processes Revisited}}: {{Bayesian Approaches}} to {{Inducing-Variable Approximations}}},
  shorttitle = {Sparse {{Gaussian Processes Revisited}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Rossi, Simone and Heinonen, Markus and Bonilla, Edwin and Shen, Zheyang and Filippone, Maurizio},
  year = {2021},
  month = mar,
  pages = {1837--1845},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Besides enabling scalability, one of their main advantages over sparse approximations using direct marginal likelihood maximization is that they provide a robust alternative for point estimation of the inducing inputs, i.e. the location of the inducing variables. In this work we challenge the common wisdom that optimizing the inducing inputs in the variational framework yields optimal performance. We show that, by revisiting old model approximations such as the fully-independent training conditionals endowed with powerful sampling-based inference methods, treating both inducing locations and GP hyper-parameters in a Bayesian way can improve performance significantly. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its state-of-the-art performance through an extensive experimental campaign across several regression and classification problems.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/IZRAMFPA/Rossi et al. - 2021 - Sparse Gaussian Processes Revisited Bayesian Appr.pdf;/Users/scannea1/Zotero/storage/UUBVVQJT/Rossi et al. - 2021 - Sparse Gaussian Processes Revisited Bayesian Appr.pdf}
}

@article{rossPseudospectral2004,
  title = {Pseudospectral Methods for Optimal Motion Planning of Differentially Flat Systems},
  author = {Ross, I. M. and Fahroo, F.},
  year = {2004},
  month = aug,
  journal = {IEEE Transactions on Automatic Control},
  volume = {49},
  number = {8},
  pages = {1410--1413},
  issn = {1558-2523},
  doi = {10.1109/TAC.2004.832972},
  abstract = {The article presents some preliminary results on combining two new ideas from nonlinear control theory and dynamic optimization. We show that the computational framework facilitated by pseudospectral methods applies quite naturally and easily to Fliess' implicit state variable representation of dynamical systems. The optimal motion planning problem for differentially flat systems is equivalent to a classic Bolza problem of the calculus of variations. We exploit the notion that derivatives of flat outputs given in terms of Lagrange polynomials at Legendre-Gauss-Lobatto points can be quickly computed using pseudospectral differentiation matrices. Additionally, the Legendre pseudospectral method approximates integrals by Gauss-type quadrature rules. The application of this method to the two-dimensional crane model reveals how differential flatness may be readily exploited.},
  keywords = {Asymptotic stability,Automatic control,Bolza problem,Control systems,Control theory,differentially flat systems,differentiation,dynamic optimization,dynamical systems,Fliess implicit state variable representation,Gauss-type quadrature rules,integration,Lagrange polynomials,Lagrange pseudospectral methods,Legendre-Gauss-Lobatto points,Lyapunov method,matrix algebra,nonlinear control systems,nonlinear control theory,Nonlinear systems,optimal control,optimal motion planning,optimisation,Optimization methods,path planning,polynomials,pseudospectral differentiation matrices,Rivers,Time varying systems,time-varying systems,two-dimensional crane model,variational calculus,variational techniques},
  file = {/Users/scannea1/Zotero/storage/TY8D8LGT/Ross and Fahroo - 2004 - Pseudospectral methods for optimal motion planning.pdf;/Users/scannea1/Zotero/storage/BN6XJXPM/1323189.html}
}

@inproceedings{royDirectBehaviorSpecification2022,
  title = {Direct {{Behavior Specification}} via {{Constrained Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Roy, Julien and Girgis, Roger and Romoff, Joshua and Bacon, Pierre-Luc and Pal, Chris J.},
  year = {2022},
  month = jun,
  pages = {18828--18843},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The standard formulation of Reinforcement Learning lacks a practical way of specifying what are admissible and forbidden behaviors. Most often, practitioners go about the task of behavior specification by manually engineering the reward function, a counter-intuitive process that requires several iterations and is prone to reward hacking by the agent. In this work, we argue that constrained RL, which has almost exclusively been used for safe RL, also has the potential to significantly reduce the amount of work spent for reward specification in applied RL projects. To this end, we propose to specify behavioral preferences in the CMDP framework and to use Lagrangian methods to automatically weigh each of these behavioral constraints. Specifically, we investigate how CMDPs can be adapted to solve goal-based tasks while adhering to several constraints simultaneously. We evaluate this framework on a set of continuous control tasks relevant to the application of Reinforcement Learning for NPC design in video games.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Roy et al-2022 Direct Behavior Specification via Constrained Reinforcement Learning/Roy et al_2022_Direct Behavior Specification via Constrained Reinforcement Learning.pdf}
}

@article{rybkinModelBased2021,
  title = {Model-{{Based Reinforcement Learning}} via {{Latent-Space Collocation}}},
  author = {Rybkin, Oleh and Zhu, Chuning and Nagabandi, Anusha and Daniilidis, Kostas and Mordatch, Igor and Levine, Sergey},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.13229 [cs]},
  eprint = {2106.13229},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The ability to plan into the future while utilizing only raw high-dimensional observations, such as images, can provide autonomous agents with broad capabilities. Visual model-based reinforcement learning (RL) methods that plan future actions directly have shown impressive results on tasks that require only short-horizon reasoning, however, these methods struggle on temporally extended tasks. We argue that it is easier to solve long-horizon tasks by planning sequences of states rather than just actions, as the effects of actions greatly compound over time and are harder to optimize. To achieve this, we draw on the idea of collocation, which has shown good results on long-horizon tasks in optimal control literature, and adapt it to the image-based setting by utilizing learned latent state space models. The resulting latent collocation method (LatCo) optimizes trajectories of latent states, which improves over previously proposed shooting methods for visual model-based RL on tasks with sparse rewards and long-term goals. Videos and code at https://orybkin.github.io/latco/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/MDHUIW24/Rybkin et al. - 2021 - Model-Based Reinforcement Learning via Latent-Spac.pdf;/Users/scannea1/Zotero/storage/RARRLXIJ/2106.html}
}

@inproceedings{sadighSafe2016,
  ids = {sadighSafe2016a},
  title = {Safe {{Control Under Uncertainty}} with {{Probabilistic Signal Temporal Logic}}},
  booktitle = {Proceedings of {{Robotics}}: {{Science}} and {{Systems XII}}},
  author = {Sadigh, Dorsa and Kapoor, Ashish},
  year = {2016},
  month = jun,
  abstract = {Controller synthesis for hybrid systems that satisfy temporal specifications expressing various system properties is a challenging problem that has drawn the attention of many researchers. However, making the assumption that such temporal properties are deterministic is far from the reality. For example, many of the properties the controller has to satisfy are learned through machine [\ldots ]},
  langid = {american},
  file = {/Users/scannea1/Zotero/storage/33KXXXBE/Sadigh and Kapoor - 2016 - Safe Control under Uncertainty with Probabilistic .pdf;/Users/scannea1/Zotero/storage/JT2L6HUF/safe-control-uncertainty-probabilistic-signal-temporal-logic.html}
}

@inproceedings{salimbeniDeep2019,
  title = {Deep {{Gaussian Processes}} with {{Importance-Weighted Variational Inference}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Salimbeni, Hugh and Dutordoir, Vincent and Hensman, James and Deisenroth, Marc},
  year = {2019},
  month = may,
  pages = {5589--5598},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Deep Gaussian processes (DGPs) can model complex marginal densities as well as complex mappings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DG...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/M3UC5GE3/Salimbeni et al. - 2019 - Deep Gaussian Processes with Importance-Weighted V.pdf;/Users/scannea1/Zotero/storage/SN6XD5GN/salimbeni19a.html}
}

@inproceedings{salimbeniDoubly2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  year = {2017},
  volume = {30},
  pages = {4588--4599},
  langid = {english},
  keywords = {deep-gaussian-processes,gaussian-processes,variational-inference},
  file = {/Users/scannea1/Zotero/storage/6IZTGNS9/Salimbeni and Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;/Users/scannea1/Zotero/storage/QV5N9ZPA/8208974663db80265e9bfe7b222dcb18-Abstract.html}
}

@article{sasakiVariational2021,
  title = {Variational Policy Search Using Sparse {{Gaussian}} Process Priors for Learning Multimodal Optimal Actions},
  author = {Sasaki, Hikaru and Matsubara, Takamitsu},
  year = {2021},
  month = jun,
  journal = {Neural Networks},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.06.010},
  abstract = {Policy search reinforcement learning has been drawing much attention as a method of learning a robot control policy. In particular, policy search using such non-parametric policies as Gaussian process regression can learn optimal actions with high-dimensional and redundant sensors as input. However, previous methods implicitly assume that the optimal action becomes unique for each state. This assumption can severely limit such practical applications as robot manipulations since designing a reward function that appears in only one optimal action for complex tasks is difficult. The previous methods might have caused critical performance deterioration because the typical non-parametric policies cannot capture the optimal actions due to their unimodality. We propose novel approaches in non-parametric policy searches with multiple optimal actions and offer two different algorithms commonly based on a sparse Gaussian process prior and variational Bayesian inference. The following are the key ideas: (1) multimodality for capturing multiple optimal actions and (2) mode-seeking for capturing one optimal action by ignoring the others. First, we propose a multimodal sparse Gaussian process policy search that uses multiple overlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian process policy search that uses the student-t distribution for a likelihood function. The effectiveness of those algorithms is demonstrated through applications to object manipulation tasks with multiple optimal actions in simulations.},
  langid = {english},
  keywords = {Gaussian processes,Mode-seeking,Multimodality,Policy search,Reinforcement learning},
  file = {/Users/scannea1/Zotero/storage/JK92RTY5/S0893608021002422.html}
}

@inproceedings{scannellTrajectory2021,
  title = {Trajectory {{Optimisation}} in {{Learned Multimodal Dynamical Systems Via Latent-ODE Collocation}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Scannell, Aidan and Ek, Carl Henrik and Richards, Arthur},
  year = {2021},
  publisher = {{IEEE}},
  abstract = {This paper presents a two-stage method to perform trajectory optimisation in multimodal dynamical systems with unknown nonlinear stochastic transition dynamics. The method finds trajectories that remain in a preferred dynamics mode where possible and in regions of the transition dynamics model that have been observed and can be predicted confidently. The first stage leverages a Mixture of Gaussian Process Experts method to learn a predictive dynamics model from historical data. Importantly, this model learns a gating function that indicates the probability of being in a particular dynamics mode at a given state location. This gating function acts as a coordinate map for a latent Riemannian manifold on which shortest trajectories are solutions to our trajectory optimisation problem. Based on this intuition, the second stage formulates a geometric cost function, which it then implicitly minimises by projecting the trajectory optimisation onto the second-order geodesic ODE; a classic result of Riemannian geometry. A set of collocation constraints are derived that ensure trajectories are solutions to this ODE, implicitly solving the trajectory optimisation problem.},
  file = {/Users/scannea1/Zotero/storage/JLG5TV8J/Scannell et al. - 2021 - Trajectory Optimisation in Learned Multimodal Dyna.pdf}
}

@inproceedings{schneiderExploiting1996,
  title = {Exploiting {{Model Uncertainty Estimates}} for {{Safe Dynamic Control Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schneider, J.},
  year = {1996},
  volume = {9},
  pages = {1047--1053},
  abstract = {Model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems. The simplest method assumes the learned model is correct and applies dynamic programming to it, but many approximators provide uncertainty estimates on the fit. How can they be exploited? This paper addresses the case where the system must be prevented from having catastrophic failures during learning. We propose a new algorithm adapted from the dual control literature and use Bayesian locally weighted regression models with dynamic programming. A common reinforcement learning assumption is that aggressive exploration should be encouraged. This paper addresses the converse case in which the system has to reign in exploration. The algorithm is illustrated on a 4 dimensional simulated control problem.}
}

@article{schonSystem2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  journal = {Automatica},
  volume = {47},
  number = {1},
  pages = {39--49},
  issn = {00051098},
  doi = {10.1016/j.automatica.2010.10.013},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provide arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/GHK4KDGX/Schön et al. - 2011 - System identification of nonlinear state-space mod.pdf}
}

@inproceedings{schreiterSafe2015,
  title = {Safe {{Exploration}} for {{Active Learning}} with {{Gaussian Processes}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Schreiter, Jens and {Nguyen-Tuong}, Duy and Eberts, Mona and Bischoff, Bastian and Markert, Heiner and Toussaint, Marc},
  year = {2015},
  pages = {133--149},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23461-8_9},
  abstract = {In this paper, the problem of safe exploration in the active learning context is considered. Safe exploration is especially important for data sampling from technical and industrial systems, e.g. combustion engines and gas turbines, where critical and unsafe measurements need to be avoided. The objective is to learn data-based regression models from such technical systems using a limited budget of measured, i.e. labelled, points while ensuring that critical regions of the considered systems are avoided during measurements. We propose an approach for learning such models and exploring new data regions based on Gaussian processes (GP's). In particular, we employ a problem specific GP classifier to identify safe and unsafe regions, while using a differential entropy criterion for exploring relevant data regions. A theoretical analysis is shown for the proposed algorithm, where we provide an upper bound for the probability of failure. To demonstrate the efficiency and robustness of our safe exploration scheme in the active learning setting, we test the approach on a policy exploration task for the inverse pendulum hold up problem.},
  isbn = {978-3-319-23461-8},
  langid = {english},
  keywords = {Decision Boundary,Discriminative Function,Exploration Scheme,Gaussian Process,Input Space},
  file = {/Users/scannea1/Zotero/storage/8HEA83P2/Schreiter et al. - 2015 - Safe Exploration for Active Learning with Gaussian.pdf}
}

@article{schrittwieserMastering2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/JW8GMQEQ/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf;/Users/scannea1/Zotero/storage/SA8X3VBI/1911.html}
}

@article{schwarmChanceconstrained1999,
  title = {Chance-Constrained Model Predictive Control},
  author = {Schwarm, Alexander T. and Nikolaou, Michael},
  year = {1999},
  journal = {AIChE Journal},
  volume = {45},
  number = {8},
  pages = {1743--1752},
  issn = {1547-5905},
  doi = {10.1002/aic.690450811},
  abstract = {This work focuses on robustness of model-predictive control with respect to satisfaction of process output constraints. A method of improving such robustness is presented. The method relies on formulating output constraints as chance constraints using the uncertainty description of the process model. The resulting on-line optimization problem is convex. The proposed approach is illustrated through a simulation case study on a high-purity distillation column. Suggestions for further improvements are made.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690450811},
  file = {/Users/scannea1/Zotero/storage/G8ZC8FQA/aic.html}
}

@inproceedings{seegerFast2003,
  title = {Fast Forward Selection to Speed up Sparse Gaussian Process Regression},
  booktitle = {Proceedings of the {{Ninth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Seeger, Matthias and Williams, Christopher K. I. and Lawrence, Neil D.},
  year = {2003},
  abstract = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the ``support'' patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically. We demonstrate the model selection capabilities of the algorithm in a range of experiments. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods.},
  file = {/Users/scannea1/Zotero/storage/HUFUTERH/Seeger et al. - 2003 - Fast forward selection to speed up sparse gaussian.pdf;/Users/scannea1/Zotero/storage/F3LX47VP/summary.html}
}

@inproceedings{sekarPlanning2020,
  title = {Planning to {{Explore}} via {{Self-Supervised World Models}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  year = {2020},
  month = nov,
  pages = {8583--8592},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code: https://ramanans1.github.io/plan2explore/},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4RJFWDE7/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf;/Users/scannea1/Zotero/storage/79JZ8WW6/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf}
}

@article{shahriariTaking2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {1558-2256},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  keywords = {Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning},
  file = {/Users/scannea1/Zotero/storage/PX9FTIL2/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf;/Users/scannea1/Zotero/storage/XLYEGD6W/7352306.html}
}

@article{shumwayAPPROACH1982,
  title = {{{AN APPROACH TO TIME SERIES SMOOTHING AND FORECASTING USING THE EM ALGORITHM}}},
  author = {Shumway, R. and Stoffer, D.},
  year = {1982},
  journal = {Journal of Time Series Analysis},
  doi = {10.1111/J.1467-9892.1982.TB00349.X},
  abstract = {Abstract. An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters.}
}

@inproceedings{shyamModelBasedActiveExploration2019,
  title = {Model-{{Based Active Exploration}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  year = {2019},
  month = may,
  pages = {5779--5788},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Shyam et al-2019 Model-Based Active Exploration/Shyam et al_2019_Model-Based Active Exploration.pdf;/Users/scannea1/Zotero/storage/8Z8RVUMW/Shyam et al. - 2019 - Model-Based Active Exploration.pdf}
}

@article{Silverman1985,
  title = {Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting},
  author = {Silverman, B W},
  year = {1985},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {47},
  number = {1},
  pages = {1--21},
  doi = {10.1111/j.2517-6161.1985.tb01327.x},
  abstract = {Non-parametric regression using cubic splines is an attractive, flexible and widely-applicable approach to curve estimation. Although the basic idea was formulated many years ago, the method is not as widely known or adopted as perhaps it should be. The topics and examples discussed in this paper are intended to promote the understanding and extend the practicability of the spline smoothing methodology. Particular subjects covered include the basic principles of the method; the relation with moving average and other smoothing methods; the automatic choice of the amount of smoothing; and the use of residuals for diagnostic checking and model adaptation. The question of providing inference regions for curves-and for relevant properties of curves\textendash is approached via a finite-dimensional Bayesian formulation.},
  mendeley-groups = {GP/Mixture of GPs},
  keywords = {mcycle,motorcycle},
  file = {/Users/scannea1/Zotero/storage/GVBR4H9A/Silverman - 1985 - Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting.pdf}
}

@inproceedings{snelsonSparse2005a,
  title = {Sparse {{Gaussian Processes}} Using {{Pseudo-inputs}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  year = {2005},
  abstract = {We present a new Gaussian process (GP) regression model whose co-variance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M {$\ll$} N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.},
  file = {/Users/scannea1/Zotero/storage/J54QN2B7/Snelson and Ghahramani - Sparse Gaussian Processes using Pseudo-inputs.pdf}
}

@book{stengelStochastic1986,
  title = {Stochastic Optimal Control: Theory and Application},
  shorttitle = {Stochastic Optimal Control},
  author = {Stengel, Robert F.},
  year = {1986},
  publisher = {{John Wiley \& Sons, Inc.}},
  isbn = {978-0-471-86462-2}
}

@book{sutton2018reinforcement,
  title = {Reinforcement Learning, Second Edition: {{An}} Introduction},
  author = {Sutton, R.S. and Barto, A.G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  isbn = {978-0-262-35270-3}
}

@inproceedings{tassaReceding2007,
  title = {Receding {{Horizon Differential Dynamic Programming}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Tassa, Yuval and Erez, Tom and Smart, Bill},
  year = {2007},
  volume = {20},
  file = {/Users/scannea1/Zotero/storage/Y7BD5423/Jang et al. - 1988 - An Optimization Network for Matrix Inversion.pdf;/Users/scannea1/Zotero/storage/FKB3ZUD6/c6bff625bdb0393992c9d4db0c6bbe45-Paper.html}
}

@inproceedings{tassaSynthesis2012,
  title = {Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  year = {2012},
  month = oct,
  pages = {4906--4913},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6386025},
  abstract = {We present an online trajectory optimization method and software platform applicable to complex humanoid robots performing challenging tasks such as getting up from an arbitrary pose on the ground and recovering from large disturbances using dexterous acrobatic maneuvers. The resulting behaviors, illustrated in the attached video, are computed only 7 \texttimes{} slower than real time, on a standard PC. The video also shows results on the acrobot problem, planar swimming and one-legged hopping. These simpler problems can already be solved in real time, without pre-computing anything.},
  keywords = {Computational modeling,Heuristic algorithms,Mathematical model,Optimization,Real-time systems,Robots,Trajectory},
  file = {/Users/scannea1/Zotero/storage/R6QNBA2X/Tassa et al. - 2012 - Synthesis and stabilization of complex behaviors t.pdf;/Users/scannea1/Zotero/storage/W7EMBPV4/6386025.html}
}

@misc{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@article{theodorouGeneralized2010,
  title = {A {{Generalized Path Integral Control Approach}} to {{Reinforcement Learning}}},
  author = {Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {104},
  pages = {3137--3181},
  issn = {1533-7928},
  file = {/Users/scannea1/Zotero/storage/V9XNWFXI/Theodorou et al. - 2010 - A Generalized Path Integral Control Approach to Re.pdf;/Users/scannea1/Zotero/storage/P82WK5SN/theodorou10a.html}
}

@inproceedings{theodorouStochastic2010,
  title = {Stochastic {{Differential Dynamic Programming}}},
  booktitle = {Proceedings of the 2010 {{American Control Conference}}},
  author = {Theodorou, Evangelos and Tassa, Yuval and Todorov, Emo},
  year = {2010},
  month = jun,
  pages = {1125--1132},
  issn = {2378-5861},
  doi = {10.1109/ACC.2010.5530971},
  abstract = {Although there has been a significant amount of work in the area of stochastic optimal control theory towards the development of new algorithms, the problem of how to control a stochastic nonlinear system remains an open research topic. Recent iterative linear quadratic optimal control methods iLQG handle control and state multiplicative noise while they are derived based on first order approximation of dynamics. On the other hand, methods such as Differential Dynamic Programming expand the dynamics up to the second order but so far they can handle nonlinear systems with additive noise. In this work we present a generalization of the classic Differential Dynamic Programming algorithm. We assume the existence of state and control multiplicative process noise, and proceed to derive the second-order expansion of the cost-to-go. We find the correction terms that arise from the stochastic assumption. Despite having quartic and cubic terms in the initial expression, we show that these vanish, leaving us with the same quadratic structure as standard DDP.},
  keywords = {Control systems,Dynamic programming,Iterative algorithms,Nonlinear control systems,Nonlinear dynamical systems,Nonlinear systems,Optimal control,Stochastic processes,Stochastic resonance,Stochastic systems},
  file = {/Users/scannea1/Zotero/storage/V3X594IF/Theodorou et al. - 2010 - Stochastic Differential Dynamic Programming.pdf;/Users/scannea1/Zotero/storage/FVYIC7HX/5530971.html}
}

@inproceedings{thomasSafeReinforcementLearning2021,
  title = {Safe {{Reinforcement Learning}} by {{Imagining}} the {{Near Future}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Thomas, Garrett and Luo, Yuping and Ma, Tengyu},
  year = {2021},
  volume = {34},
  pages = {13859--13869},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states.We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Thomas et al-2021 Safe Reinforcement Learning by Imagining the Near Future/Thomas et al_2021_Safe Reinforcement Learning by Imagining the Near Future.pdf}
}

@inproceedings{titsiasBayesian2010,
  title = {Bayesian {{Gaussian Process Latent Variable Model}}},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Titsias, Michalis and Lawrence, Neil D.},
  year = {2010},
  month = mar,
  pages = {844--851},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/VU5NQZLV/Titsias and Lawrence - 2010 - Bayesian Gaussian Process Latent Variable Model.pdf}
}

@inproceedings{titsiasVariational2009,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Titsias, Michalis},
  year = {2009},
  month = apr,
  pages = {567--574},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximat...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/DLNKJ5XI/Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf;/Users/scannea1/Zotero/storage/LBI9US9A/titsias09a.html}
}

@inproceedings{todorovGeneralized2005,
  title = {A Generalized Iterative {{LQG}} Method for Locally-Optimal Feedback Control of Constrained Nonlinear Stochastic Systems},
  booktitle = {Proceedings of the 2005, {{American Control Conference}}, 2005.},
  author = {Todorov, E. and Li, Weiwei},
  year = {2005},
  month = jun,
  pages = {300-306 vol. 1},
  issn = {2378-5861},
  doi = {10.1109/ACC.2005.1469949},
  abstract = {We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu//spl sim/todorov.},
  keywords = {Control systems,Convergence,Costs,Feedback control,Iterative methods,Linear feedback control systems,Mathematical model,Nonlinear control systems,Stochastic processes,Stochastic systems},
  file = {/Users/scannea1/Zotero/storage/5YFS2PIJ/Todorov and Li - 2005 - A generalized iterative LQG method for locally-opt.pdf;/Users/scannea1/Zotero/storage/XVSZ5EIE/1469949.html}
}

@inproceedings{tosiMetrics2014,
  title = {Metrics for {{Probabilistic Geometries}}},
  booktitle = {Proceedings of the 30th {{Conference}}},
  author = {Tosi, Alessandra and Hauberg, S{\o}ren and Vellido, Alfredo and Lawrence, Neil D},
  year = {2014},
  pages = {800--808},
  abstract = {We investigate the geometrical structure of probabilistic generative dimensionality reduction models using the tools of Riemannian geometry. We explicitly define a distribution over the natural metric given by the models. We provide the necessary algorithms to compute expected metric tensors where the distribution over mappings is given by a Gaussian process. We treat the corresponding latent variable model as a Riemannian manifold and we use the expectation of the metric under the Gaussian process prior to define interpolating paths and measure distance between latent points. We show how distances that respect the expected metric lead to more appropriate generation of new data.},
  langid = {english},
  keywords = {gaussian-processes,geometric-learning,gplvm},
  file = {/Users/scannea1/Zotero/storage/U65536P3/Tosi et al. - Metrics for Probabilistic Geometries.pdf}
}

@inproceedings{toussaintProbabilistic2006,
  title = {Probabilistic Inference for Solving Discrete and Continuous State {{Markov Decision Processes}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Machine Learning}}},
  author = {Toussaint, Marc and Storkey, Amos},
  year = {2006},
  month = jan,
  volume = {2006},
  pages = {945--952},
  doi = {10.1145/1143844.1143963},
  abstract = {Inference in Markov Decision Processes has recently received interest as a means to in- fer goals of an observed action, policy recog- nition, and also as a tool to compute poli- cies. A particularly interesting aspect of the approach is that any existing inference tech- nique in DBNs now becomes available for an- swering behavioral questions-including those on continuous, factorial, or hierarchical state representations. Here we present an Expecta- tion Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbi- trary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continu- ous stochastic optimal control problems.},
  file = {/Users/scannea1/Zotero/storage/X266VYD8/Toussaint and Storkey - 2006 - Probabilistic inference for solving discrete and c.pdf}
}

@inproceedings{toussaintRobot2009,
  title = {Robot {{Trajectory Optimization}} Using {{Approximate Inference}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Toussaint, Marc},
  year = {2009},
  abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot. 1.},
  file = {/Users/scannea1/Zotero/storage/9HG93BHX/Toussaint - Robot Trajectory Optimization using Approximate In.pdf;/Users/scannea1/Zotero/storage/G6S25WB6/summary.html}
}

@inproceedings{trappDeep2020,
  title = {Deep {{Structured Mixtures}} of {{Gaussian Processes}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Trapp, Martin and Peharz, Robert and Pernkopf, Franz and Rasmussen, Carl Edward},
  year = {2020},
  month = jun,
  pages = {2251--2261},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Gaussian Processes (GPs) are powerful non-parametric Bayesian regression models that allow exact posterior inference, but exhibit high computational and memory costs. In order to improve scalability of GPs, approximate posterior inference is frequently employed, where a prominent class of approximation techniques is based on local GP experts. However, local-expert techniques proposed so far are either not well-principled, come with limited approximation guarantees, or lead to intractable models. In this paper, we introduce deep structured mixtures of GP experts, a stochastic process model which i) allows exact posterior inference, ii) has attractive computational and memory costs, and iii) when used as GP approximation, captures predictive uncertainties consistently better than previous expert-based approximations. In a variety of experiments, we show that deep structured mixtures have a low approximation error and often perform competitive or outperform prior work.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/BK6FBB7Z/Trapp et al. - 2020 - Deep Structured Mixtures of Gaussian Processes.pdf;/Users/scannea1/Zotero/storage/EKQQQYDQ/Trapp et al. - 2020 - Deep Structured Mixtures of Gaussian Processes.pdf}
}

@article{trespBayesian2000a,
  title = {A {{Bayesian Committee Machine}}},
  author = {Tresp, Volker},
  year = {2000},
  month = nov,
  journal = {Neural Computation},
  volume = {12},
  number = {11},
  pages = {2719--2741},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976600300014908},
  abstract = {The Bayesian committee machine (BCM) is a novel approach to combining estimators which were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators the main foci are Gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for online learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to Gaussian process regression. Finally, we also show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input dependent combination of estimators.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/6B6B86E3/Tresp - 2000 - A Bayesian Committee Machine.pdf}
}

@inproceedings{trespMixtures2000a,
  title = {Mixtures of {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tresp, Volker},
  year = {2000},
  volume = {13},
  pages = {654--660},
  abstract = {We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes - in particular in form of Gaussian process classification, the support vector machine and the MGP model--can be used for quantifying the dependencies in graphical models.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/VD6CC6WK/Tresp - 2000 - Mixtures of Gaussian Processes.pdf;/Users/scannea1/Zotero/storage/4XNN58JI/9fdb62f932adf55af2c0e09e55861964-Abstract.html}
}

@inproceedings{turchettaSafe2016,
  title = {Safe {{Exploration}} in {{Finite Markov Decision Processes}} with {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.},
  file = {/Users/scannea1/Zotero/storage/WUKK7N6W/Turchetta et al. - 2016 - Safe Exploration in Finite Markov Decision Process.pdf}
}

@inproceedings{ustyuzhaninovCompositional2020,
  title = {Compositional Uncertainty in Deep {{Gaussian}} Processes},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Kaiser, Markus and Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
  year = {2020},
  eprint = {1909.07698},
  eprinttype = {arxiv},
  abstract = {Gaussian processes (GPs) are nonparametric priors over functions. Fitting a GP implies computing a posterior distribution of functions consistent with the observed data. Similarly, deep Gaussian processes (DGPs) should allow us to compute a posterior distribution of compositions of multiple functions giving rise to the observations. However, exact Bayesian inference is intractable for DGPs, motivating the use of various approximations. We show that the application of simplifying mean-field assumptions across the hierarchy leads to the layers of a DGP collapsing to near-deterministic transformations. We argue that such an inference scheme is suboptimal, not taking advantage of the potential of the model to discover the compositional structure in the data. To address this issue, we examine alternative variational inference schemes allowing for dependencies across different layers and discuss their advantages and limitations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,deep-gaussian-processes,gaussian-process,Statistics - Machine Learning,variational-inference},
  file = {/Users/scannea1/Zotero/storage/6RMC67MW/Ustyuzhaninov et al. - 2020 - Compositional uncertainty in deep Gaussian process.pdf;/Users/scannea1/Zotero/storage/XHN32FUW/1909.html}
}

@inproceedings{ustyuzhaninovMonotonic2020,
  title = {Monotonic {{Gaussian Process Flow}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill D. F.},
  year = {2020},
  volume = {23},
  eprint = {1905.12930},
  eprinttype = {arxiv},
  abstract = {We propose a new framework for imposing monotonicity constraints in a Bayesian nonparametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric model of monotonic functions that allows for interpretable priors and principled quantification of hierarchical uncertainty. We demonstrate the efficacy of the proposed model by providing competitive results to other probabilistic monotonic models on a number of benchmark functions. In addition, we consider the utility of a monotonic random process as a part of a hierarchical probabilistic model; we examine the task of temporal alignment of time-series data where it is beneficial to use a monotonic random process in order to preserve the uncertainty in the temporal warpings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/YF8MS6ER/Ustyuzhaninov et al. - 2020 - Monotonic Gaussian Process Flow.pdf;/Users/scannea1/Zotero/storage/ZSC3RQJ7/1905.html}
}

@article{vanderwilkFramework2020,
  title = {A {{Framework}} for {{Interdomain}} and {{Multioutput Gaussian Processes}}},
  author = {{van der Wilk}, Mark and Dutordoir, Vincent and John, S. T. and Artemev, Artem and Adam, Vincent and Hensman, James},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.01115 [cs, stat]},
  eprint = {2003.01115},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {One obstacle to the use of Gaussian processes (GPs) in large-scale problems, and as a component in deep learning system, is the need for bespoke derivations and implementations for small variations in the model or inference. In order to improve the utility of GPs we need a modular system that allows rapid implementation and testing, as seen in the neural network community. We present a mathematical and software framework for scalable approximate inference in GPs, which combines interdomain approximations and multiple outputs. Our framework, implemented in GPflow, provides a unified interface for many existing multioutput models, as well as more recent convolutional structures. This simplifies the creation of deep models with GPs, and we hope that this work will encourage more interest in this approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/C89VTA94/van der Wilk et al. - 2020 - A Framework for Interdomain and Multioutput Gaussi.pdf;/Users/scannea1/Zotero/storage/EMGDFEFE/2003.html}
}

@article{vasudevanGaussian2009a,
  ids = {vasudevanGaussian2009},
  title = {Gaussian Process Modeling of Large-Scale Terrain},
  author = {Vasudevan, Shrihari and Ramos, Fabio and Nettleton, Eric and {Durrant-Whyte}, Hugh},
  year = {2009},
  journal = {Journal of Field Robotics},
  volume = {26},
  number = {10},
  pages = {812--840},
  issn = {1556-4967},
  doi = {10.1002/rob.20309},
  abstract = {Building a model of large-scale terrain that can adequately handle uncertainty and incompleteness in a statistically sound way is a challenging problem. This work proposes the use of Gaussian processes as models of large-scale terrain. The proposed model naturally provides a multiresolution representation of space, incorporates and handles uncertainties aptly, and copes with incompleteness of sensory information. Gaussian process regression techniques are applied to estimate and interpolate (to fill gaps in occluded areas) elevation information across the field. The estimates obtained are the best linear unbiased estimates for the data under consideration. A single nonstationary (neural network) Gaussian process is shown to be powerful enough to model large and complex terrain, effectively handling issues relating to discontinuous data. A local approximation method based on a ``moving window'' methodology and implemented using k-dimensional (KD)-trees is also proposed. This enables the approach to handle extremely large data sets, thereby completely addressing its scalability issues. Experiments are performed on large-scale data sets taken from real mining applications. These data sets include sparse mine planning data, which are representative of a global positioning system\textendash based survey, as well as dense laser scanner data taken at different mine sites. Further, extensive statistical performance evaluation and benchmarking of the technique has been performed through cross-validation experiments. They conclude that for dense and/or flat data, the proposed approach will perform very competitively with grid-based approaches using standard interpolation techniques and triangulated irregular networks using triangle-based interpolation techniques; for sparse and/or complex data, however, it would significantly outperform them. \textcopyright{} 2009 Wiley Periodicals, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20309},
  file = {/Users/scannea1/Zotero/storage/95WPXWM3/Vasudevan et al. - 2009 - Gaussian Process Modeling of Large-Scale Terrain.pdf;/Users/scannea1/Zotero/storage/VXPH4VRG/download.html;/Users/scannea1/Zotero/storage/ZNZ2R2RM/rob.html}
}

@article{vinogradskaNumerical2020,
  title = {Numerical {{Quadrature}} for {{Probabilistic Policy Search}}},
  author = {Vinogradska, Julia and Bischoff, Bastian and Achterhold, Jan and Koller, Torsten and Peters, Jan},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {42},
  number = {1},
  pages = {164--175},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2879335},
  abstract = {Learning control policies has become an appealing alternative to the derivation of control laws based on classic control theory. Model-based approaches have proven an outstanding data efficiency, especially when combined with probabilistic models to eliminate model bias. However, a major difficulty for these methods is that multi-step-ahead predictions typically become intractable for larger planning horizons and can only poorly be approximated. In this paper, we propose the use of numerical quadrature to overcome this drawback and provide significantly more accurate multi-step-ahead predictions. As a result, our approach increases data efficiency and enhances the quality of learned policies. Furthermore, policy learning is not restricted to optimizing locally around one trajectory, as numerical quadrature provides a principled approach to extend optimization to all trajectories starting in a specified starting state region. Thus, manual effort, such as choosing informative starting points for simultaneous policy optimization, is significantly decreased. Furthermore, learning is highly robust to the choice of initial policy and, thus, interaction time with the system is minimized. Empirical evaluations on simulated benchmark problems show the efficiency of the proposed approach and support our theoretical results.},
  keywords = {Computational modeling,control,Data models,Gaussian processes,Numerical models,Policy search,Predictive models,reinforcement learning,System dynamics,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/32JH2IZA/Vinogradska et al. - 2020 - Numerical Quadrature for Probabilistic Policy Sear.pdf;/Users/scannea1/Zotero/storage/K82ZSX66/8520758.html}
}

@inproceedings{vinogradskaStability2016,
  title = {Stability of {{Controllers}} for {{Gaussian Process Forward Models}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Vinogradska, Julia and Bischoff, Bastian and {Nguyen-Tuong}, Duy and Romer, Anne and Schmidt, Henner and Peters, Jan},
  year = {2016},
  month = jun,
  pages = {545--554},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance g...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/ZXXMUJ9K/Vinogradska et al. - 2016 - Stability of Controllers for Gaussian Process Forw.pdf;/Users/scannea1/Zotero/storage/685IQFF8/vinogradska16.html}
}

@article{vonstrykDirect1992,
  title = {Direct and {{Indirect Methods}} for {{Trajectory Optimization}}},
  author = {Von Stryk, Oskar and Bulirsch, Roland},
  year = {1992},
  month = dec,
  journal = {Annals of Operations Research},
  volume = {37},
  pages = {357--373},
  doi = {10.1007/BF02071065},
  abstract = {This paper gives a brief list of commonly used direct and indirect efficient methods for the numerical solution of optimal control problems. To improve the low accuracy of the direct methods and to increase the convergence areas of the indirect methods we suggest a hybrid approach. For this a special direct collocation method is presented. In a hybrid approach this direct method can be used in combination with multiple shooting. Numerical examples illustrate the direct method and the hybrid approach.},
  file = {/Users/scannea1/Zotero/storage/D99SU5KA/Von Stryk and Bulirsch - 1992 - Direct and Indirect Methods for Trajectory Optimiz.pdf}
}

@article{wachiSafeExplorationOptimization2018,
  title = {Safe {{Exploration}} and {{Optimization}} of {{Constrained MDPs Using Gaussian Processes}}},
  author = {Wachi, Akifumi and Sui, Yanan and Yue, Yisong and Ono, Masahiro},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.12103},
  abstract = {We present a reinforcement learning approach to explore and optimize a safety-constrained Markov Decision Process(MDP). In this setting, the agent must maximize discounted cumulative reward while constraining the probability of entering unsafe states, defined using a safety function being within some tolerance. The safety values of all states are not known a priori, and we probabilistically model them via aGaussian Process (GP) prior. As such, properly behaving in such an environment requires balancing a three-way trade-off of exploring the safety function, exploring the reward function, and exploiting acquired knowledge to maximize reward. We propose a novel approach to balance this trade-off. Specifically, our approach explores unvisited states selectively; that is, it prioritizes the exploration of a state if visiting that state significantly improves the knowledge on the achievable cumulative reward. Our approach relies on a novel information gain criterion based on Gaussian Process representations of the reward and safety functions. We demonstrate the effectiveness of our approach on a range of experiments, including a simulation using the real Martian terrain data.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Gaussian Processes,Markov Decision Process},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Wachi et al-2018 Safe Exploration and Optimization of Constrained MDPs Using Gaussian Processes/Wachi et al_2018_Safe Exploration and Optimization of Constrained MDPs Using Gaussian Processes.pdf}
}

@inproceedings{wanEvaluating2022,
  title = {Towards {{Evaluating Adaptivity}} of {{Model-Based Reinforcement Learning Methods}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wan, Yi and {Rahimi-Kalahroudi}, Ali and Rajendran, Janarthanan and Momennejad, Ida and Chandar, Sarath and Seijen, Harm H. Van},
  year = {2022},
  month = jun,
  pages = {22536--22561},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In recent years, a growing number of deep model-based reinforcement learning (RL) methods have been introduced. The interest in deep model-based RL is not surprising, given its many potential benefits, such as higher sample efficiency and the potential for fast adaption to changes in the environment. However, we demonstrate, using an improved version of the recently introduced Local Change Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and DreamerV2 perform poorly in their ability to adapt to local environmental changes. Combined with prior work that made a similar observation about the other popular model-based method, MuZero, a trend appears to emerge, suggesting that current deep model-based methods have serious limitations. We dive deeper into the causes of this poor performance, by identifying elements that hurt adaptive behavior and linking these to underlying techniques frequently used in deep model-based RL. We empirically validate these insights in the case of linear function approximation by demonstrating that a modified version of linear Dyna achieves effective adaptation to local changes. Furthermore, we provide detailed insights into the challenges of building an adaptive nonlinear model-based method, by experimenting with a nonlinear version of Dyna.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/MPAG7B3V/Wan et al. - 2022 - Towards Evaluating Adaptivity of Model-Based Reinf.pdf}
}

@inproceedings{wangSafe2018,
  title = {Safe {{Learning}} of {{Quadrotor Dynamics Using Barrier Certificates}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Wang, Li and Theodorou, Evangelos A. and Egerstedt, Magnus},
  year = {2018},
  month = may,
  pages = {2460--2465},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460471},
  abstract = {To effectively control complex dynamical systems, accurate nonlinear models are typically needed. However, these models are not always known. In this paper, we present a data-driven approach based on Gaussian processes that learns models of quadrotors operating in partially unknown environments. What makes this challenging is that if the learning process is not carefully controlled, the system will go unstable, i.e., the quadcopter will crash. To this end, barrier certificates are employed for safe learning. The barrier certificates establish a non-conservative forward invariant safe region, in which high probability safety guarantees are provided based on the statistics of the Gaussian Process. A learning controller is designed to efficiently explore those uncertain states and expand the barrier certified safe region based on an adaptive sampling scheme. Simulation results are provided to demonstrate the effectiveness of the proposed approach.},
  keywords = {Adaptation models,Computational modeling,Control systems,Gaussian processes,Lyapunov methods,Safety,System dynamics},
  file = {/Users/scannea1/Zotero/storage/SA29II8X/Wang et al. - 2018 - Safe Learning of Quadrotor Dynamics Using Barrier .pdf;/Users/scannea1/Zotero/storage/UC3UH27R/8460471.html}
}

@inproceedings{watsonAdvancing2021,
  title = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}: {{Exploration}}, {{Covariance Control}} and {{Adaptive Risk}}},
  shorttitle = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}},
  booktitle = {American {{Control Conference}} ({{ACC}})},
  author = {Watson, Joe and Peters, Jan},
  year = {2021},
  eprint = {2103.06319},
  eprinttype = {arxiv},
  abstract = {Discrete-time stochastic optimal control remains a challenging problem for general, nonlinear systems under significant uncertainty, with practical solvers typically relying on the certainty equivalence assumption, replanning and/or extensive regularization. Control as inference is an approach that frames stochastic control as an equivalent inference problem, and has demonstrated desirable qualities over existing methods, namely in exploration and regularization. We look specifically at the input inference for control (i2c) algorithm, and derive three key characteristics that enable advanced trajectory optimization: An `expert' linear Gaussian controller that combines the benefits of open-loop optima and closed-loop variance reduction when optimizing for nonlinear systems, inherent adaptive risk sensitivity from the inference formulation, and covariance control functionality with only a minor algorithmic adjustment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/WLS2UV6J/Watson and Peters - 2021 - Advancing Trajectory Optimization with Approximate.pdf;/Users/scannea1/Zotero/storage/HBV63NMC/2103.html}
}

@inproceedings{watsonStochastic2020,
  title = {Stochastic {{Optimal Control}} as {{Approximate Input Inference}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Watson, Joe and Abdulsamad, Hany and Peters, Jan},
  year = {2020},
  month = may,
  pages = {697--716},
  publisher = {{PMLR}},
  issn = {2640-3498},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/AQK5B45X/Watson et al. - 2020 - Stochastic Optimal Control as Approximate Input In.pdf}
}

@article{watsonStochastic2021,
  title = {Stochastic {{Control}} through {{Approximate Bayesian Input Inference}}},
  author = {Watson, Joe and Abdulsamad, Hany and Findeisen, Rolf and Peters, Jan},
  year = {2021},
  month = may,
  journal = {arXiv:2105.07693 [cs, eess]},
  eprint = {2105.07693},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Optimal control under uncertainty is a prevailing challenge in control, due to the difficulty in producing tractable solutions for the stochastic optimization problem. By framing the control problem as one of input estimation, advanced approximate inference techniques can be used to handle the statistical approximations in a principled and practical manner. Analyzing the Gaussian setting, we present a solver capable of several stochastic control methods, and was found to be superior to popular baselines on nonlinear simulated tasks. We draw connections that relate this inference formulation to previous approaches for stochastic optimal control, and outline several advantages that this inference view brings due to its statistical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/HSJRT7F9/Watson et al. - 2021 - Stochastic Control through Approximate Bayesian In.pdf;/Users/scannea1/Zotero/storage/ATILHJPE/2105.html}
}

@inproceedings{wilkinsonSparse2021,
  title = {Sparse {{Algorithms}} for {{Markovian Gaussian Processes}}},
  booktitle = {{{AISTATS}}},
  author = {Wilkinson, William J. and Solin, A. and Adam, Vincent},
  year = {2021},
  abstract = {Approximate Bayesian inference methods that scale to very large datasets are crucial in leveraging probabilistic models for real-world time series. Sparse Markovian Gaussian processes combine the use of inducing variables with efficient Kalman filter-like recursions, resulting in algorithms whose computational and memory requirements scale linearly in the number of inducing points, whilst also enabling parallel parameter updates and stochastic optimisation. Under this paradigm, we derive a general site-based approach to approximate inference, whereby we approximate the non-Gaussian likelihood with local Gaussian terms, called sites. Our approach results in a suite of novel sparse extensions to algorithms from both the machine learning and signal processing literature, including variational inference, expectation propagation, and the classical nonlinear Kalman smoothers. The derived methods are suited to large time series, and we also demonstrate their applicability to spatio-temporal data, where the model has separate inducing points in both time and space.},
  file = {/Users/scannea1/Zotero/storage/VJTCCV38/Wilkinson et al. - 2021 - Sparse Algorithms for Markovian Gaussian Processes.pdf}
}

@misc{williamsAdvancing,
  title = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}: {{Exploration}}, {{Covariance Control}} and {{Adaptive Risk}}},
  shorttitle = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}},
  author = {Williams, Jon},
  journal = {Max Planck Institute for Intelligent Systems},
  abstract = {Our goal is to understand the principles of Perception, Action and Learning in autonomous systems that successfully interact with complex environments and to use this understanding to design future systems.},
  howpublished = {https://is.mpg.de},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/RT8WP95Y/watpet21.html}
}

@inproceedings{williamsInformation2017,
  ids = {williamsInformation2017a},
  title = {Information Theoretic {{MPC}} for Model-Based Reinforcement Learning},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M. and Boots, Byron and Theodorou, Evangelos A.},
  year = {2017},
  month = may,
  pages = {1714--1721},
  doi = {10.1109/ICRA.2017.7989202},
  abstract = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
  keywords = {Cost function,Heuristic algorithms,Learning (artificial intelligence),Optimal control,Robots,Trajectory},
  file = {/Users/scannea1/Zotero/storage/4T95X4BV/Williams et al. - 2017 - Information theoretic MPC for model-based reinforc.pdf;/Users/scannea1/Zotero/storage/H3WL5XS4/Williams et al. - 2017 - Information theoretic MPC for model-based reinforc.pdf;/Users/scannea1/Zotero/storage/DGP8VC62/7989202.html;/Users/scannea1/Zotero/storage/GYW9MRB7/7989202.html}
}

@article{williamsInformationTheoretic2018,
  title = {Information-{{Theoretic Model Predictive Control}}: {{Theory}} and {{Applications}} to {{Autonomous Driving}}},
  shorttitle = {Information-{{Theoretic Model Predictive Control}}},
  author = {Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.},
  year = {2018},
  month = dec,
  journal = {IEEE Transactions on Robotics},
  volume = {34},
  number = {6},
  pages = {1603--1622},
  issn = {1941-0468},
  doi = {10.1109/TRO.2018.2865891},
  abstract = {We present an information-theoretic approach to stochastic optimal control problems that can be used to derive general sampling-based optimization schemes. This new mathematical method is used to develop a sampling-based model predictive control algorithm. We apply this information-theoretic model predictive control scheme to the task of aggressive autonomous driving around a dirt test track, and compare its performance with a model predictive control version of the cross-entropy method.},
  keywords = {Autonomous vehicles,Monte Carlo methods,Monte-Carlo methods,nonlinear control systems,Nonlinear control systems,optimal control,Optimal control,parallel algorithms,Parallel processing,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/CNPYIW9A/Williams et al. - 2018 - Information-Theoretic Model Predictive Control Th.pdf;/Users/scannea1/Zotero/storage/HWUPYAJG/8558663.html}
}

@article{williamsModel2017,
  title = {Model {{Predictive Path Integral Control}}: {{From Theory}} to {{Parallel Computation}}},
  shorttitle = {Model {{Predictive Path Integral Control}}},
  author = {Williams, Grady and Aldrich, Andrew and Theodorou, Evangelos A.},
  year = {2017},
  journal = {Journal of Guidance, Control, and Dynamics},
  volume = {40},
  number = {2},
  pages = {344--357},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/1.G001921},
  abstract = {In this paper, a model predictive path integral control algorithm based on a generalized importance sampling scheme is developed and parallel optimization via sampling is performed using a graphics processing unit. The proposed generalized importance sampling scheme allows for changes in the drift and diffusion terms of stochastic diffusion processes and plays a significant role in the performance of the model predictive control algorithm. The proposed algorithm is compared in simulation with a model predictive control version of differential dynamic programming on nonlinear systems. Finally, the proposed algorithm is applied on multiple vehicles for the task of navigating through a cluttered environment. The current simulations illustrate the efficiency and robustness of the proposed approach and demonstrate the advantages of computational frameworks that incorporate concepts from statistical physics, control theory, and parallelization against more traditional approaches of optimal control theory.},
  annotation = {\_eprint: https://doi.org/10.2514/1.G001921},
  file = {/Users/scannea1/Zotero/storage/UADARRDZ/Williams et al. - 2017 - Model Predictive Path Integral Control From Theor.pdf;/Users/scannea1/Zotero/storage/9C2RDKAE/1.html}
}

@misc{wilsonBayesian2022,
  title = {Bayesian {{Deep Learning}} and a {{Probabilistic Perspective}} of {{Generalization}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  year = {2022},
  month = mar,
  number = {arXiv:2002.08791},
  eprint = {2002.08791},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/L5NKD3KU/Wilson and Izmailov - 2022 - Bayesian Deep Learning and a Probabilistic Perspec.pdf;/Users/scannea1/Zotero/storage/CRZ3DE6F/2002.html}
}

@inproceedings{wilsonEfficiently2020,
  title = {Efficiently {{Sampling Functions}} from {{Gaussian Process Posteriors}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2020},
  volume = {37},
  eprint = {2002.09309},
  eprinttype = {arxiv},
  abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/LDX7GNL2/Wilson et al. - 2020 - Efficiently Sampling Functions from Gaussian Proce.pdf;/Users/scannea1/Zotero/storage/WCQHCJV4/2002.html}
}

@article{wilsonPathwise2021,
  ids = {wilsonPathwise},
  title = {Pathwise {{Conditioning}} of {{Gaussian Processes}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {105},
  pages = {1--47},
  abstract = {As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning.},
  file = {/Users/scannea1/Zotero/storage/EQYCWMQY/Wilson et al. - 2021 - Pathwise Conditioning of Gaussian Processes.pdf}
}

@article{Woodcock2009FormalMP,
  title = {Formal Methods: {{Practice}} and Experience},
  author = {Woodcock, Jim and Larsen, Peter Gorm and Bicarregui, Juan and Fitzgerald, John S.},
  year = {2009},
  journal = {Acm Computing Surveys},
  volume = {41},
  pages = {19:1-19:36}
}

@misc{yeMastering2021,
  title = {Mastering {{Atari Games}} with {{Limited Data}}},
  author = {Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  year = {2021},
  month = dec,
  number = {arXiv:2111.00210},
  eprint = {2111.00210},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3\% mean human performance and 109.0\% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/VUXEVCAX/Ye et al. - 2021 - Mastering Atari Games with Limited Data.pdf;/Users/scannea1/Zotero/storage/H8KF25TH/2111.html}
}

@inproceedings{yildizContinuoustime2021,
  title = {Continuous-Time {{Model-based Reinforcement Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yildiz, Cagatay and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri},
  year = {2021},
  month = jul,
  pages = {12009--12018},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, and can solve classic control problems in a sample-efficient manner.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/A3KC3JAW/Yildiz et al. - 2021 - Continuous-time Model-based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/M9J5CRBP/Yildiz et al. - 2021 - Continuous-time Model-based Reinforcement Learning.pdf}
}

@inproceedings{Yong1999StochasticCH,
  title = {Stochastic Controls: {{Hamiltonian}} Systems and {{HJB}} Equations},
  author = {Yong, Jiongmin and Zhou, Xun Yu},
  year = {1999}
}

@article{yongStochastic1999,
  title = {Stochastic {{Controls}}: {{Hamiltonian Systems}} and {{HJB Equations}}},
  shorttitle = {Stochastic {{Controls}}},
  author = {Yong, J. and Zhou, X.},
  year = {1999},
  journal = {undefined},
  abstract = {1. Basic Stochastic Calculus.- 1. Probability.- 1.1. Probability spaces.- 1.2. Random variables.- 1.3. Conditional expectation.- 1.4. Convergence of probabilities.- 2. Stochastic Processes.- 2.1. General considerations.- 2.2. Brownian motions.- 3. Stopping Times.- 4. Martingales.- 5. Ito\&\#39;s Integral.- 5.1. Nondifferentiability of Brownian motion.- 5.2. Definition of Ito\&\#39;s integral and basic properties.- 5.3. Ito\&\#39;s formula.- 5.4. Martingale representation theorems.- 6. Stochastic Differential Equations.- 6.1. Strong solutions.- 6.2. Weak solutions.- 6.3. Linear SDEs.- 6.4. Other types of SDEs.- 2. Stochastic Optimal Control Problems.- 1. Introduction.- 2. Deterministic Cases Revisited.- 3. Examples of Stochastic Control Problems.- 3.1. Production planning.- 3.2. Investment vs. consumption.- 3.3. Reinsurance and dividend management.- 3.4. Technology diffusion.- 3.5. Queueing systems in heavy traffic.- 4. Formulations of Stochastic Optimal Control Problems.- 4.1. Strong formulation.- 4.2. Weak formulation.- 5. Existence of Optimal Controls.- 5.1. A deterministic result.- 5.2. Existence under strong formulation.- 5.3. Existence under weak formulation.- 6. Reachable Sets of Stochastic Control Systems.- 6.1. Nonconvexity of the reachable sets.- 6.2. Noncloseness of the reachable sets.- 7. Other Stochastic Control Models.- 7.1. Random duration.- 7.2. Optimal stopping.- 7.3. Singular and impulse controls.- 7.4. Risk-sensitive controls.- 7.5. Ergodic controls.- 7.6. Partially observable systems.- 8. Historical Remarks.- 3. Maximum Principle and Stochastic Hamiltonian Systems.- 1. Introduction.- 2. The Deterministic Case Revisited.- 3. Statement of the Stochastic Maximum Principle.- 3.1. Adjoint equations.- 3.2. The maximum principle and stochastic Hamiltonian systems.- 3.3. A worked-out example.- 4. A Proof of the Maximum Principle.- 4.1. A moment estimate.- 4.2. Taylor expansions.- 4.3. Duality analysis and completion of the proof.- 5. Sufficient Conditions of Optimality.- 6. Problems with State Constraints.- 6.1. Formulation of the problem and the maximum principle.- 6.2. Some preliminary lemmas.- 6.3. A proof of Theorem 6.1.- 7. Historical Remarks.- 4. Dynamic Programming and HJB Equations.- 1. Introduction.- 2. The Deterministic Case Revisited.- 3. The Stochastic Principle of Optimality and the HJB Equation.- 3.1. A stochastic framework for dynamic programming.- 3.2. Principle of optimality.- 3.3. The HJB equation.- 4. Other Properties of the Value Function.- 4.1. Continuous dependence on parameters.- 4.2. Semiconcavity.- 5. Viscosity Solutions.- 5.1. Definitions.- 5.2. Some properties.- 6. Uniqueness of Viscosity Solutions.- 6.1. A uniqueness theorem.- 6.2. Proofs of Lemmas 6.6 and 6.7.- 7. Historical Remarks.- 5. The Relationship Between the Maximum Principle and Dynamic Programming.- 1. Introduction.- 2. Classical Hamilton-Jacobi Theory.- 3. Relationship for Deterministic Systems.- 3.1. Adjoint variable and value function: Smooth case.- 3.2. Economic interpretation.- 3.3. Methods of characteristics and the Feynman-Kac formula.- 3.4. Adjoint variable and value function: Nonsmooth case.- 3.5. Verification theorems.- 4. Relationship for Stochastic Systems.- 4.1. Smooth case.- 4.2. Nonsmooth case: Differentials in the spatial variable.- 4.3. Nonsmooth case: Differentials in the time variable.- 5. Stochastic Verification Theorems.- 5.1. Smooth case.- 5.2. Nonsmooth case.- 6. Optimal Feedback Controls.- 7. Historical Remarks.- 6. Linear Quadratic Optimal Control Problems.- 1. Introduction.- 2. The Deterministic LQ Problems Revisited.- 2.1. Formulation.- 2.2. A minimization problem of a quadratic functional.- 2.3. A linear Hamiltonian system.- 2.4. The Riccati equation and feedback optimal control.- 3. Formulation of Stochastic LQ Problems.- 3.1. Statement of the problems.- 3.2. Examples.- 4. Finiteness and Solvability.- 5. A Necessary Condition and a Hamiltonian System.- 6. Stochastic Riccati Equations.- 7. Global Solvability of Stochastic Riccati Equations.- 7.1. Existence: The standard case.- 7.2. Existence: The case C = 0, S = 0, and Q, G ?0.- 7.3. Existence: The one-dimensional case.- 8. A Mean-variance Portfolio Selection Problem.- 9. Historical Remarks.- 7. Backward Stochastic Differential Equations.- 1. Introduction.- 2. Linear Backward Stochastic Differential Equations.- 3. Nonlinear Backward Stochastic Differential Equations.- 3.1. BSDEs in finite deterministic durations: Method of contraction mapping.- 3.2. BSDEs in random durations: Method of continuation.- 4. Feynman-Kac-Type Formulae.- 4.1. Representation via SDEs.- 4.2. Representation via BSDEs.- 5. Forward-Backward Stochastic Differential Equations.- 5.1. General formulation and nonsolvability.- 5.2. The four-step scheme, a heuristic derivation.- 5.3. Several solvable classes of FBSDEs.- 6. Option Pricing Problems.- 6.1. European call options and the Black--Scholes formula.- 6.2. Other options.- 7. Historical Remarks.- References.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JC3ZG78J/f60223dee8815324a4b9df9d1825e7c33fc099f3.html}
}

@article{yuActive2021,
  title = {Active {{Learning}} in {{Gaussian Process State Space Model}}},
  author = {Yu, Hon Sum Alec and Yao, Dingling and Zimmer, Christoph and Toussaint, Marc and {Nguyen-Tuong}, Duy},
  year = {2021},
  month = jul,
  journal = {arXiv:2108.00819 [cs, stat]},
  eprint = {2108.00819},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We investigate active learning in Gaussian Process state-space models (GPSSM). Our problem is to actively steer the system through latent states by determining its inputs such that the underlying dynamics can be optimally learned by a GPSSM. In order that the most informative inputs are selected, we employ mutual information as our active learning criterion. In particular, we present two approaches for the approximation of mutual information for the GPSSM given latent states. The proposed approaches are evaluated in several physical systems where we actively learn the underlying non-linear dynamics represented by the state-space model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/GQRXJZJA/Yu et al. - 2021 - Active Learning in Gaussian Process State Space Mo.pdf;/Users/scannea1/Zotero/storage/XS273K4X/2108.html}
}

@inproceedings{yuImplicit2019,
  title = {Implicit {{Posterior Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {YU, Haibin and Chen, Yizhou and Low, Bryan Kian Hsiang and Jaillet, Patrick and Dai, Zhongxiang},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Zotero/storage/W7EQKCL5/YU et al. - 2019 - Implicit Posterior Variational Inference for Deep .pdf}
}

@article{yukselTwenty2012,
  title = {Twenty {{Years}} of {{Mixture}} of {{Experts}}},
  author = {Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
  year = {2012},
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {23},
  number = {8},
  pages = {1177--1193},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2012.2200299},
  abstract = {In this paper, we provide a comprehensive survey of the mixture of experts (ME). We discuss the fundamental models for regression and classification and also their training with the expectation-maximization algorithm. We follow the discussion with improvements to the ME model and focus particularly on the mixtures of Gaussian process experts. We provide a review of the literature for other training methods, such as the alternative localized ME training, and cover the variational learning of ME in detail. In addition, we describe the model selection literature which encompasses finding the optimum number of experts, as well as the depth of the tree. We present the advances in ME in the classification area and present some issues concerning the classification model. We list the statistical properties of ME, discuss how the model has been modified over the years, compare ME to some popular algorithms, and list several applications. We conclude our survey with future directions and provide a list of publicly available datasets and a list of publicly available software that implement ME. Finally, we provide examples for regression and classification. We believe that the study described in this paper will provide quick access to the relevant literature for researchers and practitioners who would like to improve or use ME, and that it will stimulate further studies in ME.},
  keywords = {Applications,Bayesian,Bayesian methods,classification,comparison,Data models,Decision trees,Gaussian processes,Hidden Markov models,hierarchical mixture of experts (HME),mixture of Gaussian process experts,regression,Regression analysis,statistical properties,Support vector machines,survey,variational},
  file = {/Users/scannea1/Zotero/storage/DBIQM3DW/Yuksel et al. - 2012 - Twenty Years of Mixture of Experts.pdf}
}

@inproceedings{yuMOPOModelbasedOffline2020,
  title = {{{MOPO}}: {{Model-based Offline Policy Optimization}}},
  shorttitle = {{{MOPO}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  year = {2020},
  volume = {33},
  pages = {14129--14142},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy. While there has been significant progress in model-free offline RL, the most successful prior methods constrain the policy to the support of the data, precluding generalization to new states. In this paper, we observe that an existing model-based RL algorithm on its own already produces significant gains in the offline setting, as compared to model-free approaches, despite not being designed for this setting. However, although many standard model-based RL methods already estimate the uncertainty of their model, they do not by themselves provide a mechanism to avoid the issues associated with distributional shift in the offline setting. We therefore propose to modify existing model-based RL methods to address these issues by casting offline model-based RL into a penalized MDP framework. We theoretically show that, by using this penalized MDP, we are maximizing a lower bound of the return in the true MDP. Based on our theoretical results, we propose a new model-based offline RL algorithm that applies the variance of a Lipschitz-regularized model as a penalty to the reward function. We find that this algorithm outperforms both standard model-based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as two challenging continuous control tasks that require generalizing from data collected for a different task.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yu et al-2020 MOPO/Yu et al_2020_MOPO.pdf}
}

@inproceedings{zhangNoisy2018,
  title = {Noisy {{Natural Gradient}} as {{Variational Inference}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
  year = {2018},
  month = jul,
  pages = {5852--5861},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.~fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/MX87JMRW/Zhang et al. - 2018 - Noisy Natural Gradient as Variational Inference.pdf;/Users/scannea1/Zotero/storage/XWEARQZV/Zhang et al. - 2018 - Noisy Natural Gradient as Variational Inference.pdf}
}

@inproceedings{zhouVector2014,
  title = {Vector Field Following for Quadrotors Using Differential Flatness},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhou, Dingjiang and Schwager, Mac},
  year = {2014},
  month = may,
  pages = {6567--6572},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907828},
  abstract = {This paper proposes a differential flatness-based method for maneuvering a quadrotor so that its position follows a specified velocity vector field. Existing planning and control algorithms often give a 2D or 3D velocity vector field to be followed by a robot. However, quadrotors have complex nonlinear dynamics that make vector field following difficult, especially in aggressive maneuvering regimes. This paper exploits the differential flatness property of a quadrotor's dynamics to control its position along a given vector field. Differential flatness allows for the analytical derivation of control inputs in order to control the 12D dynamical state of the quadrotor such that the 2D or 3D position of the quadrotor follows the flow specified by a given vector field. The method is derived mathematically, and demonstrated in numerical simulations and in experiments with a quadrotor robot for three different vector fields.},
  keywords = {Collision avoidance,Heuristic algorithms,Navigation,Robots,Spirals,Trajectory,Vectors},
  file = {/Users/scannea1/Zotero/storage/83EI6RHU/Zhou and Schwager - 2014 - Vector field following for quadrotors using differ.pdf;/Users/scannea1/Zotero/storage/YPFEFDI6/6907828.html}
}

@phdthesis{ziebartModeling2010,
  title = {Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
  author = {Ziebart, Brian D.},
  year = {2010},
  abstract = {Predicting human behavior from a small amount of training examples is a challenging machine learning problem. In this thesis, we introduce the principle of maximum causal entropy, a general technique for applying information theory to decision-theoretic, game-theoretic, and control settings where relevant information is sequentially revealed over time. This approach guarantees decision-theoretic performance by matching purposeful measures of behavior (Abbeel \& Ng, 2004), and/or enforces game-theoretic rationality constraints (Aumann, 1974), while otherwise being as uncertain as possible, which minimizes worst-case predictive log-loss (Grunwald \& Dawid, 2003).  We derive probabilistic models for decision, control, and multi-player game settings using this approach. We then develop corresponding algorithms for efficient inference that include relaxations of the Bellman equation (Bellman, 1957), and simple learning algorithms based on convex optimization. We apply the models and algorithms to a number of behavior prediction tasks. Specifically, we present empirical evaluations of the approach in the domains of vehicle route preference modeling using over 100,000 miles of collected taxi driving data, pedestrian motion modeling from weeks of indoor movement data, and robust prediction of game play in stochastic multi-player games.},
  file = {/Users/scannea1/Zotero/storage/IS248UI6/Ziebart - 2010 - Modeling purposeful adaptive behavior with the pri.pdf}
}
