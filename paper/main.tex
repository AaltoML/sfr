% !TeX spellcheck = en_GB
\documentclass{article}

% Pass options to natbib
\PassOptionsToPackage{numbers, compress}{natbib}

% NeurIPS packages
\usepackage[]{neurips_2023}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Redefine paragraph to be tighter
\renewcommand{\paragraph}[1]{{\bf #1}~~}

% Array/table packages
\usepackage{tabularx}
\usepackage{array,multirow}
\usepackage{colortbl}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newlength{\tblw}

% Latin
\usepackage{xspace}
\newcommand{\eg}{\textit{e.g.\@}\xspace}
\newcommand{\ie}{\textit{i.e.\@}\xspace}
\newcommand{\cf}{\textit{cf.\@}\xspace}
\newcommand{\etc}{\textit{etc.\@}\xspace}
\newcommand{\etal}{\textit{et~al.\@}\xspace}

% Our method
\newcommand{\our}{\textsc{sfr}\xspace}

% Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations,backgrounds,arrows.meta,calc}
\usetikzlibrary{shapes,arrows,positioning}

% Appendix/supplement title
\newcommand{\nipstitle}[1]{{%
    % rules for title box at top and bottom
    \def\toptitlebar{\hrule height4pt \vskip .25in \vskip -\parskip} 
    \def\bottomtitlebar{\vskip .29in \vskip -\parskip \hrule height1pt \vskip .09in} 
    \phantomsection\hsize\textwidth\linewidth\hsize%
    \vskip 0.1in%
    \toptitlebar%
    \begin{minipage}{\textwidth}%
        \centering{\LARGE\bf #1\par}%
    \end{minipage}%
    \bottomtitlebar%
    \addcontentsline{toc}{section}{#1}%
}}

% Bibliography
%\usepackage[maxcitenames=1, maxbibnames=4, doi=false, isbn=false, eprint=true, backend=bibtex, hyperref=true, url=false, style=authoryear-comp]{biblatex}
%\addbibresource{zotero-library.bib}
% \addbibresource{paper/zotero-library.bib}

% Let's use good old bibtex instead

% Figure customization: Tight legend box
\pgfplotsset{every axis/.append style={
		legend style={inner xsep=1pt, inner ysep=0.5pt, nodes={inner sep=1pt, text depth=0.1em},draw=none,fill=none}
}}

% Our packages
\usepackage{todonotes}
\usepackage[colorlinks=true,linkcolor=blue,allcolors=blue]{hyperref}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{derivative}

\usepackage{tikz,pgfplots}
\usepackage{subcaption}
\usetikzlibrary{}

\input{aidans-utils.tex}

% Short section names etc
% This must be imported last!
%\usepackage{cleveref}
\usepackage[capitalise,nameinlink]{cleveref}
\crefname{section}{Sec.}{Secs.}
\crefname{algorithm}{Alg.}{Algs.}
\crefname{appendix}{App.}{Apps.}
\crefname{definition}{Def.}{Defs.}
\crefname{table}{Tab.}{Tabs}

% Config for Arno's awesome TikZ plotting stuff
\newlength{\figurewidth}
\newlength{\figureheight}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\action}{\ensuremath{\mathbf{a}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}
\newcommand{\numDataNew}{\ensuremath{N^{\text{new}}}}
\newcommand{\numDataOld}{\ensuremath{N^{\text{old}}}}
\newcommand{\numInducing}{\ensuremath{M}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\inputDomain}{\ensuremath{\mathbb{R}^{D}}}
\newcommand{\outputDomain}{\ensuremath{\mathbb{R}^{C}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}

% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
% \newcommand{\weights}{\ensuremath{\bm\phi}}
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transitionFnWithParams}{\ensuremath{\transitionFn_{\weights}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}


% Packages for bold math
\usepackage{bm}
\newcommand{\mathbold}[1]{\bm{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\renewcommand{\mid}{\,|\,}


% Math Macros
\newcommand{\MB}{\mbf{B}}
\newcommand{\MC}{\mbf{C}}
\newcommand{\MZ}{\mbf{Z}}
\newcommand{\MV}{\mbf{V}}
\newcommand{\MX}{\mbf{X}}
\newcommand{\MA}{\mbf{A}}
\newcommand{\MK}{\mbf{K}}
\newcommand{\MI}{\mbf{I}}
\newcommand{\MH}{\mbf{H}}
\newcommand{\T}{\top}
\newcommand{\vzeros}{\mbf{0}}
\newcommand{\vtheta}[0]{\mathbold{\theta}}
\newcommand{\valpha}[0]{\mathbold{\alpha}}
\newcommand{\vkappa}[0]{\mathbold{\kappa}}
\newcommand{\vbeta}[0]{\mathbold{\beta}}
\newcommand{\vlambda}[0]{\mathbold{\lambda}}
\newcommand{\diag}{\text{{diag}}}

\newcommand{\vm}{\mbf{m}}
\newcommand{\vz}{\mbf{z}}
\newcommand{\vf}{\mbf{f}}
\newcommand{\vu}{\mbf{u}}
\newcommand{\vx}{\mbf{x}}
\newcommand{\vy}{\mbf{y}}
\newcommand{\vw}{\mbf{w}}
\newcommand{\va}{\mbf{a}}

\newcommand{\Jac}[2]{\mathcal{J}_{#1}(#2)}
\newcommand{\JacT}[2]{\mathcal{J}_{#1}^\top(#2)}


\newcommand{\GP}{\mathcal{GP}}
\newcommand{\KL}[2]{\mathrm{D}_\textrm{KL} \dbar*{#1}{#2}}
\newcommand{\MKzz}{\mbf{K}_{\mbf{z}\mbf{z}}}
\newcommand{\MKxx}{\mbf{K}_{\mbf{x}\mbf{x}}}
\newcommand{\MKzx}{\mbf{K}_{\mbf{z}\mbf{x}}}
\newcommand{\MKxz}{\mbf{K}_{\mbf{x}\mbf{z}}}
\newcommand{\vkzi}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vkzs}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vk}{\mbf{k}}
\newcommand{\MLambda}[0]{\mathbold{\Lambda}}
\newcommand{\MSigma}[0]{\mathbold{\Sigma}}
\definecolor{matplotlib-blue}{HTML}{1f77b4}
\newcommand{\N}{\mathrm{N}}
%\newcommand{\R}{\mathrm{R}}
\newcommand{\myexpect}{\mathbb{E}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\Norm}{\mathcal{N}}


%\title{Investigatin Uncertainty Quantification in Model-based Reinforcement Learning}
% \title{Model-based Reinforcement Learning with Fast Posterior Updates}
%\title{Sequential Decision-Making under Uncertainty with Big Data}
% \title{Neural Network to Vatiational Sparse Gaussian Process: For Adaptive Exploration}
% \title{Neural Network to Sparse Variational Gaussian Process: For Updates in Sequential Decision Making}
% \title{Adapting Neural Networks to New Data For Updates in Sequential Decision Making via Gaussian Processes}
% \title{Converting Neural Networks to Gaussian Processes for Sequential Decision-Making Under Uncertainty}
%\title{Sparse Function Space Representation of Neural Networks for Exploration and Retention}
%\title{Sparse Function-space Neural Networks}
\title{Sparse Function-space Representation \\ of Neural Networks}% for Adaptation and Retention}
\author{%
  Aidan Scannell\textsuperscript{\star} \\
  Aalto University \\
  Finnish Center for Artificial Intelligence \\
  \texttt{aidan.scannell@aalto.fi}
  \And
  Riccardo Mereu\textsuperscript{\star} \\
  Aalto University\\
  \texttt{riccardo.mereu@aalto.fi}
  \And
  Paul Chang \\
  Aalto University\\
  \texttt{paul.chang@aalto.fi}
  \And
  Ella Tamir \\
  Aalto University\\
  \texttt{ella.tamir@aalto.fi}
  \And
  Joni Pajarinen \\
  Aalto University\\
  \texttt{joni.pajarinen@aalto.fi}
  \And
  Arno Solin \\
  Aalto University\\
  \texttt{arno.solin@aalto.fi}
}


\begin{document}

\maketitle

\begin{abstract}
% OLDER VERSION
%Sequential learning paradigms such as Continual Learning (CL) or Reinforcement Learning (RL) pose a challenge for gradient-based deep learning techniques as they struggle to incorporate new data and retain previous knowledge. Existing methods for converting neural networks from weight to function space allow a probabilistic treatment of the distribution over the function learned by the neural networks but are computationally expensive. We propose a method that converts a neural network to a low-rank functional representation as a sparse Gaussian process. With this approach, we can build a compact representation of the function encoded by the neural network that can replace previous data in continual settings and be used for fast adaptation in RL, avoiding full retraining of the model. 
%
% Rewrite on 2023-05-10
%Deep neural networks are known to lack uncertainty estimates, struggle to incorporate new data, and fail to retain previous knowledge. We present a method that mitigates these issues by transforming a weight-space neural network to a low-rank function-space representation, via the so-called dual parameters. In contrast to previous work, we model the joint distribution across the entire data set rather than a subset. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We demonstrate the proposed approach for quantifying uncertainty in supervised learning and maintaining a compact representation in sequential learning.\looseness-1

Deep neural networks are known to lack uncertainty estimates, struggle to incorporate new data, and suffer from catastrophic forgetting. We present a method that mitigates these issues by converting neural networks from weight-space to a low-rank function-space representation, via the so-called dual parameters. In contrast to previous work, our sparse representation captures the joint distribution over the entire data set, rather than only over a subset. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We demonstrate the proposed approach for quantifying uncertainty in supervised learning and maintaining an expressive functional representation for sequential learning.\looseness-1

\end{abstract}

%, maintaining a summary representation in continual learning,

\section{Introduction}
\label{sec:intro}
%
Deep learning \cite{goodfellow2016deep} has become the cornerstone of contemporary artificial intelligence, proving remarkably effective in tackling supervised and unsupervised learning tasks in the {\em large data}, {\em offline}, and {\em gradient-based training} regime. Despite its success, gradient-based learning techniques exhibit limitations. Firstly, how can we efficiently quantify uncertainty without resorting to expensive and hard-to-interpret sampling in the model's weight-space? Secondly, how to update the weights of an already trained model with new batches of data without compromising the performance on past data? These questions become central when applied to sequential learning paradigms, such as continual learning (CL, \citep{parisi2019continual, de2021continual}) and reinforcement learning (RL, \cite{sutton2018reinforcement}). In CL, access to the previous data is lost and the challenge is retaining a compact representation of the problem and avoiding forgetting over the life-long learning horizon~\cite{mccloskey1989catastrophic}. Similarly, in RL, the model must adapt to environmental observations through exploration, while leveraging prediction uncertainties to assess potential future paths.\looseness-1

%Secondly, how to retain information from previous tasks whilst learning new tasks where the tasks are such as Continual Learning (CL, \cite{de2021continual})
%Thirdly and differently from the CL problem is that given some data from the same distribution 


%Current state of affairs 
Recent techniques (\eg, \cite{ritter2018kfac,khan2019approximate,daxberger2021laplace,fortuin2021bayesian,immer2021scalable}) show that converting trained neural networks to a Bayesian neural network through the Laplace-GGN can provide uncertainty without sacrificing additional training cost \cite{foong2019between}. Furthermore, the resultant weight space posterior can be converted to the function space as shown \cite{khan2019approximate, immer2021improving}. The function space allows for a principled mathematical approach for analyzing the behaviour \cite{cho2009kernel,meronen2020stationary}, performing probabilistic inference \cite{khan2019approximate}, and quantifying uncertainty in neural networks \cite{foong2019between}. These methods rely on the linearization of the neural network and the resultant NTK \cite{jacot2018neural}. The result function space neural network is characterized by its first two moment functions, a mean function and covariance function (or kernel)---defining a Gaussian process (GP, \cite{rasmussen2006gaussian}). GPs provide a widely-used probabilistic toolkit with principled uncertainty estimates, and they serve as a standard surrogate model for Bayesian optimization \citep{garnett_bayesoptbook_2022} and are effective in model-based reinforcement learning \citep{deisenroth2011pilco} with theoretical guarantees on regret bounds \citep{srinivas2009gaussian}. 
%Yet many problems lie in high dimensional input space; for example, images are where GPs cannot learn representations. In such scenarios such as in many reinforcement learning environments neural networks are used as the surrogate model. However, uncertainty is still essential to ensure effective exploration for sequential algorithms. Successful approaches have attempted to blend neural networks with uncertainty estimates around predictions, allowing for sophisticated exploration strategies. However, there has been limited use of hybrid models that possess the feature representation ability of neural networks but also attractive the properties of GPs, such a hybrid method we propose in this paper.

%Need for adaptive learning methods + failures with current methods
In this paper, we demonstrate that any neural network can be represented in terms of the so-called `dual' parameters of a GP~\cite{csato2002sparse, adam2021dual, chang2020fast}. In contrast to previous work that utilizes subsets \cite{immer2021scalable}, this parameterization allows for the consolidation of contributions from {\em all} training data into a sparse representation. Crucially, the resulting GP now predicts in the same space as the originally trained neural network, a feature not present in previous approaches, while still avoiding the notorious cubic complexity of vanilla GPs. Through the dual parameter formulation, we establish a connection between the neural network, the resulting full GP, and a sparse approximation similar to variational sparse GPs~\cite{titsias2009variational}. Using the sparse GP we can then take advantage of the property of dual parameters that allows for fast conditioning recently shown in \cite{chang2022fantasizing}. Thus we are able to avoid retraining and condition new data in to our model. The resulting \our method can be applied in any sequential decision-making scenario where retraining is not feasible.

%Need uncertainty and adaptive methods
%Dual formulation in GPs space solves this
%Talk about planning and exploration in RL.


The contributions of this paper are:
%
{\em (i)}~We introduce \our, a new approach for sparse representation of the neural network in the function space.
{\em (ii)}~We demonstrate that, despite its sparsity, our method effectively captures predictive uncertainty, provides means of updating the model post-training, and gives a a compact representation suitable for continual learning.
{\em (iii)}~We provide extensive experiments for showcasing our approach and demonstrate significance and applicability across supervised, continual, and reinforcement learning, aiming to stimulate future use of the approach.

%List the contributions.
%The contributions of the paper our is as follows:
%\begin{itemize}
%\item We show how to take a trained neural network and convert it to a dual sparse GP. We are able to do this without retraining a variational objective for the Sparse GP. Our sparse GP uses the variational formulation, and thus gives better uncertainty estimates than other no variational sparse approaches used previously.
%\item The sparse GP now gives us a compact representation of our parameters in the function space. We can therefore take advantage of the dual parameters formulation for fast conditioning of new data in to our posterior avoiding retraining of the neural network. Crucially this allows for fast adaptation of models that our used in sequential decision making. We show how this is effective in the planning stage of model-based reinforcement exploration.
%\end{itemize}




%
%
%\begin{itemize}
%  \item Many real-world problems require learning-based systems that can adapt to new data.
%  \begin{itemize}
%    % \item For example, in domains such as robotics and healthcare,
%    \item For example, when controlling robots in non-stationary environments it is important for the robot to adapt to the changing dynamics.
%    \item However neural networks rely upon gradient-based optimisation.
%    \item Uncertainty can be used to improve sample efficiency via targeted exploration.
%    \item Uncertainty can be used to handle risk in decision making.
%  \end{itemize}
%
%  \item Gaussian processes can easily adapt to new data and they offer well-calibrated uncertainty estimates.
%  \begin{itemize}
%    \item However, they don't scale to high-dimensional and large data sets.
%  \end{itemize}
%  \item
%  \begin{itemize}
%    \item
%  \end{itemize}
%\end{itemize}




\begin{figure}[t!]
  \centering\scriptsize
  % Figure options
  \pgfplotsset{axis on top,scale only axis,width=\figurewidth,height=\figureheight, ylabel near ticks,ylabel style={yshift=-2pt},y tick label style={rotate=90},legend style={nodes={scale=1., transform shape}},tick label style={font=\tiny,scale=1}}
  \pgfplotsset{xlabel={Input, $x$},axis line style={rounded corners=2pt}}
  % Set figure 
  \setlength{\figurewidth}{.28\textwidth}
  \setlength{\figureheight}{\figurewidth}
  %
  \def\inducing{\large Sparse inducing points}
  %
  \begin{subfigure}[c]{.34\textwidth}
    \raggedleft
    \pgfplotsset{ylabel={Output, $y$}}
    \input{./fig/regression-nn.tex}%
  \end{subfigure}
  \hfill  
  \begin{subfigure}[c]{.01\textwidth}
    \centering
    \tikz[overlay,remember picture]\node(p0){};
  \end{subfigure}  
  \hfill
  \begin{subfigure}[c]{.28\textwidth}
    \raggedleft
    \pgfplotsset{yticklabels={,,},ytick={\empty}}
    \input{./fig/regression-nn2svgp.tex}%
  \end{subfigure}
  \hfill  
  \begin{subfigure}[c]{.01\textwidth}
    \centering
    \tikz[overlay,remember picture]\node(p1){};
  \end{subfigure}  
  \hfill
  \begin{subfigure}[c]{.28\textwidth}
    \raggedleft
    \pgfplotsset{yticklabels={,,},ytick={\empty}}        
    \input{./fig/regression-update.tex}%
  \end{subfigure}
  \caption{\textbf{Regression example on an MLP with two hidden layers.} Left:~Predictions from the trained neural network. Middle:~Our approach summarizes all the training data with the help of an iducing set of points. The model captures the predictive mean and uncertainty, and (right) makes it possible to update the model with new data without degrading previous performance.}
  \label{fig:teaser} 
  % 
  \begin{tikzpicture}[remember picture,overlay]
    % Arrow style
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=14mm, minimum width=2pt, single arrow head extend=4pt, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    % Arrows
    \node[myarrow] (p0-arr) at ($(p0) + (1em,1.5em)$) {};
    \node[myarrow] (p1-arr) at ($(p1) + (1em,1.5em)$) {};
    % Arrow labels
    \node[font=\scriptsize\sc,color=white] at (p0-arr) {\our};
    \node[font=\scriptsize\sc,color=white] at (p1-arr) {new data};   
  \end{tikzpicture}
\end{figure}





\subsection{Related work}
\label{sec:related}
%
% General Bayesian deep learning
\textbf{Bayesian deep learning}
Probabilistic method in deep learning~\cite{Wilson:ensembles,neal1995bayesian} have recently gained increasing attention in the machine learning community as a means for uncertainty quantification (\eg, \cite{kendall2017what,wilson2020bayes}) and model selection (\eg,~\cite{immer2021scalable,antoran2022marginal}) with advancements in prior specification (\eg, \cite{cho2009kernel,meronen2020stationary,meronen2021periodic,fortuin2021bayesian,nalisnick2018do}) and efficient approximate inference under the specified model.
Calculating the posterior distribution of a Bayesian neural network is usually intractable, and approximate inference techniques need to be used, such as variational inference \cite{blei2017variational}, deep ensembles \cite{lakshminarayanan2017simple}, MC dropout \cite{gal2016dropout}, or Laplace approximation \cite{ritter2018kfac,kristiadi2020being,immer2021improving}---each having strengths and weaknesses. % point out strngths and weaknesses for each

% Introduce the laplace GGN + shortcomings
%\textbf{Function-space methods}
One common approach for uncertainty with neural networks is a Laplace-GGN approximation \citep{daxberger2021laplace}, which takes a trained neural network and linearises it around the optimal weights. The Hessian can be efficiently approximated using the generalized Gauss--Newton approximation (GGN)~\cite{botev2017practical} but typically involves a cubic scaling in the number of parameters, in practice a further approximation such as the Kronecker factorisation \cite{martens2015optimizing,ritter2018kfac} is needed. The resulting linear model can be used to obtain uncertainty estimates and refine the neural network predictions \citep{immer2021scalable}, and is linear with respect to the weights. Therefore, as shown in \cite{immer2021scalable, khan2019approximate,maddox2021fast}, the linear model can be converted to a GP, which is then used to obtain the uncertainty estimates. 
%
However, the GP introduces a cubic scaling $\mathcal{O}(N^3)$ in the number of data points $N$. Previous approaches apply crude approximations to alleviate the computational costs, \eg, by considering only a subset of the training data \cite{immer2021scalable}. In the GP community, the scaling problem is not seen as an issue any more due to efficient and stochastic methods for sparse approximations (\eg, \cite{hensman2013gaussian,wang2019exact}). However, it is not entirely clear how to combine the linearization of the neural network with sparse methods \cite{titsias2009variational} without falling back to crude subset methods. We tackle this problem by framing the problem in the dual parameters \cite{csato2002sparse} of the GP that has previously been used for non-conjugate likelihood models in GPs \cite{adam2021dual}. 


% CL section
\textbf{Function-space methods for sequential learning} The central problem in continual learning is how to deal with the non-stationary nature of the training distribution, which causes the training process to overwrite the previously learnt parameters---leading to the model forgetting the previously learnt functions. 
The approaches proposed in the CL literature can be categorized into inference-based, rehearsal-based, and model-based methods; we refer the reader to \citep{parisi2019continual, de2021continual} for complete reviews. The methods in the first category usually tackle this problem with weight-space regularization, such as EWC~\citep{kirkpatrick2017overcoming}, SI~\citep{zenke17a}), or VCL~\citep{nguyen-tuongModel2009} that induce retention of previously learned information in the weights alone.
However, encoding functional knowledge in the weights does not guarantee a good quality of the predictions, thus a better to introduce a function-space regularization \citep{li2018lwf, benjamin2018measuring, titsias2019functional, buzzega2020dark, pan2020continual, rudner2022continual}. These methods bridge the gap between objective and rehearsal-based methods since they rely on a subset of the training data for each task to compute the regularization term. Recently proposed methods, such as DER~\citep{buzzega2020dark}, FROMP~\citep{pan2020continual}, and S-FSVI~\citep{rudner2022continual} achieve state-of-the-art performances among the objective-based techniques in several CL benchmarks. 

% It should be written clearly that our method actually is doing a "functional" reharsal, because we store a set of inducing points that are replayed in order to check if the logits change.

% Other methods that could be worth citing are 
% Learning without forgetting \citep{li2018lwf} :  first paper proposing to use functional regularization computes a smoothed version of the current logits for the new examples at the beginning of each task, minimizing their drift during training. It doesn't have any set of inducing points
% \citep{benjamin2018measuring} this is not a proper CL paper, but it's usually referred in the functional regularization papers, because shows how to improve Adam with measuring the network difference in function space. They also have some easy CL examples there.



%Why we need uncertainty and adaptiveness
%How to adapt to new information is a limitation of current machine learning models. The problem is specifically relevant when we combine a model with a decision-making process. In such a sequential setting, an agent typically makes decisions in an environment and obtains new information and would ideally incorporate the information into their parameters. Not only do we need models that adapt to new informatoion but all express uncertainty over the models predictions. The reason being twofold: in many applications, \eg\ healthcare, autonomous driving is an essential requirement, and many advanced exploration techniques require uncertainty to determine where to query next.



\begin{figure}[t!]
  \centering
  % Set figure size
  \setlength{\figurewidth}{.31\textwidth}
  \setlength{\figureheight}{\figurewidth}
  %
  % Colours
  \definecolor{C0}{HTML}{DF6679}
  \definecolor{C1}{HTML}{69A9CE}
  %
  \begin{tikzpicture}[outer sep=0,inner sep=0]

    \newcommand{\addfig}[2]{
    \begin{scope}
      \clip[rounded corners=3pt] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
      \node (#2) at (#1) {\includegraphics[width=1.05\figurewidth]{./fig/#2}};
    \end{scope}
    %\draw[rounded corners=3pt,line width=1.2pt,black!60] ($(#1)+(-.5\figurewidth,-.5\figureheight)$) rectangle ++(\figurewidth,\figureheight);
    }

    % The neural network
    \addfig{0,0}{banana-nn}

    % The nn2svgp
    \addfig{1.1\figurewidth,0}{banana-nn2svgp}

    % The update
    \addfig{2.2\figurewidth,0}{banana-hmc}

    % The arrow
    \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=14mm, minimum width=2pt, single arrow head extend=4pt, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    \tikzstyle{myblock} = [draw=black!80, minimum height=4mm, minimum width=7mm, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    \node[myarrow] (first-arr) at ($(banana-nn)!0.5!(banana-nn2svgp)$) {};
    \node[myblock] (second-arr) at ($(banana-nn2svgp)!0.5!(banana-hmc)$) {};

    % Arrow labels
    \node[font=\scriptsize\sc,color=white] at (first-arr) {\our};
    \node[font=\scriptsize\sc,color=white] at (second-arr) {\normalsize$\bm\approx$};
         
    % Labels
    \node[anchor=north, font=\small] at ($(banana-nn) + (0,-.55\figureheight)$) {Neural network prediction};
    \node[anchor=north, font=\small] at ($(banana-nn2svgp) + (0,-.55\figureheight)$) {Sparse GP representation};
    \node[anchor=north, font=\small] at ($(banana-hmc) + (0,-.55\figureheight)$) {HMC result as baseline};      

  \end{tikzpicture}
  \newcommand{\mycircle}{\protect\tikz[baseline=-.6ex]\protect\node[circle,inner sep=2pt,draw=black,fill=C0,opacity=.5]{};}
  \newcommand{\mysquare}{\protect\tikz[baseline=-.6ex]\protect\node[inner sep=2.5pt,draw=black,fill=C1,opacity=.5]{};}
  \newcommand{\myinducing}{\protect\tikz[baseline=-.7ex]\protect\node[circle,inner sep=1.5pt,draw=black,fill=black]{};}
  %
  \caption{\textbf{Uncertainty quantification} for binary classification (\mysquare~vs.~\mycircle). We convert the trained neural network (left) to a sparse GP model that summarizes all data onto a sparse set of inducing points~\myinducing\ (middle). This gives similar behaviour as would running full Hamiltonian Monte Carlo (HMC) on the original neural network model weights (right). Marginal uncertainty depicted by colour intensity.\looseness-1}
  \label{fig:banana}  
\end{figure}



\section{Background: Function-space representation of neural networks}
\label{sec:methods}
%
%In this section, we recap how a trained neural network (NN) can be converted to a Gaussian process by locally linearising its weights.
% a GP posterior -- can be obtained
% around the Maximum a Posteriori (MAP) weights.

% by linearising a neural network around the Maximum a Posteriori (MAP) weights.
% a GP posterior -- can be obtained
%
In supervised learning, given a data set $\dataset = \{(\vx_{i} , \vy_{i})\}_{i=1}^{n}$, with input $\vx_i \in \inputDomain$ and output $\vy_i \in \outputDomain$ pairs, the weights $\weights \in \R^{P}$ of a neural network, $f_\mathbf{w} : \inputDomain \to \outputDomain$, are usually trained to minimize the (regularized) empirical risk,
%
\begin{equation} \label{eq-empirical-risk}
  \weights^{*} = 
  \arg \min_{\weights} \mathcal{L}(\dataset,\weights) =
  \arg \min_{\weights} \sum_{i=1}^{n} \ell(f_\weights(\mathbf{x}_{i}), \mathbf{y}_i) + \delta \mathcal{R}(\weights).
\end{equation}
%
If $\ell(f_\weights(\vx_{i}), \vy_i) = -\log(p(\vy \mid f_\weights(\vx_{i}))$ and the regularizer $\mathcal{R}(\weights) = \frac{1}{2}\|\weights\|^{2}_2$, then we can view \cref{eq-empirical-risk} as the maximum {\it a~posteriori} (MAP) solution to a Bayesian objective, where the regularization weight takes the role of a prior precision parameter, \ie, $p(\vw) = \Norm(\vzeros, \delta^{-1} \MI)$.
%Bayesian inference offers a principled approach to quantifying uncertainty in neural networks.
%The goal is to find the posterior over the weights ${p(\vw \mid \vy) \propto p(\vy \mid f_{\weights}(\vx)) \, p(\weights)}$ as it represents our belief in the parameters after combining data $\dataset$ with our prior $p(\vw)$.
%Although the true posterior $p(\vw \mid \dataset)$ is intractable given the non-linearities of the neural network, one can resort to sampling techniques such as Hamiltonian Monte-Carlo (HMC).
%However, for most applications their high computational costs make them impractical so we need approximate inference techniques.
%In \cref{fig:banana}, we show a qualitative example of the induced function-space posterior obtained with our method compared to the posterior obtained through HMC sampling on the Banana toy dataset. 
The posterior over the weights ${p(\vw \mid \dataset) \propto p(\vy \mid f_{\weights}(\vx)) \, p(\weights)}$ is generally not available in closed form. Sampling methods---such as the Hamiltonian Monte Carlo (HMC) baseline used in \cref{fig:banana} (right)---are general-prupose, but comutationally heavy and impractical for downstream applications.

\textbf{Function space}
As a neural network is a deterministic mapping, the weight-space posterior induces a distribution over the function values.
Intuitively, if one was to sample from the weight posterior, the corresponding functions created can
be viewed as perturbed versions of the function at the MAP estimate $f_{\vw^*}$.
% \todo{not super sure about the perturbed part here. I know what \textrm{you}'re trying to say but a little confused}
In most applications, we care about predictions from the neural network and not the weights themselves.
As such, it is the distribution over function values that we are actually interested in.


\textbf{Linearization gives rise to a Gaussian process}
Our goal is to build a sparse function-space representation of a neural network.
Gaussian processes $f$ are an extension of the multivariate normal that characterizes a distribution over
functions in terms of the first two moments.
They are defined by a mean function $\mu(\cdot)$ and a covariance (aka kernel) function $\kappa(\cdot,\cdot)$.
Thus, we need these two functions based on the neural network to build the Gaussian process posterior.
Recent work \citep{khan2019approximate,maddox2021fast} has shown that linearizing approximations in the weight space
lead to function space equivalent approximations.
As Gaussian distributions remain tractable under linear transformations, a linear function in
terms of parameters can be converted from the weight space to the function space (see Ch.~2.1 in \cite{rasmussen2006gaussian}) as follows:\begin{equation} \label{eq:weight_func}
f_\weights(\vx) \approx 
%g_\weights(\mathbf{x}) = 
\phi^\top\!(\vx) \, \vw \implies \mu(\vx) = 0 \quad \kappa(\vx, \vx') = \frac{1}{\delta} \phi^\T\!(\vx) \, \phi(\vx')
\end{equation}
% The posterior structure directly relates to the optimization loss around the MAP weights $\vw^*$.
A common approach is to approximate the correlation structure of a distribution centred at the MAP estimate as done in the Laplace-GGN \citep{khan2019approximate, daxberger2021laplace, maddox2021fast}. The Laplace-GGN takes the MAP solution and approximates the Hessian of the loss function
% $\ell(f_\weights(\mathbf{x}_{i}), \mathbf{y}_i)$
with the GGN.
Following the work of \citep{khan2019approximate}, we can view this approximation as building an approximate linear model of the neural network as $f_{\weights^*}(\vx) \approx 
%g_{\weights}(\vx) = 
\Jac{\weights_*}{\vx} \, \weights$, where $\Jac{\weights}{\vx} \coloneqq \left[ \nabla_\weights f_\weights(\vx)\right]^\top \in \R^{C \times P}$ is the Jacobian at the MAP.
Using the Hessian of the approximate model, we arrive at the Laplace-GGN approximate posterior for $q(\vw)$.
\todo{do we need this sentence about the Hessian to get Laplace-GGN??}
Therefore, the Laplace-GGN linear model can be used to convert our weight space model to the function space by,
\begin{equation} 
\label{eq-laplace-approx-function-space} 
% g(\vx) \sim \GP \left( \mu(\vx), \kappa(\vx, \vx') \right) \quad \text{with} \quad
  \mu(\vx) =  0 \quad \text{and} \quad
  \kappa(\vx, \vx')
  = \frac{1}{\delta} \Jac{\weights^*}{\vx} \, \JacT{\weights^*}{\vx'}, 
\end{equation}
where the kernel is the so-called Neural Tangent Kernel (NTK, \cite{jacot2018neural}). In \cref{eq-laplace-approx-function-space}, for notational convenience, we restricted the function output to single dimension, but this could be easily extended for the multi-dimensional case. With this kernel function, we can then combine it with the dataset $\dataset$ to form the posterior $q(\vf)$. \citet{khan2019approximate} took a similar approach, but instead of fitting the GP posterior to the actual $\vy$, they rely on a transformation of $\vy$ referred as pseudo-data. Similarly, \citet{immer2021improving} obtain the same covariance function but a different mean because they rely on a first order approximation of the neural network to form a Bayesian GLM model. Both approaches attempted to adjust the GP posterior mean function to ensure the predictions are from the neural network $f_{\vw^*}(\vx)$. In our method, we wish to make predictions from the derived GP and thus we avoid such adjustments. We now present the general formulation of the sparse functional representation of our trained neural network based on the non-sparse approximation. 



\section{\our: Sparse Function-space Represenation of neural networks}
\paragraph{GP in the dual parameters}
As stated in \citet[Parameterization Lemma~1]{csato2002sparse}, the mean and covariance functions of the posterior process $f_{\textrm{post}}$, found through the Bayesian update using the GP prior and the likelihood function, can be parameterized by the dual parameters, $\valpha$ and $\vbeta$,
\begin{equation}  \label{eq:gp_pred}
  \myexpect_{p(f_i \mid\vy)}[f_i]= \vk_{\vx i}^\top \valpha \quad \text{and} \quad
  \mathrm{Var}_{p(f_i \mid \vy)}[f_i] = \kappa_{ii} - \vk_{\vx i}^\top ( \MKxx + \diag(\vbeta)^{-1})^{-1} \vk_{\vx i},
\end{equation}
where the $ij$th entry of the matrix $\MKxx \in \R^{n \times n}$ is $\kappa(\vx_i,\vx_j)$, $\vk_{\vx i}$ denotes a vector where each $j$th element is $\kappa(\vx_i, \vx_j)$, and $\kappa_{ii} = \kappa(\vx_i, \vx_i)$.  The dual parameters are the vector $\valpha \in \R^{m}$ and vector $\vbeta \in \R^{m}$, defined as:
\begin{equation}
\label{eq:dual_param}
\alpha_i \coloneqq \myexpect_{p(\vw \mid \vy)}[\nabla_{f_i}\log p(y_i \mid f_i)],  \quad
\beta_i \coloneqq - \myexpect_{p(\vw \mid \vy)}[\nabla^2_{f_i f_i}\log p(y_i \mid f_i)] .
\end{equation}
%
The above relations hold for generic likelihoods, and no approximations are involved as the expectation is under the exact posterior. 
It also highlights that our approximate inference technique, while usually considered a posterior approximation, can instead be seen as approximating the expectation of gradients of loss/likelihoods. \todo{our approximation is late \cref{eq:dual_param_laplace}, saying this here is misleading}\\
In \citet{csato2002sparse}, they iteratively find dual variables using an Expectation Propogation (EP) method.  
At the same time, in \cite{adam2021dual, chang2020fast}, they show this relationship for variational Gaussian processes where the above expectation is with respect to the approximate variational posterior. The dual parameter $\valpha$ appears also in non-Bayesian approaches, \eg, support vector machines \citep{cortes1995support}, whose origins are found in work by \citet{kimeldorf1971some}. While in \citet{wilkinson2023bayes}, they show links between linearization methods and how they solve the \cref{eq:dual_param}.\todo{is this last part used/referred somewhere else?}

\paragraph{Dual parameters from NN}
Given that we use a Laplace approximation of the neural network, we remove the expectation over the posterior (see Ch.~3.4.1 in \cite{rasmussen2006gaussian} for derivation) and we get the the following formulation of the dual variables,
\begin{equation}
\label{eq:dual_param_laplace}
\hat{\alpha}_i \coloneqq \nabla_{f_i}\log p(y_i \mid f_i),  \quad \text{and} \quad
\hat{\beta}_i \coloneqq - \nabla^2_{f_i f_i}\log p(y_i \mid f_i).
\end{equation}

Substiuting \cref{eq:dual_param_laplace} into \cref{eq:gp_pred}, we obtain our GP model based on the converged neural network. Again, this is similar to what was derived in \citet{immer2021improving} for the posterior variance function. However, they use the $f_{\vw^*}$ for the posterior mean and do not spot the significance of the dual variables. The problem with \cref{eq:gp_pred} is that to make predictions and compute variances we must incur a cost of $O(n^3)$ which limits the use of the GP on large data sets.

\paragraph{Sparsifying the NNGP}
\label{sec:sparse-dual-gp}
%
Sparse Gaussian processes are a way to reduce the computational complexity of a full GP using a low-rank approximation. Many sparsifying approaches exist and we refer the reader to \cite{quinonero2005unifying} for an overview. Given that we computed the dual variables derived from our neural network predictions and a kernel function, we could essentially use any approximation.\todo{approximation to get what? kernel and dual variables} We decide to follow the sparsifying approach of \citet{titsias2009variational} and also used in the DTC approximation \citep{quinonero2005unifying}, which defines the marginal predictive distribution as $q_{\vu}(f_i)  = \int p(f_i  \mid \vu) q(\vu)  d\vu$. Given that we have $p(f_i  \mid \vu)$ determined by our GP prior, the goal is to find a $q(\vu)$. 

As was shown in \cite{adam2021dual}, the posterior under this model has a particular structure similar to \cref{eq:gp_pred}. In the paper, the authors spot the dual variables' importance for approximate inference, but write them with the natural parameterization as this form is more suitable for optimization through natural gradients. In order to link the dual variables defined in \cref{eq:dual_param}, we write them in the sparse GP using the dual variables
\begin{equation} 
	\valpha_{\vu}  =  \sum_{i \in \mathcal{D}_{n}}  \vkzi \, \hat{\alpha}_{i}, \quad
	\vbeta_{\vu} =  \sum_{i \in \mathcal{D}_{n}} \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T} ,    
\label{eq:dual_sparse}
\end{equation}
where the sparse dual variables are now a sum over all data points, with $\valpha_{\vu} \in \R^{M}$ and $\vbeta_{\vu} \in \R^{M  \times M}$. Using these sparse definition of the dual variables, our sparse GP posterior takes the following form:
\begin{equation}\label{eq:dual_sparse_post}
   \myexpect_{q_{\vu}(\vf)}[f_i] = \vkzs^{\T} \MKzz^{-1}   \valpha_{\vu} , \quad 
   \textrm{Var}_{q_{\vu}(\vf)}[f_i]  = \kappa_{ii} - \vkzs^\top [\MKzz^{-1} - (\MKzz + \vbeta_{\vu})^{-1} ]\vkzs
\end{equation}
where $\MKzz$ and $\vkzs$ are defined similarly to $\MKxx$ and $\vk_{\vx i}$ but over the inducing points $\MZ \in \R^{M \times D} $  instead of the full dataset $\dataset$. The key quantities we need to make predictions from our sparse GP from the converged neural network are $(\hat{\alpha}_i, \hat{\beta}_i)$ \cref{eq:dual_param_laplac}\todo{these variables need another subscript for differentiate that from the values computed at the data points} and a kernel function $\kappa$ . Contrasting \cref{eq:dual_sparse_post} and \cref{eq:gp_pred}, we can see that the computational complexity went from $\mathcal{O}(n^3)$ to $\mathcal{O}(m^3)$, with $m \ll n$.  Crucially, given the structure of our probabilistic model, our sparse dual variables \cref{eq:dual_sparse} are a compact representation of the full model projected using the kernel. 

In contrast to our approach, \citet{immer2021improving} use a subset of the data points to make a sparse GP model to reduce the computational complexity. As a result, this is a less principled approach for building a sparse model since it ignores the dataset's information. It is important to highlight that the sparsifying process is independent of the approximate inference technique because the dual variables of \cref{eq:dual_param} are computed using \cref{eq-empirical-risk} and a Laplace approximation.
More complicated inference techniques, such as variational inference, could be used, but given its simplicity, in our experiments we used the Laplace-GGN approximation with the trained neural network. Furthermore, our dual variable view of approximate inference means we do not need to retrain a separate sparse GP model. \todo{shouldn't this sentence be clear from the context? we derive the mean/kernel function that define the process, then we have a sparse GP already}
\section{Sparse Function-space Sequential Learning}

We have presented a method for converting any trained neural network into a sparse function-space representation.
In this section, we demonstrate how sequential learning methods can benefit from our function-space representation and it's associated uncertainty estimates.



\subsection{\our for Continual Learning}
% Definition of CL -> a sequence of tasks 
In the Continual learning setting, the training is divided into $T$ tasks, each with its training data set $\dataset_t = \{(\mathbf{x}_{i}, \mathbf{y}_{i})\}_{i=1}^{n_t}$, which after the task is discarded and cannot be accessed in the following stages of the training. Rehearsal and function regularization-based models keep a subset of the training data for each task to help the model alleviate forgetting. In the same way, after each task, we can use our \our to build a compact representation of the neural network and then exploit this representation to build a functional regularizer to add to the training loss for the subsequent tasks. Recently proposed GP-based regularizers \cite{ pan2020continual, rudner2022continual} have shown better performances than weight space equivalents getting state-of-the-art on CL benchmarks. However, they are limited to use subset approximations of the posterior.
%, which in practice means randomly selecting some data computing its posterior at only those points and then using that as the regularizer. 
A more principled approach would be to sparsify the resultant GP into a low-rank approximation to ensure information coming from the entire task dataset is not lost.%, essentially what the sparse dual variables do.   \todo{this term is replacing the missing data}
% Where the regularizer comes from
At the end of the training for each task $t$, we compute the Laplace-GGN approximated kernel function for the current model and randomly select $m$ inducing points. Given the issues described in the previous sections, related to computing the full covariance posterior $\MKxx$,\todo{this should be $\kappa(\MX_t, \MX_t)$}, we can exploit the dual parameterization to efficiently encode the information from the $\dataset_t$ using the dual parameterization, as follows, 
\begin{equation}
 	\bar{\MB}^{-1}_t = \MKzz^{-1} \vbeta_\vu \MKzz^{-1} \in \R^{m \times m}, 
 	\quad 
 	\vbeta_{\vu} =  \sum_{i \in \dataset_t} \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T} ,    
 	% \quad \forall t \in [1, T]
 \end{equation}
 where $\MKzz$ and $\vkzi$ are the kernel function computed on the inducing points $\MZ_t \in \R^{m \times D}$ for task $t$.
 
We can then keep a memory buffer $\mathcal{M} = \{(\MZ_t, \vu_t, \bar{\MB}^{-1}_t)\}_{t=1}^T$, where $\MZ_t$ are the inducing points selected from $\dataset_t$ and  $\vu_t = f_{\vw_t^*}(\MZ_t)\in \R^{m}$ are the neural network outputs over them computed with the MAP estimate after task $t$, $\vw_t^*$. Note that, for notational convenience, we are extending our neural network notation to be compatible with multiple inputs points, \ie, each $i$th entry is $f_{\vw_t^*}(\vz_i)\in \R$. Moreover, we restrict ourselves to single-output neural networks and refer the reader to further details on the multi-output setting to \cref{sec:cl_multioutput}. 

After each task, we update $\mathcal{M}$, and we can construct a regularizer term, which aims to reduce the drift of the current model from diverging on the inducing points $\MZ_s$ for all previous tasks $ s \in \left[ 1, t-1 \right]$. The resulting regularizer can be computed as follows
%\begin{equation}
%	\mathcal{R}_\textit{SFR}(\weights, \mathcal{M}) = \sum_{s=1}^{t-1} \frac{1}{m} \left[\left(f_{\weights}(\MZ_{s}) - f_{\weights_{s}}(\MZ_s) \right)^\T \bar{\MB}^{-1}_{s} \left(f_{\weights}(\MZ_{s}) - f_{\weights_{s}}(\MZ_s) \right) \right].
%\end{equation}

\begin{equation}
	\mathcal{R}_\textit{SFR}(\weights, \mathcal{M}) = \sum_{s=1}^{t-1} \frac{1}{m} 
	\left\lVert 
	f_{\weights}(\MZ_{s}) - \vu_t % f_{\weights_{s}}(\MZ_s)
	\right\rVert_{\bar{\MB}^{-1}_{s}}.
\end{equation}
%\todo{choose one option}
% where $f_\vw(\MZ_{s}) \in \R^m$ are the outputs of neural networks with the current weights $\vw$ and stored function outputs of the neural network at the MAP optimum for task $s$, i.e., each $i$th entry is $f_\cdot(\vz_i) \in \R$.
%\begin{equation}
%	\argmin_{\weights} \sum_{i=1}^{n} \ell(f_\weights(\mathbf{x}_{i}), \mathbf{y}_i) + \delta \mathcal{R}(\weights) + \tau \mathcal{R}_\textit{SFR}(\weights)
%\end{equation}
Then, when the training on the successive task, the new objective to minimize takes the following form
\begin{equation}
	\weights_t^* = \argmin_{\weights} \mathcal{L}(\mathcal{D}_t, \weights) + \tau \mathcal{R}_\textit{SFR}(\weights, \mathcal{M})
\end{equation}






% We start by obtaining a posterior over the weights of a trained neural network via the Laplace approximation.
% We then show how we can obtain a function-space posterior, (i.e. a GP posterior)
% by linearising the neural network around the weights Maximum a Posteriori (MAP) estimate.
% We then show that by linearising the neural network we can obtain a function-space posterior, i.e. a GP posterior.
% We paraterise our single-step dynamic model $\transitionFnWithParams : \inputDomain \rightarrow \outputDomain$
% as an $L\text{-layer}$ NN with weights $\weights$.

%A Gaussian process (GP) is a distribution over real-valued functions $f(\cdot): \mathcal{X} \rightarrow \R$ defined over $\mathcal{X}$.
%Formally, a GP is defined as,
%\begin{align}
%\label{eq-gp-prior}
%  f(\cdot) \sim \mathcal{N}\left( \mu(\cdot), k(\cdot,\cdot') \right) \quad
%  \mu(\cdot): \mathcal{X} \rightarrow \R \quad
%  k(\cdot,\cdot'): \mathcal{X} \times \mathcal{X} \rightarrow \R
%\end{align}
%where $\mu(\cdot)$ denotes a mean function and $k(\cdot,\cdot')$ a positive definite covariance function, also known as a kernel.
%Given a data set $\dataset = \{\mathbf{x}_{n} \in \inputDomain, \mathbf{y}_{n} \in \outputDomain\}_{n=0}^{N}$,
%we can obtain the GP posterior via multivariate normal conditioning,
%\begin{align}
%\label{eq-gp-predictive-posterior}
%  p(f(\mathbf{x}_{*}) \mid \mathbf{y}) &= \mathcal{N}
%  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{x}\mathbf{x}}^{-1} \mathbf{A}^{T} \right) \\
%  \left( f(\mathbf{x}_{*}) \mid \mu(\mathbf{x}_{*}) + \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} (\mathbf{y} - \bm\mu_{\mathbf{X}})),
%  k_{**} - \mathbf{k}_{*\mathbf{x}} \left(\mathbf{K}_{\mathbf{x}\mathbf{x}} \right)^{-1} \mathbf{k}_{*\mathbf{x}}^{T} \right)
%\end{align}
%% Our GP posterior is then given by,
%\todo{add mean func to GP posterior if we're using one}
%\begin{align}
%\label{eq-gp-predictive-posterior}
%  p(f(\mathbf{x}_{*}) \mid \mathbf{y}) &= \mathcal{N}
%  % \left( f(\mathbf{x}_{*}) \mid \mathbf{A} \mathbf{m}^{*}, \mathbf{A}\mathbf{K}_{\mathbf{x}\mathbf{x}}^{-1} \mathbf{A}^{T} \right) \\
%  \left( f(\mathbf{x}_{*}) \mid \mu(\mathbf{x}_{*}) + \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}} \mu_{\mathbf{x}},
%  k_{**} - \mathbf{k}_{*\mathbf{x}} \left(\mathbf{K}_{\mathbf{x}\mathbf{x}} + \bm\Lambda(\mathbf{y} ; \mathbf{f})^{-1} \right)^{-1} \mathbf{k}_{*\mathbf{x}}^{T} \right)
%\end{align}
%where $\bm\Lambda(\mathbf{y} ; \mathbf{f}) = \nabla^{2}_{\mathbf{f} \mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$ can be viewed as per-input noise. \todo{I'm unsure about this per input noise bit. Check with paul.}
%\todo{what about regression/classification likelhoods}
%This GP predictive posterior is computationally expensive as it requires inverting an $N\times N$ matrix.
%
%\textbf{Sparse Gaussian processes}
%% The GP predictive posterior in \cref{eq-gp-predictive-posterior} is computationally expensive as it requires inverting an $N\times N$ matrix.
%A practical and computationally appealing alternative is to augment the joint probability space with pseudo inputs $\mathbf{Z} \in \R^{M \times D}$
%and corresponding outputs
%$\mathbf{u} = f(\mathbf{Z}; \weights) \in \R^{M \times C}$, known as inducing variables, where $M \ll N$.
%\todo{cite sparse gp papers}
%Instead of collapsing the inducing variables like \cite{titsiasVariational2009}, they can be treated variationally
%$p(\mathbf{u} \mid \mathbf{y}) \approx q(\mathbf{u}) = \mathcal{N}\left( \mathbf{u} \mid \mathbf{m}, \mathbf{V} \right)$, where $\mathbf{m}$ and $\mathbf{V}$ are variational parameters
%which need to be optimised \citep{hensmanGaussian2013}.
%The sparse variational GP (SVGP) predictive posterior is then given by,
%\begin{align}
%\label{eq-dual-svgp-predictive-posterior}
%  p(f(\mathbf{x}_{*}) \mid \mathbf{y})
%&\approx \mathcal{N} \left( f(\mathbf{x}_{*}) \mid \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{m}^{*},
%  k_{**} - \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\mathbf{z}}^{T}
%  % \mathbf{k}_{*\inducingInput} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\inducingInput}^{T}
%  + \mathbf{k}_{*\mathbf{z}} \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}}  \mathbf{V}^{*}  \mathbf{K}^{-1}_{\mathbf{z}\mathbf{z}} \mathbf{k}_{*\mathbf{z}}^{T}
%  \right)
%\coloneqq q_{\inducingVariable}(f(\mathbf{x}_{*}))
%\end{align}
%
%
%\textbf{Deep learning}
%Given the data set $\dataset = \{\mathbf{x}_{n} \in \inputDomain, \mathbf{y}_{n} \in \outputDomain\}_{n=0}^{N}$,
%the goal of (supervised) deep learning, is to train the weights $\weights \in \R^{P}$ of an $L\text{-layer}$ NN
%$f : \inputDomain \rightarrow \outputDomain$ to minimize the (regularized) empirical risk,
%\begin{align} \label{eq-empirical-risk}
%  \weights^{*} = \arg \min_{\weights \in \R^{D}} \mathcal{L}(\dataset;\weights) = \arg \min_{\weights \in \R^{D}} \left(
%  \sum_{n=0}^{N-1} l(f(\mathbf{x}_{n} ; \weights), \mathbf{y}_{n}) + R(\weights)  \right).
%\end{align}
%In practice, it is common to use the mean squared error (MSE) loss
%$l(f(\mathbf{x}_{n} ; \weights),\mathbf{y}_{n}) = \|f(\mathbf{x}_{n} ; \weights) - \mathbf{y}_{n} \|^{2}_{2}$
%and to use the weight decay regularizer $R(\weights)=\frac{1}{2}\gamma^{-2}\|\weights\|^{2}_{2}$.
%However, the practioner is free to choose any loss and regularizer. \todo{I think?}
%% \textbf{Bayesian neural networks}
%% \textbf{Weight space to function space}
%From a Bayesian perspective, we can interpret the terms in \cref{eq-empirical-risk} as the log-prior and the i.i.d. log likelihood, i.e.,
%\begin{align} \label{eq-log-prior}
%  R(\weights) &= \underbrace{\log p(\weights)}_{\text{log prior}} = \log \mathcal{N}(\weights \mid 0, \gamma^{2} \mathbf{I}) \\
%  l(f(\mathbf{x}_{n} ; \weights), \mathbf{y}_{n}) &= \underbrace{\log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}; \weights))}_{\text{log likelihood}}
%  = \log \mathcal{N} \left( \mathbf{y}_{n} \mid f(\mathbf{x}_{n}; \weights), \mathbf{I} \right) \label{eq-log-likelihood}
%\end{align}
%
%\textbf{NN to GP}
%Given this interpretation, we can represent the weight space prior from \cref{eq-log-prior}, in
%function space, by linearising the BNN around $\weights^{*}$ and interpretting it as a GP.
%The GP prior is then given by,
%\begin{align} \label{eq-laplace-approx-function-space}
%  % \transitionFn_{\weights_{i}}(\cdot) \sim \mathcal{N} \left( \mu_{\weights_{i}^{*}}(\cdot), K_{\weights_{i}^{*}}(\cdot, \cdot') \right) \qquad
%  f(\cdot ;\weights) \sim \mathcal{N} \left( \mu(\cdot), k(\cdot, \cdot') \right) \quad
%  % \mu_{\weights_{i}^{*}}(\cdot)
%  \mu(\cdot)
%  = 0 \quad
%  % = f(\cdot ;\weights^{*}) \quad
%  % K_{\weights_{i}^{*}}(\cdot, \cdot')
%  k(\cdot, \cdot')
%  = \frac{1}{\gamma^{2}} \mathbf{J}(\cdot) \mathbf{J}(\cdot')^{T},
%\end{align}
%where the kernel is the neural tangent kernel (NTK) \citep{immerImprovingPredictionsBayesian2021},
%\todo{what mean function shoudl this use???}
%\todo{NTK times some factor??}
%with Jacobian given by $\mathbf{J}(\cdot) = \odv{\transitionFn(\cdot; \weights)}{\weights}_{\weights=\weights^{*}}$.
%% The kernel in \cref{eq-laplace-approx-function-space} is the neural tangent kernel (NTK)
%% $k(\mathbf{x}, \mathbf{x}') = \sigma_{0}^{2} J_{\weights^{*}}(\mathbf{x}) J_{\weights^{*}}(\mathbf{x}')^{T}$
%% \citep{immerImprovingPredictionsBayesian2021}
%% to formulate the GP posterior $p(f(\mathbf{X} ;\weights_{0:i}) \mid \dataset_{0:i})$.
%Given this GP prior, we can use \cref{eq-gp-predictive-posterior} to obtain our GP posterior by conditioning on the data.
%% Given our GP prior, we can use the properties of multivariate normals to obtain our GP posterior by conditioning on the data.
%% where $\mathbf{A} = \mathbf{k}_{*\mathbf{x}} \mathbf{K}^{-1}_{\mathbf{x}\mathbf{x}}$
%In contrast to conventional GP kernels, the NTK does not have any hyperparameters which need to be learned.
%Intuitively, we can view the optimisation in \cref{eq-empirical-risk} as learning our kernel.
%As a result, our NTK may be highly non stationary as it is dependent on the NN architecture, for example, the activation functions.
%It is worth noting that the NTK linearises the network around the optimised parameters $\weights^{*}$,
%so the function space prior (and thus posterior) is only a locally linear approximation.
%
%We now have a method to obtain a GP posterior from a trained DNN.
%% In contrast to other approaches, our formulation has not restricted us to using a weight-space posterior obtained from the Laplace approximation.
%In contrast to other approaches, our formulation has not restricted us to using a weight-space posterior from a BNN.
%However, in practice, the GP posterior is not very useful as it requires inverting an $N\times N$ matrix which has complexity $\mathcal{O}(N^{3})$.
%The SVGP in \cref{eq-dual-svgp-predictive-posterior} offers a solution to this problem but it is not immediately clear how to set $\mathbf{m}$ and $\mathbf{V}$.
%In the next section, we show how we can directly obtain an SVGP posterior (i.e. set $\mathbf{m}$ and $\mathbf{V}$) directly from both
%1) a trained NN and 2) a BNN weight-space posterior.
%% $\mathbf{m}$ and $\mathbf{V}$
%% However, it is not obvious how we can use a SVGP to set $\mathbf{m}$ and $\mathbf{V}$
%
%
%
%% In contrast, to previous work, this approach did not require us Laplace approximation. we will not restrict our
%% It is worth noting that the NTK linearises the network around the optimised parameters $\weights^{*}$,
%% so the function space prior (and thus posterior) is only a locally linear approximation.
%% In contrast to conventional GP kernels, the NTK does not have any hyperparameters which need to be learned.
%% It is also worth noting that the NTK may be highly non stationary and is dependent on the NN architecture, for example, the activation functions.
%
%
%\section{Methods}
%In the next section we show how we can obtain a SVGP directly from 1) a trained NN and 2) a BNN posterior.
%
%We follow \cite{csatoSparseOnlineGaussian2002} and express our GP posterior in the dual parameter space.
%However, we consider the variational formulation introduced by \cite{adamDualParameterizationSparse2021}, where the optimal variational parameters are given by,
%\begin{align} \label{eq-dual-params}
%\mathbf{m}^{*} = \mathbf{V}^{*}\bm\alpha^{*} \quad \mathbf{V}^{*} = [\mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} + \bm\beta^{*}]
%\end{align}
%Importantly, we can calculate the natural parameters $(\bm\alpha, \bm\beta)$ in closed form,
%\todo{paul needs to update \cref{eq-dual-svgp-params}}
%\begin{align} \label{eq-dual-svgp-params}
%\bm\alpha_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right] \\
%\bm\beta_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right].
%\end{align}
%This simplifies SVGP inference \citep{hensmanGaussian2013} as it no longer requires the variational parameters $(\mathbf{m}, \mathbf{V})$ to be optimised.
%
%% Importantly, \cite{changFantasizingDualGPs2022} show that in the dual space,
%% conditioning on new observations reduces to suming the dual variables from the previous time
%% step with an update, given by,
%% \todo{pick best way to show new data (using superscript new or just time indexing?)}
%% \begin{align} \label{eq-dual-update-svgp}
%% \bm\alpha_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right] \\
%% \bm\beta_{n} &=  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(f(\mathbf{x}_{n}))} \left[ \log p(\mathbf{y}_{n} \mid f(\mathbf{x}_{n}) ) \right]
%% \end{align}
%
%% We follow \cite{csatoSparseOnlineGaussian2002} and express our GP posterior in the dual parameter space,
%% % to write GP posterior $p(f(\mathbf{X} ;\weights_{0:i}) \mid \dataset_{0:i})$ as,
%% \begin{align} \label{eq-gp-post}
%%   \mathbb{E}_{p(f(\mathbf{x}_{*}) \mid \mathbf{y}_{n})} \left[ f(\mathbf{x}_{*}) \right] &= \mu(\mathbf{x}_{*}) + \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%%   \mathbb{V}_{p(f(\mathbf{x}_{*}) \mid \mathbf{y}_{n})} \left[ f(\mathbf{x}_{*}) \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \text{diag}(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
%% \end{align}
%% % \begin{align} \label{eq-laplace-approx-function-space}
%% %   \mathbb{E}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%% %   \mathbb{V}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \diag(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
%% % \end{align}
%% where the dual parameters $(\bm\alpha_n, \bm\beta_n)$ are vectors of,
%% \begin{align} \label{eq-gp-dual-params}
%%   \bm\alpha_{n} \coloneqq \left\{ \mathbb{E}_{p(f(\mathbf{x}_{i}) \mid \mathbf{y}_{n})} \left[ \nabla_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \right\}_{i=1}^{N} \qquad
%%   \bm\beta_{n} \coloneqq \left\{ \mathbb{E}_{p(f(\mathbf{x}_{i}) \mid \mathbf{y}_{n})} \left[ \nabla^{2}_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \right\}_{i=1}^{N}
%% \end{align}
%% Importantly, this parameterisation
%
%% \textbf{dual-SVGP}
%% \begin{align} \label{eq-svgp-post}
%%   \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ f(\mathbf{x}_{*}) \right] &= \mu(\mathbf{x}_{*}) + \mathbf{k}_{\mathbf{z}*}^{T} \bm\alpha_{\mathbf{u}} \\
%%   \mathbb{V}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ f(\mathbf{x}_{*}) \right] &= k_{**} - \mathbf{k}_{\mathbf{z}*}^{T} \left[ \mathbf{K}_{\mathbf{z}\mathbf{z}}^{-1} - \left( \mathbf{K}_{\mathbf{z} \mathbf{z}} + \bm\beta_{\mathbf{u}} \right)^{-1} \right] \mathbf{k}_{\mathbf{z}*}
%% \end{align}
%% % \begin{align} \label{eq-laplace-approx-function-space}
%% %   \mathbb{E}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \\
%% %   \mathbb{V}_{p(f_{*} \mid \mathbf{y}_{n})} \left[ f_{*} \right] &= k_{**} - \mathbf{k}_{\mathbf{x}*}^{T} \bm\alpha_{n} \left( \mathbf{K}_{\mathbf{x} \mathbf{x}} + \diag(\bm\beta_{n})^{-1} \right)^{-1} \mathbf{k}_{\mathbf{x}*}
%% % \end{align}
%% where the dual parameters $(\bm\alpha_{\mathbf{u}}, \bm\beta_{\mathbf{u}})$ are vectors of,
%% \begin{align} \label{eq-svgp-dual-params}
%%   \bm\alpha_{\mathbf{u}} \coloneqq \mathbf{k}_{\mathbf{z}i} \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ \nabla_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \qquad
%%   \bm\beta_{\mathbf{u}} \coloneqq \mathbf{k}_{\mathbf{z}i} \mathbb{E}_{q_{\mathbf{u}}(f(\mathbf{x}_{i}))} \left[ \nabla^{2}_{f_{i}} \log p(y_{i} \mid f(\mathbf{x}_{i})) \right] \mathbf{k}_{\mathbf{z}i}^{T}
%% \end{align}
%
%% weight space posterior in \cref{eq-laplace-approx-weight-space} in
%% function space by linearising the BNN and interpreting it as a GP.
%% \begin{align} \label{eq-laplace-approx-function-space}
%%   \transitionFn_{\weights_{i}}(\cdot) &\sim \mathcal{N} \left( \mu_{\weights_{i}^{*}}(\cdot), K_{\weights_{i}^{*}}(\cdot, \cdot') \right) \quad
%%   \mu_{\weights_{i}^{*}}(\cdot) &= \transitionFn_{\weights^{*}_{i}}(\cdot) \quad
%%   K_{\weights_{i}^{*}}(\cdot, \cdot') &= \mathbf{J}_{\weights^{*}_{i}}(\cdot) \bm\Sigma_{\weights^{*}_{i}} \mathbf{J}_{\weights^{*}_{i}}(\cdot')^{T}
%% \end{align}
%% where the Jacobian is given by $\mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}$.
%
%% \begin{align} \label{eq-laplace-approx-jacobian}
%% \mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}
%% \end{align}
%
%
%% % Note that the posterior in \cref{eq-laplace-approx-weight-space} is in weight space.
%% We can represent the weight space posterior in \cref{eq-laplace-approx-weight-space} in
%% function space by linearising the BNN and interpreting it as a GP.
%% \begin{align} \label{eq-laplace-approx-function-space}
%%   \transitionFn_{\weights_{i}^{*}}(\cdot) &\sim \mathcal{N} \left( \mu_{\weights_{i}^{*}}(\cdot), K_{\weights_{i}^{*}}(\cdot, \cdot') \right) \quad
%%   \mu_{\weights_{i}^{*}}(\cdot) &= \transitionFn_{\weights^{*}_{i}}(\cdot) \quad
%%   K_{\weights_{i}^{*}}(\cdot, \cdot') &= \mathbf{J}_{\weights^{*}_{i}}(\cdot) \bm\Sigma_{\weights^{*}_{i}} \mathbf{J}_{\weights^{*}_{i}}(\cdot')^{T}
%% \end{align}
%% where the Jacobian is given by $\mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}$.
%
%% \begin{align} \label{eq-laplace-approx-jacobian}
%% \mathbf{J}_{\weights^{*}_{i}}(\cdot) = \odv{\transitionFnWithParams(\cdot)}{\weights}_{\weights=\weights_{i}^{*}}
%% \end{align}
%
%
%\subsection{Fast Updates}
%Importantly, \cite{changFantasizingDualGPs2022} show that in the dual space,
%conditioning on new observations reduces to suming the dual variables from the previous time
%step with an update, given by,
%\todo{pick best way to show new data (using superscript new or just time indexing?)}
%\begin{align} \label{eq-dual-update-svgp}
%\dualParam{1}^{t+1} &\leftarrow \dualParam{1}^{t} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))} \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right] \\
%\dualParam{2}^{t+1} &\leftarrow \dualParam{2}^{t} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))}  \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right]
%\end{align}
%Importantly, for a single new observation $((\state_{t}, \action_{t}), \state_{t+1})$ this update has
%complexity $\mathcal{O}(\numInducing^{2})$.
%This is a significant improvement to naive GP conditioning, which has complexity $\mathcal{O}((\numDataNew + \numDataOld)^{3})$
%and sparse GP conditioning, which has complexity $\mathcal{O}((\numDataNew + \numDataOld)\numInducing^{2})$.
%\todo{double check these complexities are right and cite them/show equaitons}
%It is worth highlighting that the complexity of \cite{changFantasizingDualGPs2022} update does not increase during an episode.
%
%\subsubsection{Model-based RL}
%
%\subsubsection{Efficiently Sampling Functions for Posterior Sampling}
%\cite{wilsonEfficiently2020}
%\cite{wilsonPathwise2021}
%
%% \begin{align} \label{eq-dual-update-svgp}
%%  \dualParam{1}^{\text{new}} &\leftarrow \dualParam{1}^{\text{old}} +
%%   \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}))}
%%  \left[ \log p(\state_{t+1}^{\text{new}} \mid \latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}) ) \right] \\
%%  \dualParam{2}^{\text{new}} &\leftarrow \dualParam{2}^{\text{old}} +
%%   \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}))}
%%  \left[ \log p(\state_t^{\horizon+1} \mid \latentFn(\state_t^{\text{new}}, \action_t^{\text{new}}) ) \right] \\
%%  \dualParam{1}^{\horizon+1} &\leftarrow \dualParam{1}^{\horizon} +
%%   \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
%%  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right] \\
%%  \dualParam{2}^{\horizon+1} &\leftarrow \dualParam{2}^{\horizon} +
%%   \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{\horizon}, \action_{\horizon}))}
%%  \left[ \log p(\state_{\horizon+1} \mid \latentFn(\state_{\horizon}, \action_{\horizon}) ) \right]
%% \end{align}
%
%
%\begin{assumption} \label{assumption-ntk-linearisation}
%  Something about NTK being a linearisation around $\theta^{*}_{i}$ but each update moves away from $\theta^{*}_{i}$
%\end{assumption}
%
%\cite{rossellApproximateLaplaceApproximations2021}



\section{Experiments}
\label{sec:experiments}

\todo{Overview of experiments}

Our experiments seek to answer the following questions:
\begin{enumerate}
  \item \textbf{Predictions} How do predictions with our sparse function-space approximation compare to weight-space approximations? Does our method's ability to consider the full data set offer benefits over subset function-space methods?
  % \item Does our method's ability to consider the full data set offer benefits over subset function-space methods?
  \item \textbf{Function-space updates} How fast are our function-space updates relative to retraining from scratch? Do they improve predictive performance? Are they as good as retraining from scratch?
  \item \textbf{Uncertainty} How good are our uncertainty estimates? Can they be used in downstream settings like RL?
  \item \textbf{Representation} Is our sparse function-space representation useful for continual learning?
\end{enumerate}

\subsection{Capturing uncertainty in UCI tasks under supervised learning}
%


\begin{table}[t!] 
  \centering\scriptsize
  \caption{Negative log predictive density (NLPD) (lower better) for the proposed model TODO} 
	\label{tbl:uci}
	% Control table spacing
	\renewcommand{\arraystretch}{1.}
	\setlength{\tabcolsep}{2pt}
	\setlength{\tblw}{0.14\textwidth}  
	
	% Custom error formatting
	\newcommand{\val}[2]{%
		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
	} 

    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
	\input{tables/uci.tex}
\end{table}
We trained a two-layer MLP for each of the $8$ classification tasks in UCI, see \cref{tbl:uci} for a list of the datasets. The neural network training was done using MAP, with a prior $\mathcal{N}(0,\delta^{-1} \MI)$ with prior precision $\delta$. After training, we constructed the SVGP dual and used the resulting SVGP model for uncertainty quantification. Depending on task dimension and complexity, our model can match the NN MAP in NLPD performance even with a low number of inducing points, see \cref{tbl:uci}. We performed hyperparameter search over the prior precision, and ran the experiment over $10$ seeds. During the initial NN training, we used a learning rate of $1e-3$.



\subsection{Supervised learning on image data sets}


\begin{table}[t!] 
  \centering\scriptsize
  \caption{Metrics for supervised learning with image data. SVGP is our method, and the number inside parentheses is the number of inducing points. SVGP NN is a modification of our method, where the mean is from the NN directly and the variance is from the GP model} 
	\label{tbl:imagesuper}
	% Control table spacing
	\renewcommand{\arraystretch}{1.}
	\setlength{\tabcolsep}{6pt}
	\setlength{\tblw}{0.15\textwidth}  
	
	% Custom error formatting
	\newcommand{\val}[2]{%
		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
	} 

    % THE TABLE CAN BE FORMATTED MORE LIKE THIS
    \begin{tabular}{l l C{\tblw} C{\tblw} C{\tblw} C{\tblw}}
    \toprule
    & Method & ACC~$\uparrow$ & NLPD~$\downarrow$ & ECE~$\downarrow$ & OOD-AUC~$\uparrow$  \\
    \midrule
    \multirow{2}{*}{FMNIST} 
    & MAP & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & BNN predictive & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & BNN predictive \cite{todo} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & GLM predictive & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & GP predictive & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & \our (NN) & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & \our & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\    
    \midrule
    \multirow{2}{*}{CIFAR-10} 
    & MAP & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & BNN predictive & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & BNN predictive \cite{todo} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & GLM predictive & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & GP predictive & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & \our (NN) & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\
    & \our & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} & \val{0.000}{0.000} \\    
    \bottomrule
    \end{tabular}

    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
	%\input{tables/img_super.tex}
\end{table}

Similarly to the UCI experiments, we trained neural networks (CNN, architexture matches \citet{immer2021improving}) for the FMNIST and CIFAR10 classification tasks. After training, we computed the duals for NN2SVGP and used the resulting SVGP for uncertainty estimates, see \cref{tbl:imagesuper}. We performed hyperparameter optimization over the prior precision, and found the optimal value to be [insert]. We ran the experiment over $5$ seeds. During the NN training, we used a batch size of $512$ and an Adam optimizer with learning rate $1e-3$.

In addition to uncertainty estimates, our method can update the posterior when given new data. We gave the SVGP model 10 \% of the test dataset used for evaluation, and compare the performance of the SVGP model and the retrained NN in [INSERT TABLE]. 

\subsection{Updating the  network representation for Continual learning}
The Continual learning setting is a challenging and the training and


\begin{table}[t!] 
  \centering\scriptsize
  \caption{
  TODO: CL Experiments. $^*$ Methods relying only on weight regularization. 
  \color{blue}{Results from \cite{rudner2022continual}.} 
  %\color{red}{Preliminary results need more runs or hyperparam tuning}
  }
	\label{tbl:cl_table_1}
	% Control table spacing
	\renewcommand{\arraystretch}{1.}
	\setlength{\tabcolsep}{2pt}
	\setlength{\tblw}{0.14\textwidth}  
	
	% Custom error formatting
	\newcommand{\val}[2]{%
		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
	} 
	
	\input{tables/cl_table_1}
\end{table}


\subsection{Reinforcement learning}
A key challenge in RL is balancing the trade-off between exploration and exploitation.
That is, should an agent select actions that it knows will lead to a high reward (exploitation), or should it
hope to discover new actions which lead to high reward (exploration)?
In this section, we show that \our's uncertainty estimates can be used to balance this trade-off in a model-based RL algorithim,
demonstrating the quality of our uncertainty estimates.
% In this section, we demonstrate  that our method's principled uncertainty estimates can be used to balance this trade-off in a model-based RL algorithim.
We refer the reader to \cref{app:rl} for an overview of the problem and details of our experiments.


Cart pole swing up task in MuJoCo \cite{todorov2012mujoco}
We compare our method with deep deterministic policy gradient (DDPG) \cite{lillicrapContinuousControlDeep2016} -- a model-free RL baseline --
which is sample inefficient.
We test our model's uncertainty estimates by comparing it to using Laplace-GGN with GLM predictions, using an ensemble of neural networks
(with different initialisations),
To ensure a fair comparison, we use the same MLP architecture/training scheme and we keep the same model-based RL algorithm.
See \cref{app:rl} for more details of our experiments.

We also compare to deep deterministic policy gradient (DDPG) \cite{lillicrapContinuousControlDeep2016} -- a model-free RL baseline --
which we expect to be much more sample inefficient.


\cref{fig:rl} shows training curves





\begin{figure}[!t]
 \centering
 \begin{subfigure}[c]{.24\textwidth}
 \resizebox{\textwidth}{!}{%
 \begin{tikzpicture}[inner sep=0,outer sep=0]

   % Draw decorated 'ground'
   \draw[postaction={draw, decorate, decoration={border, angle=-45,
					amplitude=1cm, segment length=.5cm}}] (-3,-1.5) -- (12,-1.5);

   % The cart
   \draw[draw=black,fill=black!50,draw=black,line width=3pt,rounded corners=1mm] (0,0) rectangle (9cm,3cm);
   \node[fill=black,circle,minimum size=.5cm] (dot) at (4.5cm,3cm) {};

   % Wheels
   \node[fill=white,draw=black,line width=3pt,circle,minimum size=2cm,,fill=black!50] at (2cm,-.5cm) {};
   \node[fill=white,draw=black,line width=3pt,circle,minimum size=2cm,fill=black!50] at (7cm,-.5cm) {};

   % The arm
   \node[anchor=north,minimum width=1cm,minimum height=14cm,draw=black,rotate=-20,rounded corners=5mm,yshift=7mm,xshift=-.3mm,fill=white,draw=black,line width=3pt] at (dot) {};
   \node[fill=black,circle,minimum size=.5cm] at (dot) {};

   % Markings
   \draw[loosely dashed,line width=1pt] (4.5,6) -- (4.5,-10);

   % Arrow
   \draw[->,black,line width=3pt,-{Latex[length=7mm,width=7mm]}] (1,5) --node[above,outer sep=8pt]{\scalebox{4}{$x$}} (4.5,5);

   \def\centerarc[#1](#2)(#3:#4:#5)% Syntax: [draw options] (center) (initial angle:final angle:radius)
   { \draw[#1] ($(#2)+({#5*cos(#3)},{#5*sin(#3)})$) arc (#3:#4:#5); }

   % Draw arc
   \centerarc[black,line width=3pt](dot)(250:270:11)
   \node at (2.5,-9) {\scalebox{4}{$\theta$}};

   %\node[white,minimum size=1cm] at (4.5,-13) {};

 \end{tikzpicture}}
 \end{subfigure}
 \hfill
 \begin{subfigure}[c]{.74\textwidth}
   \centering\scriptsize
   \setlength{\figurewidth}{\textwidth}
   \setlength{\figureheight}{.5\figurewidth}
   \pgfplotsset{axis on top,ymajorgrids,axis line style={draw=none},legend style={at={(1,1)},anchor=north west}}
   \pgfplotsset{grid style={line width=.1pt, draw=gray!10,dashed},legend style={fill=white}}
   \input{./fig/rl.tex}
 \end{subfigure}
 \hfill
 \caption{\textbf{Reinforcement learning experiments} Training curves showing that our model's uncertainty estimates improve sample efficiency in RL.
   Our method (blue/green) converges to the optimal solution in fewer environment steps than the baseline model-based RL method (magenta) and (DDPG) the model-free baseline  (yellow).}
 \label{fig:rl}
\end{figure}

%


% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\textwidth, angle=270]{fig/weight-space-to-functio-space.pdf}
%   \caption{}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\textwidth, trim=0 100 0 10]{fig/cartpole-training-curves.pdf}
%   \caption{}
% \end{figure}

% \begin{table}
%   \caption{Negative test log likelihood (lower is better) on UCI classification tasks (2 hidden layers, 50 tanh). Our SVGP predictive outperforms the GLM predictive. }
% \end{table}

\section{Discussion and conclusion}
\label{sec:conclusion}
%



It is worth noting that the NTK linearises the network around the MAP solution $\weights^{*}$,
so the function space prior (and thus posterior) is only a locally linear approximation.
In contrast to conventional GP kernels, the NTK does not have any hyperparameters which need to be learned.
It is also worth noting that the NTK may be highly non stationary and is dependent on the NN architecture, for example, the activation functions.






A reference implementation of the methods presented in this paper is currently available as supplementary material and will be made available under the MIT License on GitHub upon acceptance.


%\section*{Broader Impact}

% \section*{References}
%\small
%\printbibliography
%\normalsize
% TODO make bibliography small a better way

%References follow the acknowledgments. Use unnumbered first-level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
%when listing the references.
%Note that the Reference section does not count towards the page limit.
%\medskip

\clearpage


\phantomsection%
\addcontentsline{toc}{section}{References}
\begingroup
\small
\bibliographystyle{abbrvnat}
\bibliography{bibliography}%zotero-library
\endgroup

\clearpage

\nipstitle{
    {\Large Supplementary Material:} \\
    Sparse Function-space Representation \\ of Neural Networks}
\pagestyle{empty}

\appendix

This supplementary document is organized as follows. 
%
\cref{app:method} provides a more extensive overview of the techncical details of \our.
%
\cref{app:cl} covers the sparse-functional regularisation (SFR) setup for continual learning.
%
\cref{app:rl} provides a more extensive writeup of the reinforcement learning setup used in the experiments.
%
\cref{app:experiments} provides the full details of each individual experiments in the main paper.


\section{Method details}
\label{app:method}
%
Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.



\section{\our for continual learning}
\label{app:cl}
\subsection{Derivation of the regularizer term}
An alternative view of the dual variables is that they parameterize approximate likelihoods, as shown in \citep{adam2021dual, khan2017conjugate}. We can rewrite the GP posterior for $q(\vu)$ in terms of the following approximate normal distributions:
\begin{equation}
 q(\vu) \propto \Norm(0, \MKzz) \prod_{i=1}^n \exp \! \left(-\frac{\tilde{\beta}_i}{2}(\tilde{y}_i - \va_i^\top \vu)^2 \right) = \Norm(0, \MKzz) \, \exp\left((\bar{\vy} - \vu)^\top \bar{\MSigma}^{-1}(\bar{\vy} - \vu)\right)  
\end{equation}
The first normals are the approximate likelihoods where $\va_i^\top = \vkzi^\top \MKzz^{-1}$ and $\tilde{y}_i = \hat{\alpha}_i + f_{\vw^*}(\vx_i)\hat{\beta}$,  replacing $\va_i^\top \vu$ with $\vx_i^{\top}\vw$ and we can see the similarity between the sparse GP and a linear model where the kernel matrix determines the prior. To arrive at the MVN Gaussian, we need to rearrange the quadratic terms to match the prior sufficient statistics we present a derivation in [appendix]. Performing conjugate summation of prior and likelihood, we would arrive at the sparse GP posterior at the inducing points. The correlated covariance structure $\MSigma$ and the corresponding $\tilde{\vy}$ are simply a different form of the sparse dual variables,
\begin{equation}
\quad \bar{\MSigma} =  \MKzz \vbeta_\vu^{-1} \MKzz \quad \bar{\vy} = \MKzz \vbeta_{\vu}\valpha_{\vu}.
\end{equation}
Written as above, we can see that the spare dual variables can be interpreted as a sparse MVN representation. We now show how we use this for a regularizer in continual learning with more details in [appendix].

\subsection{Extending the CL regularizer to multi-class settings}
\begin{equation}
	\bar{\MB}^{-1}_s = \MKzz^{-1} \vbeta_\vu \MKzz^{-1} \in \R^{C \times m \times m} \quad \MKzz \in \R^{C \times m \times m} 
\end{equation}

\begin{equation}
	\mathcal{R_\textit{SFR}}(\mathbf{w}) = \sum_{s=1}^{t-1}	\sum_{k \in 	C}\left[\left(f_{\vw, k}(\MZ_{s}) - f_{\vw_{s}, k}(\MZ_s) \right)^\T \bar{\MB}^{-1}_{s} \left(f_{\vw, k}(\MZ_{s}) - f_{\vw_{s, k}}(\MZ_s) \right) \right] 
\end{equation}





\section{Sparse functional model-based reinforcement learning}
\label{app:rl}

We consider environments with states \(\state \in \stateDomain \subseteq \R^{D_{\state}} \),
actions \(\action \in \actionDomain \subseteq \R^{D_{\action}}\) and transition dynamics
\(\transitionFn: \stateDomain \times \actionDomain \rightarrow \stateDomain \), such that
$\state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t}$, where  $\noise_{t}$
is i.i.d. transition noise.
We consider the episodic setting where the system is reset to an initial state $\state_{0}$ at each episode and we
assume that there is a known reward function $r : \stateDomain \times \actionDomain \rightarrow \R$.
% Following the Markov decision process formulation \cite{bellmanMarkovianDecisionProcess1957a}, we
% denote the states \(\state \in \stateDomain \subseteq \R^{D_{\state}} \) and actions \(\action \in \actionDomain \subseteq \R^{D_{\action}}\)
% of the sytem, the reward function $r : \stateDomain \times \actionDomain \rightarrow \R$, and transition dynamics
% \(\transitionFn: \stateDomain \times \actionDomain \rightarrow \stateDomain \), such that
% $\state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t}$ where  $\noise_{t}$
% is i.i.d. transition noise.
% $\mathbb{E}_{\noise_{0:\infty}} \big[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \big]$
% \begin{align} \label{eq-model-free-objective}
% \policy^{*} = \arg \max_{\policy \in \policyDomain} J(\transitionFn, \policy) = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\noise_{0:\infty}} \bigg[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \bigg]
% \quad \text{s.t. } \state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t},
% \end{align}
The goal of RL is to find the policy \(\pi : \stateDomain \rightarrow \actionDomain\)
(from a set of policies $\Pi$) that maximises the sum of discounted rewards
in expectation over the transition noise,
\begin{align} \label{eq-model-free-objective}
J(\transitionFn, \policy) = \mathbb{E}_{\noise_{0:\infty}} \bigg[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \bigg]
\quad \text{s.t. } \state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t},
\end{align}
where $\gamma \in [0, 1)$ is a discount factor.
In this work, we consider model-based RL where a model of the transition dynamics is learned \(f_{\mathbf{w}} \approx \transitionFn\) and then used by a planning algorithm.
A simple approach is to use the learned dynamic model $f_{\mathbf{w}^{*}}$ and maximise the objective in \cref{eq-model-free-objective}.
% \begin{align} \label{eq-greedy}
%   \policy^{\text{Greedy}} &= \arg \max_{\pi \in \Pi} J(f_{\mathbf{w}^{*}}, \pi).
% \end{align}
However, we can leverage the method in \cref{sec:methods} to obtain a function-space posterior over the learned dynamics $q_{\mathbf{u}}(\hat{\transitionFn} \mid \dataset)$,
where $\mathcal{D}$ represents the state transition data set \(\mathcal{D} = \{(s_{i},a_{i}), s_{i+1}\}_{i=0}^{N}\).
Importantly, the uncertainty represented by this posterior distribution can be used to balance the exploration-exploitation trade-off,
using approaches such as posterior sampling \cite{osbandWhyPosteriorSampling2017,osbandMoreEfficientReinforcement2013},
\begin{align} \label{eq-posterior-sampling}
  \policy^{\text{PS}} &= \arg \max_{\pi \in \Pi} J(\hat{f}, \pi)
\quad \text{s.t. } \tilde{\transitionFn} \sim q_{\mathbf{u}}(\hat{\transitionFn} \mid \dataset),
\end{align}
where a function $\tilde{\transitionFn}$ is sampled from the (approximate) posterior $q_{\mathbf{u}}(\hat{\transitionFn} \mid \dataset)$ and used to find a policy as
in \cref{eq-greedy}.
Intuitively, this strategy will explore where the model has high uncertainty, which in turn will reduce the model's uncertainty as data is collected and used to
train the model.
See \cref{sec-rl-appendix} for more details of how we use

\todo{could try to implement Pathwise conditioning in function-space}


Model Predictive Path Integral (MPPI) control
\cite{panSample2015}
\cite{williamsModel2017}
\todo{what is correct citation for MPPI?}

% \cref{alg-mbrl} shows the typical model-based RL loop.
\begin{align} \label{eq-fast-update-mpc}
  \policy_{i+1}^{\text{PS}}(\state) = \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
\E \bigg[ \sum_{t=0}^{H-1} \gamma^{t} r(\state_{t},\action_{t}) \mid \state_{0}=\state  \bigg] + Q_{\theta}(\state_{\Horizon}, \action_{H})
\quad \text{s.t. } \state_{t+1} &= \hat{\transitionFn}(\state_{t}, \action_{t}) + \noise_{t}
\end{align}

with $\hat{\transitionFn} \sim p(\transitionFn \mid \dataset)$

We use deep deterministic policy gradient (DDPG) \cite{lillicrapContinuousControlDeep2016} to learn an action value function $Q_{\theta}$.
Note that we also learn a policy but its sole purpose is for learning the value function.

\subsection{Experiment Configuration}
This section details how we configured and ran our reinforcement learning experiments.

\textbf{Dynamic model}
In all experiments we used an MLP dynamic model with a single hidden layer of width 64 and TanH activation functions.
At each episode we used Adam \cite{adam} to optimise the NN parameters for $5000$ iterations with a learning rate of $0.001$.
We reset the optimizer after each episode.
As we are performing regression we instantiate the loss function in \cref{eq-empirical-risk} as the well-known mean squared error.
This corresponds to a Gaussian likelihood with unit variance.
We then set the prior precision as $\delta=0.0001$.
% It is worth noting that $\delta$ effects both the neur
In all experiments our sparse function-space representation uses $m=128$ inducing points and this seemed to be sufficient.



\textbf{Model predictive path integral (MPPI)}
MPPI is an online planning algorithim which iteratively improves the action trajectory $\action_{t:t+H}$ using samples.
At each iteration $j$, $N=256$ trajectories are sampled according to the currect action trajectory $\action^{j}_{t:t+H}$.
The $K=32$ top trajectories with highest returns $\sum_{h=0}^{H} = r(\state^{j}_{t+h}, \action^{j}_{t+h})$ are selected.
The next action trajectory $\action^{j+1}_{t:t+H}$ is then computed by taking the weighted average of the top $K=32$ trajectories
with weights from the softmax over returns from top $K=32$ trajectories.

$\gamma 0.9$
$\tau=0.005$
horizon $H=5$
temperature $0.5$
momentum $0.1$

\textbf{Initial data set}
We collect an initial data set using a random policy for one episode.

\textbf{DDPG}
action value function is an MLP with a single hidden layer of width $128$ with ELU activation functions.
We train the DDPG agent using Adam for $500$ iterations at each episode, using a learning rate $0.0001$.




\section{Experiment details}
\label{app:experiments}


\subsection{Uncertainty quantification on UCI data sets}
\label{app:uci}



\subsection{Uncertainty benchmarks for image data}
\label{app:image}


\subsection{\our for continual learning}
\label{app:cl-experiment}
\section{Extending the CL regularizer to multi-class settings}
\label{sec:cl_multioutput}
\begin{equation}
	\bar{\MB}^{-1}_s = \MKzz^{-1} \vbeta_\vu \MKzz^{-1} \in \R^{C \times m \times m} \quad \MKzz \in \R^{C \times m \times m} 
\end{equation}

\begin{equation}
	\mathcal{R_\textit{SFR}}(\mathbf{w}) = \sum_{s=1}^{t-1}	\sum_{k \in 	C}\left[\left(f_{\vw, k}(\MZ_{s}) - f_{\vw_{s}, k}(\MZ_s) \right)^\T \bar{\MB}^{-1}_{s} \left(f_{\vw, k}(\MZ_{s}) - f_{\vw_{s, k}}(\MZ_s) \right) \right] 
\end{equation}


\subsection{Reinforcement learning experiment}
\label{app:rl-experiment}




%
%\subsection{Model-based Reinforcement Learning (RL)}
%The goal of reinforcement learning is to find a policy \(\pi \in \Pi\) that maximises the sum of discounted
%rewards in expecation under the transition noise (aleatoric uncertainty),
%\begin{align} \label{eq-model-free-objective}
%\policy^{*} = \arg \max_{\policy \in \policyDomain} J(\transitionFn, \policy) = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\noise_{0:\infty}} \left[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \right],
%\end{align}
%where $\gamma \in [0, 1]$
%
%\textbf{Model-based}
%In Bayesian model-based RL, we obtain the posterior over the dynamics \(p(f\mid\mathcal{D})\) after performing (approximate) Bayesian
%inference given a state transition data set \(\mathcal{D} = \{\{(s_{t},a_{t}), s_{t+1}\}^{T_{i}}_{t=1}\}_{i=0}^{N}\).
%\cref{alg-mbrl} shows the typical model-based RL loop.
%Importantly, the dynamics are usually only updated after an episode $i$.
%
%\begin{algorithm}[!b]
%\caption{Model-based RL}\label{alg-mbrl}
%\begin{algorithmic}[1]
%  \Require Start state $\state_{0}$, initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%\For{$i  \in \{1, 2, \ldots, \text{num episodes} \}$}
%    \State Reset the system to $\state_{0}$ and reset trajectory buffers $\bm\tau_{t} = \emptyset \ \forall t$
%    \For{$t  \in \{1, 2, \ldots, \text{num steps} \}$}
%      % \State Collect  $\tau_{0:t} = \tau_{0:t-1} \cup (\state_{j}, \action_{j}, \state_{j+1}, r_{j+1})$
%      \State Use \cref{eq-greedy}/\cref{eq-posterior-sampling}/\cref{eq-ucrl} to collect data $\bm\tau_{t} = \bm\tau_{t-1} \cup (\state_{t}, \policy_{i}(\state_{t}), \transitionFn(\state_{t}, \policy_{i}(\state_{t})), r_{t+1})$
%      % \State Execute policy $\policy_{i}(\state_{t})$ in environment and update trajectory $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%    \EndFor
%    \State Update data set $\dataset_{0:i} = \dataset_{0:i-1} \cup \tau$
%    \State Train dynamics $p(\transitionFn \mid \dataset_{0:i}) \leftarrow \text{update\_dynamics}(\dataset_{0:i}, p(\transitionFn \mid \dataset_{0:i-1}))$
%    % \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$ using $\dataset_{0:i+1}$
%    % \State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%    \State Improve policy $\pi_{i+1} \leftarrow \text{update\_policy}(p\left(\transitionFn \mid \dataset_{0:i}), \dataset_{0:i} \right)$
%    %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%\EndFor
%\end{algorithmic}
%\end{algorithm}
%
%% \begin{minipage}{0.499\textwidth}
%% \begin{algorithm}[H]
%% \caption{Model-based RL}\label{alg-mbrl}
%% \begin{algorithmic}[1]
%%   \Require Initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%% \For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
%%     \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
%%       \State Execute policy $\policy_{i}(\state_{t})$ in environment
%%       \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%%     \EndFor
%%     \State Update data set $\mathcal{D}_{0:i+1} = \mathcal{D}_{0:i} \cup \tau_{i+1}$
%%     \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$
%%     \State Improve policy $\pi_{i+1}$
%%     %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%% \EndFor
%% \end{algorithmic}
%% \end{algorithm}
%% \end{minipage}
%% \hfill
%% \begin{minipage}{0.499\textwidth}
%% \begin{algorithm}[H]
%% \caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
%% \begin{algorithmic}[1]
%%   \Require Initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$, policy $\policy_{0}$
%%     % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
%% \For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
%%     \For{$t  \in \{0, 1, \ldots, \text{num steps} \}$}
%%       \State Execute policy $\policy_{i}(\state_{t})$ in environment
%%       % \State Append transition $\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$ to trajectory $\tau_{i}$
%%       \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%%       \State {\color{blue}Update dynamics $p(\transitionFn \mid \dataset_{0:i} \cup \tau_{i+1})$}
%%     \EndFor
%%     \State Update data set $\mathcal{D}_{0:i+1} = \mathcal{D}_{0:i} \cup \tau_{i+1}$
%%     \State Train dynamics $p(\transitionFn \mid \dataset_{0:i+1})$
%%     \State Improve policy $\pi_{i+1}$
%%     %\State Improve policy $\pi_{i+1}$ using $p(\transitionFn \mid \dataset_{0:i+1})$ and/or $\dataset_{0:i+1}$
%% \EndFor
%% \end{algorithmic}
%% \end{algorithm}
%% \end{minipage}
%
%
%
%\subsection{Exploration Strategies}
%\textbf{Greedy exploitation}
%Given the posterior dynamics \(p(\transitionFn \mid \mathcal{D})\),
%a common approach is to simply take the expecation over both the aleatoric and epistemic uncertainty,
%\begin{align} \label{eq-greedy}
%\policy_{i+1}^{\text{greedy}} = \arg \max_{\policy \in \policyDomain} \mathbb{E}_{\transitionFn \sim p(\transitionFn \mid \dataset_{0:i})} \left[ J(\transitionFn, \policy) \right],
%\end{align}
%This approach has been widely adopted, for example, in PILCO, PETS, GP-MPC
%\cite{deisenrothPILCO2011,chuaDeepReinforcementLearning2018,kamtheDataEfficient2018}.
%This approach helps to alleviate model bias as the posterior ``knows what the model does not know''.
%This is because the predictive posterior \(p(f(s_{t},a_{t}) \mid (s_{t},a_{t}),  \mathcal{D} )\) will be (or should be) uncertain when making
%predictions far away from the training data.
%The expectation considers all possible dynamics models which prevents the policy optimisation from
%exploiting innacuracies in the model.
%This approach has no guarantees for exploration in the general case.
%However, under specific dynamics and reward structures (e.g. PILCO) this objective can achieve sublinear regret.
%\todo{need to double check sublinear regret statement. And give a reference}
%
%
%\textbf{Posterior sampling}
%\cite{osbandWhyPosteriorSampling2017,osbandMoreEfficientReinforcement2013}
%\begin{align} \label{eq-posterior-sampling}
%\policy_{i+1}^{\text{PS}} = \arg \max_{\policy \in \policyDomain} \left[ J(\transitionFn, \policy) \right] \quad \text{s.t. } \transitionFn \sim p(\transitionFn \mid \dataset_{0:i})
%\end{align}
%
%\textbf{Hallucinated upper confidence RL}
%A more theoretically grounded exploration strategy is UCRL \citep{jakschNearoptimal2010}, which optimises joinly over
%policies and models inside the set
%\(\mathcal{M} = \{ f \mid | f(s,a) - \mu_{i}(s, a) | \leq \beta_{i} \Sigma_{i}(s, a) \quad \forall s, a \in \mathcal{S} \times \mathcal{A} \}\), representing all statistically plausible
%models under the posterior \(p(f(s,a) \mid \mathcal{D}_{0:i} \cup (s,a)) = \mathcal{N}(f(s,a) \mid \mu_{i}(s,a), \Sigma_{i}(s,a))\) at episode \(i\).
%This strategy is given by,
%\begin{align} \label{eq-ucrl}
%\policy_{i+1}^{\text{UCRL}} = \arg \max_{\policy \in \policyDomain} \max_{\transitionFn \in \mathcal{M}} J(\transitionFn, \policy).
%\end{align}
%This strategy optimises an optimistic policy over the set of plausible dynamics models.
%Although this joint optimisation is intractable in general,
%\cite{curiEfficient2020} proposed a practical alternative which is detailed in \cref{sec-hucrl}.
%
%\textbf{MPC vs policy learning}
%It is worth noting that the strategies in \cref{eq-greedy,eq-posterior-sampling,eq-ucrl} can be used with both model predictive control (MPC)
%techniques, such as the cross entoropy method (CEM), and model-free RL techniques, such as soft actor-critic (SAC).
%
%
%In this work we are interested in how we can use \(p(f \mid \mathcal{D})\) to alleviate some of the issues in model-based RL,
%for example, model bias and the exploration-exploitation trade-off.
%
%
%
%\todo{show how to get new $\dualParam{1}$ and $\dualParam{2}$ in Train dynamics line of \cref{alg-mbrl-fast-updates}}
%\begin{algorithm}[!t]
%\caption{Model-based RL with fast updates}\label{alg-mbrl-fast-updates}
%\begin{algorithmic}[1]
%  \Require Start state $\state_{0}$, initial data set $\dataset_{0}$, dynamics posterior $p(\transitionFn \mid \dataset_{0})$ (inc. dual parameters $\dualParam{1}, \dualParam{2}$), policy $\policy_{0}$
%    % ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
%\For{$i  \in \{1, 2, \ldots, \text{num episodes} \}$}
%    \State Reset the system to $\state_{0}$ and reset trajectory buffers $\bm\tau_{t} = \emptyset \ \forall t$
%    \For{$t  \in \{1, 2, \ldots, \text{num steps} \}$}
%      % \State Execute policy $\policy_{i}(\state_{t})$ (\cref{eq-fast-update-mpc}) in environment
%      \State Use \cref{eq-fast-update-mpc} to collect data $\bm\tau_{t} = \bm\tau_{t-1} \cup (\state_{t}, \policy_{i}(\state_{t}), \transitionFn(\state_{t}, \policy^{\text{fast}}_{i}(\state_{t})), r_{t+1})$
%      % \State Append transition $\state_{t}, \action_{t}, \state_{t+1}, r_{t+1})$ to trajectory $\tau_{i}$
%      % \State $\tau_{i+1} = \{\state_{j}, \action_{j}, \state_{j+1}, r_{j+1}) \}_{j=0}^{t}$
%      %\State {\color{blue}Update dynamics $p(\transitionFn \mid \dataset_{0:i} \cup \tau_{i+1})$}
%      \State {\color{blue}Update dynamics posterior using \cref{eq-dual-update-svgp}, i.e. fast update}
%      % \begin{align}
%      % \dualParam{1}^{t+1} &\leftarrow \dualParam{1}^{t} +  \nabla_{\meanParam{1}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))} \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right] \\
%      % \dualParam{2}^{t+1} &\leftarrow \dualParam{2}^{t} +  \nabla_{\meanParam{2}} \mathbb{E}_{q_{\inducingVariable}(\latentFn(\state_{t}, \action_{t}))}  \left[ \log p(\state_{t+1} \mid \latentFn(\state_{t}, \action_{t}) ) \right]
%      % \end{align}}
%    \EndFor
%    \State Update data set $\dataset_{0:i} = \dataset_{0:i-1} \cup \tau$
%    \State Train dynamics $p(\transitionFn \mid \dataset_{0:i}) \leftarrow \text{update\_dynamics}(\dataset_{0:i}, p(\transitionFn \mid \dataset_{0:i-1}))$
%    \State Improve policy $\pi^{\text{fast}}_{i+1} \leftarrow \text{update\_policy}(p\left(\transitionFn \mid \dataset_{0:i}), \dataset_{0:i} \right)$
%\EndFor
%\end{algorithmic}
%\end{algorithm}
%
%
%\subsubsection{Fast updates}
%
%
%In this section we extend these fast updates to environment's with high dimensional state spaces and large data sets.
%% Our method draws on the connection between BNNs and GPs and formulates a function space SVGP posterior given
%Our method uses a BNN dynamic model and draws on the connection between BNNs and GPs
%\citep{khanApproximate2019} to formulate a function space SVGP posterior,
%where we can apply the fast updates from \cref{eq-dual-update-svgp}.
%At a high-level, we first use Laplace's approximation to obtain a weight space posterior for our BNN.
%We then linearise our BNN around the optimal parameters and interpret it as a GP.
%Finally, we formulate a lower rank approximation of this GP posterior (i.e. a SVGP posterior) using inducing variables.
%% formulates a function space SVGP posterior by drawing on the connection between BNNs and GPs.
%
%
%The strategies in \cref{eq-greedy,eq-posterior-sampling,eq-hucrl} do not update the dynamic model during an episode.
%A better approach would be to update the posterior at every time step during an episode, for example,
%\begin{subequations}
%\begin{align} \label{eq-fast-update-mpc}
%  \policy_{i+1}^{\text{greedy}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
%\E_{p(\transitionFn \mid \dataset_{0:i})} \left[J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) \right] + \stateValueFn(\state_{\Horizon+1}) \\
%  \policy_{i+1}^{\text{PS}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}}
%J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) + \stateValueFn(\state_{\Horizon+1}) \quad \text{s.t. } \transitionFn \sim p(\transitionFn \mid \dataset_{0:i} \cup \bm\tau_{t}) \\
%  \policy_{i+1}^{\text{UCRL}}(\state) &= \arg \max_{\action_{0}} \max_{\action_{1:\Horizon}} \max_{\transitionFn \in \mathcal{M}}
%J^{\Horizon}(\action_{0:\Horizon}, \transitionFn) + \stateValueFn(\state_{\Horizon+1}) \quad \text{s.t. } \mathcal{M} = \{\transitionFn(\state_{t},\action_{t}) - \mu_{i,t}(\state_{t},\action_{t}) \leq \beta_{i,t} \Sigma_{i,t}(\state_{t}, \action_{t})\} \\
%  \stateValueFn(\state) &= \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right] \label{eq-value-fn}
%\end{align}
%\end{subequations}
%\begin{subequations}
%\begin{align} \label{eq-fast-update-mpc-old}
%  \policy^{\text{fast}}(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}}
%  \mathbb{E}_{\state_{\horizon} \sim p(\state_{\horizon+1} \mid \transitionFn(\state_{\horizon}, \action_{\horizon}))} \left[ \sum_{\horizon=0}^{\Horizon}     \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \mid \state_{0}=\state \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
%  \stateValueFn(\state) &= \mathbb{E} \left[ \sum_{t=0}^{\infty}     \discount^{t} \rewardFn(\state_{t},\action_{t}) \mid \state_{0}=\state \right] \label{eq-fast-update-mpc}
%\end{align}
%\end{subequations}
%
%\begin{align} \label{}
%  \policy(\state) = \arg &\max_{\action_{0}} \max_{\action_{1}, \ldots, \action_{\Horizon}} \max_{\optimisticTransition \in \optimisticTransitionSet}
%  \sum_{\horizon=0}^{\Horizon}  \mathbb{E}_{\noise_{\horizon}} \left[  \discount^{\horizon} \rewardFn(\state_{\horizon},\action_{\horizon}) \right] + \discount^{\Horizon+1} \stateValueFn(\state_{\Horizon+1}) \\
%  \text{s.t. } \state_{\horizon+1} &= \optimisticTransition(\state_{\horizon}, \action_{\horizon}) + \noise_{\horizon} \\
%  \optimisticTransition(\state_{\horizon}, \action_{\horizon}) &=
%\optimisticTransitionMean(\state_{\horizon}, \action_{\horizon}) \pm \beta_{i}
%\optimisticTransitionCov(\state_{\horizon}, \action_{\horizon})
%\end{align}
%
%\input{proof.tex}
%
%
%\section{Hallucinated Upper Confidence Reinforcement Learning (H-UCRL)} \label{sec-hucrl}
%\cite{curiEfficient2020} introduced a tractable approximation which retains some of the theoretical guarantees whilst
%being applicable with deep model-based RL.
%They introduce a function \(\eta: \mathcal{S} \times \mathcal{A} \rightarrow [-1, 1]^{p}\) which acts as a hallucinated control input.
%The strategy is given by,
%\begin{align} \label{eq-hucrl}
%\pi_i^{\text{UCRL}} = \arg \max_{\policy \in \policyDomain} \max_{\eta(\cdot) \in [-1,1]} J(\transitionFn, \policy) \quad \text{s.t.} \quad \transitionFn = \mu_{i}(\state_{t}, \action_{t}) + \beta_{i} \Sigma_{i}(\state_{t}, \action_{t}) \eta(\state_{t},\action_{t}).
%\end{align}
%Intuitively, \(\eta(\state,\action) \in [-1,1]\) enables the optimisation to select any dynamics model
%\(\transitionFn\) within \(\pm \beta \Sigma_{i}(\state_{t}, \action_{t})\) of the posterior mean \(\mu_{i}(\state_{t}, \action_{t})\).
%
%\section{Laplace Approximation} \label{sec-laplace-approximation}
%
%\textbf{Laplace approximation}
%The Laplace approximation \todo{add citation of original paper} constructs a Gaussian approximation of the weight-space posterior $p(\weights \mid \dataset)$
%by using a second-order Taylor expansion of $\mathcal{L}$ around $\weights^{*}$.
%The posterior approximation is given by,
%\begin{align} \label{eq-laplace-approx-weight-space}
%  p(\weights \mid \dataset) \approx \mathcal{N} \left( \weights \mid \weights^{*} , \bm\Sigma_{\weights^{*}} \right)
%  \quad \text{with} \quad \bm\Sigma_{\weights} =
% - \nabla_{\weights \weights}^{2} \mathcal{L} ( \dataset ; \weights)|_{\weights=\weights^{*}}
%\end{align}
%That is, it sets the posterior precision to the Hessian of the loss at the optimal parameters $\weights^{*}$.
%It is worth noting that computing the Hessian of the loss can be computationally intractable for large networks.
%The prior terms are usually trivial, so we focus on the likelihood here.
%The Jacobian and Hessian of the log likelihood can be expressed per data point,
%\begin{align} \label{eq-jac}
%  \nabla_{\weights} \log p(\mathbf{y} \mid \mathbf{f}(\mathbf{x}; \weights)) &= \mathbf{J}(\mathbf{x})^{T} \mathbf{r}(\mathbf{y} ; \mathbf{f}) \\
%  \nabla^{2}_{\weights\weights} \log p(\mathbf{y} \mid \mathbf{f}(\mathbf{x}; \weights)) &= \mathbf{H}(\mathbf{x})^{T}
%  \mathbf{r}(\mathbf{y};\mathbf{f}) - \mathbf{J}(\mathbf{x})^{T} \bm\Lambda(\mathbf{y};\mathbf{f}) \mathbf{J}(\mathbf{x}),
%\label{eq-hess}
%\end{align}
%through the Jacobian $\mathbf{J} \in \R^{C \times P}$ and Hessian $\mathbf{H} \in \R^{C \times P \times P}$ of the feature extractor $\mathbf{f}(\mathbf{x}; \bm\weights)$,
%\begin{align} \label{eq-jac}
%[\mathbf{J}(\cdot)]_{ci} = \odv{f_{c}(\cdot ; \weights)}{\weights_{i}}_{\weights=\weights^{*}} \qquad
%[\mathbf{H}(\cdot)]_{cij} = \odv{f_{c}(\cdot ; \weights)}{\weights_{i}\weights_{j}}_{\weights=\weights^{*}}
%\end{align}
%where
%$\mathbf{r}(\mathbf{y}; \mathbf{f}) = \nabla_{\mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$ can be interpreted as a residual and
%$\bm\Lambda(\mathbf{y} ; \mathbf{f}) = \nabla^{2}_{\mathbf{f} \mathbf{f}} \log p(\mathbf{y} \mid \mathbf{f})$
%as per-input noise.
%Many approximations exist and can be used alongsied our method.
%We refer the reader to \cite{daxbergerLaplace2021} for more details.
%See \cref{sec-laplace-approximation} for further details on the Laplace approximation.
%
%
%\section{Experiment details}
%
%\subsection{UCI}
%
%\begin{table}[t!] 
%  \centering\scriptsize
%  \caption{Negative log predictive density (NLPD) (lower better) for the proposed model TODO} 
%	\label{tbl:uci_all}
%	% Control table spacing
%	\renewcommand{\arraystretch}{1.}
%	\setlength{\tabcolsep}{2pt}
%	\setlength{\tblw}{0.14\textwidth}  
%	
%	% Custom error formatting
%	\newcommand{\val}[2]{%
%		$#1$\textcolor{gray}{\tiny ${\pm}#2$}
%	} 
%
%    % THE TABLE NUMBER ARE GENERATED BY A SCRIPT	
%	\input{tables/uci_all.tex}
%\end{table}
%
%\section{Template stuff}
%\subsection{Generate TikZ Figures from Python}
%We can generate figures in \texttt{.tex} format directly from Python:
%\begin{verbatim}
%tikzplotlib.save("fig.tex", axis_width="\\figurewidth", axis_height="\\figureheight")
%\end{verbatim}
%\cref{fig:example} shows that we get nicely formatted lables/titles/etc when we include them in our paper.
%\begin{figure}[h]
%    \centering\footnotesize
%
%    % Set your figure size here
%    \setlength{\figurewidth}{.33\textwidth}
%    \setlength{\figureheight}{.75\figurewidth}
%
%    % Customize your plot here
%    % (scale only axis applies the size to the axis box and not entire figure)
%    \pgfplotsset{grid style={dotted},title={Foo},scale only axis}
%
%    % Use the subcaption package (= subfigure) for sub-plots, that is
%    % plot the separate plots separately in Python
%    \begin{subfigure}{.4\textwidth}
%        \centering
%        \input{./fig/example_fig.tex}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}{.4\textwidth}
%        \centering
%        \input{./fig/example_fig.tex}
%    \end{subfigure}
%    \caption{Foo}
%    \label{fig:example}
%\end{figure}
%
%\subsection{Generate Tables from Python}
%We can also generate tables straight from python using \href{https://github.com/astanin/python-tabulate}{tabulate}:
%\begin{verbatim}
%table = [["Sun",696000,1989100000],["Earth",6371,5973.6],
%        ["Moon",1737,73.5],["Mars",3390,641.85]]
%headers = ["Planet","R (km)", "mass (x 10^29 kg)"]
%table = tabulate(table, headers=headers, tablefmt="latex")
%with open("table.tex", 'w') as file:
%    file.write(table)
%\end{verbatim}
%
%\begin{table}[h]
%    \centering
%    \input{./tables/example_table.tex}
%\end{table}

\subsection{Biblatex}
Rember when using biblatex to use 'parencite' for \citep{kamtheDataEfficient2018} and when using natbib to use 'citep'.

%\bibliography{biblio.bib}
\end{document}
