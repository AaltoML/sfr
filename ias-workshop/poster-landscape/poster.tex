%!TeX program = lualatex
\documentclass[final,12pt]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=a0,orientation=landscape,scale=1.4]{beamerposter}
% OR GIVE EXACT WIDTH/HEIGHT and adjust SCALE
%\usepackage[size=custom,width=240,height=120,orientation=landscape,scale=2.2]{beamerposter}
\usetheme{aalto}
\usecolortheme{aalto}
\usepackage{graphicx}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath,bm,amsfonts,amssymb,amsthm,mathrsfs}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{xspace}         % protect spaces
\usepackage{nicefrac}  
\usepackage{wrapfig}


% For tables
\usepackage{array,multirow}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newlength{\tblw}

% New table column type
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}


%\usepackage{natbib}
%\setlength{\bibsep}{0.0pt}
\usetikzlibrary{calc}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.01\paperwidth}
\setlength{\colwidth}{0.30\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Customization
% ====================

\newcommand{\action}{\ensuremath{\mathbf{a}}}
\newcommand{\myline}[1]{\protect\tikz[baseline=-.5ex,line width=1.6pt]\protect\draw[draw=#1](0,0)--(1.2em,0);}
\usepackage{subcaption}

% Figure size
\newlength\figureheight
\newlength\figurewidth

% Latin
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\cf}{\textit{cf.}}
\newcommand{\etc}{\textit{etc.}}
\newcommand{\etal}{\textit{et~al.}}

% Math
\newcommand{\mathbold}[1]{\bm{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vectb}[1]{\bm{#1}}
\newcommand{\T}{^\mathsf{T}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\kron}{\raisebox{1pt}{\ensuremath{\:\otimes\:}}} % Kronceker product
\newcommand{\bigO}{\mathcal{O}}

% Custom macros
%\newcommand{\T}{\top}    % Transpose
\newcommand{\dd}{\,\mathrm{d}} % E.g. \int f(x) \dd x
\newcommand{\E}{\mathbb{E}}    % Expectation
\newcommand{\N}{\mathrm{N}}   % Gaussian distribution
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\chol}{chol}
\DeclareMathOperator{\dchol}{dchol}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\gammad}{Gamma}
\DeclareMathOperator{\expd}{Exp}
\DeclareMathOperator{\sech}{sech}
%\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\argmax}{arg\,max}
\newcommand{\KL}[2]{\mathrm{D}_\mathrm{KL}\left[#1\|#2\right]}

% Bold Greek symbols
\newcommand{\valpha}[0]{\mathbold{\alpha}}
\newcommand{\vbeta}[0]{\mathbold{\beta}}
\newcommand{\vsigma}[0]{\mathbold{\sigma}}
\newcommand{\vchi}[0]{\mathbold{\chi}}
\newcommand{\vepsilon}[0]{\mathbold{\varepsilon}}
\newcommand{\veta}[0]{\mathbold{\eta}}
\newcommand{\vmu}[0]{\mathbold{\mu}}
\newcommand{\vomega}[0]{\mathbold{\omega}}
\newcommand{\vxi}[0]{\mathbold{\xi}}
\newcommand{\vphi}[0]{\mathbold{\phi}}
\newcommand{\vtheta}[0]{\mathbold{\theta}}
\newcommand{\vTheta}[0]{\mathbold{\Theta}}
\newcommand{\vzeta}[0]{\mathbold{\zeta}}
\newcommand{\MPsi}[0]{\mathbold{\Psi}}
\newcommand{\MPhi}[0]{\mathbold{\Phi}}
\newcommand{\MSigma}[0]{\mathbold{\Sigma}}
\newcommand{\MTheta}[0]{\mathbold{\Theta}}
\newcommand{\invchisq}[0]{\mathrm{Inv\text{-}}\chi^2}
\renewcommand{\mid}{\,|\,}
\newcommand{\imag}[0]{\mathrm{i}}

% Use these macros for big roman symbols
\newcommand{\RL}{\mathrm{L}}
\newcommand{\RM}{\mathrm{M}}
\newcommand{\RQ}{\mathrm{Q}}
\newcommand{\RS}{\mathrm{S}}
\newcommand{\RU}{\mathrm{U}}

% Use these macros for roman symbols
\newcommand{\rb}{\mathrm{b}}
\newcommand{\rc}{\mathrm{c}}
\newcommand{\rrm}{\mathrm{m}}
\newcommand{\ro}{\mathrm{o}}
\newcommand{\rs}{\mathrm{s}}
\newcommand{\rrq}{\mathrm{q}}

% Use these macros for vectors and matrices
\newcommand{\va}{\mbf{a}}
\newcommand{\vb}{\mbf{b}}
\newcommand{\vc}{\mbf{c}}
\newcommand{\vd}{\mbf{d}}
\newcommand{\ve}{\mbf{e}}
\newcommand{\vf}{\mbf{f}}
\newcommand{\vg}{\mbf{g}}
\newcommand{\vh}{\mbf{h}}
\newcommand{\vi}{\mbf{i}}
\newcommand{\vj}{\mbf{j}}
\newcommand{\vk}{\mbf{k}}
\newcommand{\vl}{\mbf{l}}
\newcommand{\vm}{\mbf{m}}
\newcommand{\vn}{\mbf{n}}
\newcommand{\vo}{\mbf{o}}
\newcommand{\vp}{\mbf{p}}
\newcommand{\vq}{\mbf{q}}
\newcommand{\vr}{\mbf{r}}
\newcommand{\vs}{\mbf{s}}
\newcommand{\vu}{\mbf{u}}
\newcommand{\vv}{\mbf{v}}
\newcommand{\vw}{\mbf{w}}
\newcommand{\vx}{\mbf{x}}
\newcommand{\vy}{\mbf{y}}
\newcommand{\vz}{\mbf{z}}
\newcommand{\MA}{\mbf{A}}
\newcommand{\MB}{\mbf{B}}
\newcommand{\MC}{\mbf{C}}
\newcommand{\MD}{\mbf{D}}
\newcommand{\MF}{\mbf{F}}
\newcommand{\MG}{\mbf{G}}
\newcommand{\MH}{\mbf{H}}
\newcommand{\MI}{\mbf{I}}
\newcommand{\MJ}{\mbf{J}}
\newcommand{\MK}{\mbf{K}}
\newcommand{\ML}{\mbf{L}}
\newcommand{\MM}{\mbf{M}}
\newcommand{\MP}{\mbf{P}}
\newcommand{\MQ}{\mbf{Q}}
\newcommand{\MR}{\mbf{R}}
\newcommand{\MS}{\mbf{S}}
\newcommand{\MT}{\mbf{T}}
\newcommand{\MV}{\mbf{V}}
\newcommand{\MW}{\mbf{W}}
\newcommand{\MX}{\mbf{X}}
\newcommand{\MY}{\mbf{Y}}

% ====================
% Custom
% ====================
% Our method
\newcommand{\our}{SFR}

% Custom error formatting
%\newcommand{\val}[2]{ $#1$\textcolor{gray}{\tiny ${\pm}#2$}}




\newcommand{\Jac}[2]{\mathcal{J}_{#1}(#2)}
\newcommand{\JacT}[2]{\mathcal{J}_{#1}^\top(#2)}


\newcommand{\GP}{\mathcal{GP}}
\newcommand{\MKzz}{\mbf{K}_{\mbf{z}\mbf{z}}}
\newcommand{\MKzzc}{\mbf{K}_{\mbf{z}\mbf{z}, c}}
\newcommand{\MKxx}{\mbf{K}_{\mbf{x}\mbf{x}}}
\newcommand{\MKzx}{\mbf{K}_{\mbf{z}\mbf{x}}}
\newcommand{\MKxz}{\mbf{K}_{\mbf{x}\mbf{z}}}
\newcommand{\vkzi}{\mbf{k}_{\mbf{z}i}}
\newcommand{\vkzic}{\mbf{k}_{\mbf{z}i,c}}
\newcommand{\vkzs}{\mbf{k}_{\mbf{z}i}}
\newcommand{\MBeta}[0]{\mathbold{B}}
\newcommand{\MLambda}[0]{\mathbold{\Lambda}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\myexpect}{\mathbb{E}}


% Variables
\newcommand{\state}{\ensuremath{\mathbf{s}}}
\newcommand{\noise}{\ensuremath{\bm\epsilon}}
\newcommand{\discount}{\ensuremath{\gamma}}
\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingVariable}{\ensuremath{\mathbf{u}}}
\newcommand{\dataset}{\ensuremath{\mathcal{D}}}
\newcommand{\dualParam}[1]{\ensuremath{\bm{\lambda}_{#1}}}
\newcommand{\meanParam}[1]{\ensuremath{\bm{\mu}_{#1}}}

% Indexes
\newcommand{\horizon}{\ensuremath{h}}
\newcommand{\Horizon}{\ensuremath{H}}
\newcommand{\numDataNew}{\ensuremath{N^{\text{new}}}}
\newcommand{\numDataOld}{\ensuremath{N^{\text{old}}}}
\newcommand{\numInducing}{\ensuremath{M}}

% Domains
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\actionDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\inputDomain}{\ensuremath{\mathbb{R}^{D}}}
\newcommand{\outputDomain}{\ensuremath{\mathbb{R}^{C}}}
\newcommand{\policyDomain}{\ensuremath{\Pi}}


% Functions
\newcommand{\rewardFn}{\ensuremath{r}}
\newcommand{\transitionFn}{\ensuremath{f}}
\newcommand{\latentFn}{\ensuremath{f}}

\newcommand{\optimisticTransition}{\ensuremath{\hat{f}}}
\newcommand{\optimisticTransitionMean}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionCov}{\ensuremath{\mu_{\optimisticTransition}}}
\newcommand{\optimisticTransitionSet}{\ensuremath{\mathcal{M}}}


% Parameters
% \newcommand{\weights}{\ensuremath{\bm\phi}}
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\valueFnParams}{\ensuremath{\psi}}
\newcommand{\policyParams}{\ensuremath{\theta}}

% Networks
\newcommand{\transitionFnWithParams}{\ensuremath{\transitionFn_{\weights}}}
\newcommand{\valueFn}{\ensuremath{\mathbf{Q}}}
\newcommand{\stateValueFn}{\ensuremath{\mathbf{V}}}
% \newcommand{\valueFn}{\ensuremath{\mathbf{Q}_{\valueFnParams}}}
\newcommand{\policy}{\ensuremath{\pi}}
\newcommand{\pPolicy}{\ensuremath{\pi_{\policyParams}}}

% ====================
% Title
% ====================

% \title{Investigating Bayesian Neural Network Dynamics Models for Model-Based Reinforcement Learning}

% \author{Aidan Scannell\inst{1} ~~ Arno Solin\inst{1} ~~ Joni Pajarinen\inst{1}}

% \institute[shortinst]{ \inst{1}Aalto University}

\title{Neural Networks as Sparse Gaussian Processes for Sequential Learning}

\author{%
  Aidan Scannell\textsuperscript{\star}\,\inst{1}\,\inst{2} \quad
  Riccardo Mereu\textsuperscript{\star}\,\inst{1} \quad
  Paul Chang\,\inst{1} \quad
  Ella Tamir\,\inst{1}\quad
  Joni Pajarinen\,\inst{1}\quad
  Arno Solin\,\inst{1}
}

%\author{Your Name\inst{1} ~~ Collaborator Name\inst{2} ~~ Arno Solin\inst{1}}

\institute[shortinst]{ \inst{1}Aalto University \qquad \inst{2}Finnish Center for Artificial Intelligence}

% ====================
% Footer (optional)
% ====================

\footercontent{
International Workshop of Intelligent Autonomous Learning Systems 2023 - Kleinwalsertal, Austria \hfill
  \href{mailto:aidan.scannell@aalto.fi}{aidan.scannell@aalto.fi}}


% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]

% \begin{tikzpicture}[remember picture,overlay,scale=1]

%   \newcommand{\face}[2]{%
%       \tikz\node[minimum width=6cm,circle,fill=primarycolor,draw=primarycolor,line width=6mm,text=white,path picture={\node at (path picture bounding box.center){
%           \includegraphics[width=####2,trim=0 0 0 0,clip]{fig/####1}
%         };}]{};}

%   % You can add presenter faces here
%   \node [] (box) at ($(current page.center) + (30cm,31cm)$) {\face{aidan-scannell}{6cm}};
%   \node [] (box) at ($(current page.center) + (36cm,31cm)$) {\face{arno-solin}{6cm}};
%   \node [] (box) at ($(current page.center) + (42cm,31cm)$) {\face{joni-pajarinen}{6cm}};

% \end{tikzpicture}

\begin{columns}[t]

\separatorcolumn

\begin{column}{\colwidth}

  \begin{alertblock}{TL;DR}

    \begin{itemize}
      \item Neural networks (NNs) have limitations: \textit{estimating uncertainty}, \textit{incorporating new data}, and \textit{avoiding catastrophic forgetting}.
      % \item Convert NN to sparse Gaussian process via dual parameters.
        % \begin{itemize}
        %     \item via the so-called \alert{\bf dual parameters}.
        % \end{itemize}
        \item Our method, \alert{\bf Sparse Function-space Representation (SFR)}:
            \begin{enumerate}
              \item converts NN to sparse Gaussian process (GP) via dual parameters,
              \item has good uncertainty estimates,
              \item can incorporate new data without retraining,
              \item can maintain a functional representation for continual learning,
              \item can be used for uncertainty-guided exploration in model-based RL.
            \end{enumerate}
    \end{itemize}


  \end{alertblock}

  \begin{block}{Method}

  \heading{1. Train Neural Network}

  \begin{enumerate}[(a)]
    \item \alert{\bf Inputs} in a {\it supervised setting} for NNs $f_\mathbf{w}: \inputDomain \to \outputDomain$:\
        \begin{itemize}
        \item $\dataset = \{(\vx_{i} , \vy_{i})\}_{i=1}^{N}$, a data set w/ input $\vx_i \in \inputDomain$ and output $\vy_i \in \outputDomain$;
        \item $\weights \in \R^{P}$, the initial weights of the neural network.
        \end{itemize}
    \item \alert{\bf Train: } minimize the empirical (regularized) risk:
        \begin{align}
        \weights^{*} =  \arg \min_{\weights} \mathcal{L}(\dataset,\weights)
            = \arg \min_{\weights} \textstyle\sum_{i=1}^{N} \ell(f_\weights(\mathbf{x}_{i}), y_i) + \delta \mathcal{R}(\weights).
        \end{align}
    \item \alert{\bf Output:} $\weights^*$, the Maximum A-Posteriori (MAP) weights of the NN.\\[0.5cm]

        % Use first two moments to obtain a GP with \alert{mean function $\mu(\cdot)$} and a \alert{kernel $\kappa(\cdot,\cdot)$}. \\
  \end{enumerate}

  \heading{2. Convert Neural Network to Gaussian Process}

  \begin{enumerate}[(a)]
    \item Linearise NN using \alert{Laplace-GGN approx} at MAP weights $\weights^{*}$,
    \begin{equation*}
        f_{\weights^*}(\vx) \approx \Jac{\weights_*}{\vx} \, \weights \quad
    \text{where} \quad \Jac{\weights}{\vx} \coloneqq \left[ \nabla_\weights f_\weights(\vx)\right]^\top \in \R^{C \times P}.
    \end{equation*}
    % \item Interpret linear model as Gaussian process with \alert{mean function $\mu(\cdot)$} and \alert{covariance function $\kappa(\cdot,\cdot)$},
    \item Interpret linear model as GP,
        \begin{equation*}
            \mu(\vx) =  0 \quad \text{and} \quad
        \kappa(\vx, \vx') = \frac{1}{\delta} \, \Jac{\weights^*}{\vx} \, \JacT{\weights^*}{\vx'},
        \end{equation*}
    \item Formulate GP predictive posterior via dual parameterization,
        \begin{align}  \label{eq:gp_pred_mean}
        \myexpect_{p(f_i \mid\vy)}[f_i] &= \vk_{\vx i}^\top \valpha \quad \text{and} \\
\label{eq:gp_pred_var}
        \mathrm{Var}_{p(f_i \mid \vy)}[f_i] &= k_{ii} - \vk_{\vx i}^\top ( \MKxx + \diag(\vbeta)^{-1})^{-1} \vk_{\vx i}
        \end{align}
        % with \alert{dual parameters}, %$\valpha = \{\alpha_i\}_{i=1}^N, \vbeta = \{ \beta_i\}_{i=1}^N$,
        % \begin{align}
        % \alpha_i &\coloneqq {\color{red}{\myexpect_{p(\vw \mid \vy)}}}[\nabla_{f}\log p(y_i \mid f) |_{f=f_i}]
        % \quad \text{and} \\
        % \beta_i &\coloneqq - {\color{red}{\myexpect_{p(\vw \mid \vy)}}}[\nabla^2_{f f}\log p(y_i \mid f_i) |_{f=f_i}]
        % \end{align}
        % \begin{align}
        % \alpha_i \approx \hat{\alpha}_i &\coloneqq %{\color{cgreen}{\myexpect_{q(\vw)}}} [
        % \nabla_{f}\log p(y_i \mid f) |_{f=f_i}
        % %  ]
        % \quad \text{and} \\
        % \beta_i \approx \hat{\beta}_i &\coloneqq -
        % %  {\color{cgreen}{\myexpect_{q(\vw)}}} [
        % \nabla^2_{ff}\log p(y_i \mid f) |_{f=f_i}
        % %  ]
        % \end{align}
        % {\bf Cons:} requires access to all data $\rightarrow \color{red}{\mathcal{O}(N^3)}$
    \end{enumerate}
  \heading{3. Sparsify the Gaussian Process}

  \begin{enumerate}[(a)]
    \item Eqs. \ref{eq:gp_pred_mean}/\ref{eq:gp_pred_var} use all data $\color{red}{\mathcal{O}(N^3)}$ so sparsify using inducing points %$\vu_j=f_{\vw^*}(\vz_j)$ with   $\{\vz_j\}_{j=1}^M$:
            %We can build a sparse GP posterior with the following form:
            \begin{align}
            \myexpect_{p(f_i \mid\vy)}[f_i] &\approx   \vkzs^{\T} \MKzz^{-1} \valpha_{\vu}
            \quad \text{and} \\
            \mathrm{Var}_{p(f_i \mid \vy)}[f_i] &\approx  k_{ii} - \vkzs^\top [\MKzz^{-1} - (\MKzz + \MBeta_{\vu})^{-1} ]\vkzs
            \end{align}
            % where $q_\vu(f_i)=\int p(f_i|\vu) q(\vu)\,\textrm{d}\vu$
            % $\color{red}{\mathcal{O}(M^3) \text{ with } (M\ll N)}$
            %\heading{Dual parameterization}
            with \alert{SFR dual parameters} %$\valpha_{\vu} \in \R^M, \MBeta_{\vu} \in \R^{M \times M}$:
            \begin{equation}
            \valpha_{\vu}  =  \sum_{i=1}^N  \vkzi \, \hat{\alpha}_{i} %\in \R^M
            \quad \text{and} \quad
            \MBeta_{\vu} =  \sum_{i=1}^N \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T} %\in \R^{M \times M}
            \end{equation}
    \end{enumerate}

  \end{block}

\end{column}

\separatorcolumn

\begin{column}{2.05\colwidth}

\begin{minipage}{\textwidth}

  % % Figure placeholder
  % \begin{tikzpicture}[inner sep=0,outer sep=0]

  %   \node[inner sep=1em, minimum width=\textwidth,minimum height=.2\textwidth, fill=primarycolor!10, rounded corners=6pt] at (0,0) {Large figure};

  % \end{tikzpicture}

    \begin{figure}[t!]
    \begin{subfigure}[b]{0.65\textwidth}\centering
    \centering\scriptsize
    \includegraphics[width=0.92\textwidth]{fig/regression.png}
    % % Figure options
    % \pgfplotsset{axis on top,scale only axis,width=\figurewidth,height=\figureheight, ylabel near ticks,ylabel style={yshift=-2pt},y tick label style={rotate=90},legend style={nodes={scale=1., transform shape}},tick label style={font=\tiny,scale=1}}
    % \pgfplotsset{xlabel={Input, $x$},axis line style={rounded corners=2pt}}
    % % Set figure
    % \setlength{\figurewidth}{.28\textwidth}
    % \setlength{\figureheight}{\figurewidth}
    % %
    % \def\inducing{\large Sparse inducing points}
    % %
    % \begin{subfigure}[c]{.30\textwidth}
    %     \raggedleft
    %     \pgfplotsset{ylabel={Output, $y$}}
    %     \input{./fig/regression-nn.tex}%
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[c]{.01\textwidth}
    %     \centering
    %     \tikz[overlay,remember picture]\node(p0){};
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[c]{.22\textwidth}
    %     \raggedleft
    %     \pgfplotsset{yticklabels={,,},ytick={\empty}}
    %     \input{./fig/regression-nn2svgp.tex}%
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[c]{.01\textwidth}
    %     \centering
    %     \tikz[overlay,remember picture]\node(p1){};
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[c]{.22\textwidth}
    %     \raggedleft
    %     \pgfplotsset{yticklabels={,,},ytick={\empty}}
    %     \input{./fig/regression-update.tex}%
    % \end{subfigure}
    \caption{\footnotesize \textbf{Regression example on an MLP with two hidden layers.} Left:~Predictions from the trained neural network. Middle:~Our approach summarizes all the training data with the help of a set of inducing points. The model captures the predictive mean and uncertainty, and (right) makes it possible to incorporate new data without retraining the model.}
    \label{fig:teaser}
    % %
    % \begin{tikzpicture}[remember picture,overlay]
    %     % Arrow style
    %     \tikzstyle{myarrow} = [draw=black!80, single arrow, minimum height=14mm, minimum width=2pt, single arrow head extend=4pt, fill=black!80, anchor=center, rotate=0, inner sep=5pt, rounded corners=1pt]
    %     % Arrows
    %     \node[myarrow] (p0-arr) at ($(p0) + (1em,1.5em)$) {};
    %     \node[myarrow] (p1-arr) at ($(p1) + (1em,1.5em)$) {};
    %     % Arrow labels
    %     \node[font=\scriptsize\sc,color=white] at (p0-arr) {\our};
    %     \node[font=\scriptsize\sc,color=white] at (p1-arr) {new data};
    % \end{tikzpicture}
    \end{subfigure}
    \hspace{2cm}
    \begin{subfigure}{0.3\textwidth}\centering
    \centering
    %     \includegraphics[scale=0.5]{fig/table.pdf}
    % \end{subfigure}\hspace{2cm}
        \includegraphics[scale=5]{fig/nlpd.pdf}
    	\definecolor{steelblue31119180}{RGB}{31,119,180}
     \definecolor{darkorange25512714}{RGB}{255,127,14}
    %  \newcommand{\myline}[1]{\protect\tikz[baseline=-.5ex,line width=1.6pt]\protect\draw[draw=#1](0,0)--(1.2em,0);}
    \caption{\footnotesize \textbf{\our \ (\myline{steelblue31119180}) requires fewer inducing points $M$} to converge to a good NLPD than a GP subset (\myline{darkorange25512714}) on UCI classification tasks.}
    \end{subfigure}
    \end{figure}

  % \alert{\bf Figure~1:}~Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus interdum dignissim urna, ac blandit enim condimentum a. Etiam ultricies faucibus tellus, sed auctor nibh tempus sed. Integer id vehicula arcu, ut aliquet lectus. Pellentesque non tempus tellus. Pellentesque elit erat, porttitor non magna ac, posuere porta massa. Etiam placerat, tellus ut accumsan feugiat, ipsum sapien ornare est, cursus maximus.
\end{minipage}\\[2cm]

\begin{columns}[t]

\begin{column}{\colwidth}

    \begin{block}{Dual Parameters}
      \begin{itemize}
        \item \alert{Dual parameters} are expectations of derivatives, %$\valpha = \{\alpha_i\}_{i=1}^N, \vbeta = \{ \beta_i\}_{i=1}^N$,
            \begin{align}
            \alpha_i &\coloneqq {\color{red}{\myexpect_{p(\vw \mid \vy)}}}[\nabla_{f}\log p(y_i \mid f) |_{f=f_i}]
            \quad \text{and} \\
            \beta_i &\coloneqq - {\color{red}{\myexpect_{p(\vw \mid \vy)}}}[\nabla^2_{f f}\log p(y_i \mid f_i) |_{f=f_i}]
            \end{align}
            and we simply consider the MAP of ${\color{red}{p(\vw \mid \vy)}}$,
        % \begin{align}
        % \alpha_i \approx \hat{\alpha}_i &\coloneqq %{\color{cgreen}{\myexpect_{q(\vw)}}} [
        % \nabla_{f}\log p(y_i \mid f) |_{f=f_i}
        % %  ]
        % \quad \text{and} \\
        % \beta_i \approx \hat{\beta}_i &\coloneqq -
        % %  {\color{cgreen}{\myexpect_{q(\vw)}}} [
        % \nabla^2_{ff}\log p(y_i \mid f) |_{f=f_i}
        % %  ]
        % \end{align}

        \item Conditioning on new data $\dataset^{\text{new}}$ with \alert{dual parameters} is easy, %$\valpha_{\vu} \in \R^M, \MBeta_{\vu} \in \R^{M \times M}$:
            \begin{equation}
            \valpha_{\vu}  = \valpha_{\vu}^{\text{old}} +  \underbrace{\sum_{\vx_{i},y_{i} \in \dataset^{\text{new}}}  \vkzi \, \hat{\alpha}_{i}}_{\text{update}} %\in \R^M
            \quad \text{and} \quad
            \MBeta_{\vu} = \MBeta_{\vu}^{\text{old}} +  \underbrace{\sum_{\vx_{i},y_{i} \in \dataset^{\text{new}}} \vkzi \,\hat{\beta}_{i} \, \vkzi^{\T}}_{\text{update}} %\in \R^{M \times M}
            \nonumber
            \end{equation}
        \end{itemize}

    \end{block}

    % \begin{block}{Continual Learning}

    % \end{block}

    \begin{block}{Model-based Reinforcement Learning}

        % \alert{\bf Goal}
        % \begin{align} \label{eq-model-free-objective}
        % J(\transitionFn, \policy) = \mathbb{E}_{\noise_{0:\infty}} \bigg[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \bigg]
        % \quad \text{s.t. } \state_{t+1} = \transitionFn(\state_{t}, \action_{t}) + \noise_{t},
        % \end{align}

        \alert{\bf Strategy} Policy \(\pi : \stateDomain \rightarrow \actionDomain\) based on posterior sampling:
        \begin{align} \label{eq-posterior-sampling}
        \policy^{\text{PS}} &= \arg \max_{\pi \in \Pi} \mathbb{E}_{\noise_{0:\infty}} \bigg[ \sum_{t=0}^{\infty} \discount^{t} \rewardFn(\state_{t},\action_{t}) \bigg]
        \quad \text{s.t. } \tilde{\transitionFn} \sim q_{\mathbf{u}}({\transitionFn} \mid \dataset),
        \end{align}

        % Make custom TikZ command (argument: colour)
        \newcommand{\lab}[1]{\protect\tikz[baseline=-.5ex]{\protect\node[minimum width=1.5em,minimum height=.8em,fill=#1,opacity=.1](a){};\protect\draw[#1,semithick](a.west)--(a.east);}}

        % The methods
        \definecolor{darkgray176}{RGB}{176,176,176} % darkgray176
        \definecolor{color-our}{RGB}{0,191,191} %darkturquoise0191191
        \definecolor{color-mlp}{RGB}{191,0,191} % darkviolet1910191
        \definecolor{color-ddpg}{RGB}{191,191,0} % goldenrod1911910
        \definecolor{color-ensemble}{RGB}{0,127,0} % green01270
        \definecolor{lightgray204}{RGB}{204,204,204} % lightgray204

        \begin{figure}[!t]
        \centering\scriptsize
        \includegraphics[width=0.9\textwidth]{fig/rl.pdf}
        \caption{\footnotesize \textbf{Cartpole swingup with sparse reward.} Training curves showing that \our's uncertainty estimates improve sample efficiency in RL.
        Our method (\lab{color-our}) converges in fewer environment steps than the baseline model-based RL method and DDPG, the model-free baseline.}
        \label{fig:rl}
        % \vspace*{-1em}
        \end{figure}

        % \begin{figure}[!t]
        % \centering\scriptsize
        % \begin{subfigure}[c]{.9\textwidth}
        % \raggedleft\scriptsize
        % \setlength{\figurewidth}{.6\textwidth}
        % \setlength{\figureheight}{.55\figurewidth}
        % \pgfplotsset{axis on top,ymajorgrids,axis line style={draw=none},legend style={at={(-.1,-.1)},anchor=south east}}
        % \pgfplotsset{grid style={line width=.1pt, draw=gray!10,densely dotted}}
        % \hspace*{-1.7cm}%
        % \def\our{{\sc sfr} (ours)}
        % \input{./fig/rl.tex}
        % \end{subfigure}\\[-1.5em]
        % \caption{\textbf{Cartpole swingup with sparse reward.} Training curves showing that \our's uncertainty estimates improve sample efficiency in RL.
        % Our method (\lab{color-our}) converges in fewer environment steps than the baseline model-based RL method and DDPG, the model-free baseline. The dashed lines mark the asymptotic return for the methods not coverged in the plot.}
        % \label{fig:rl}
        % % \vspace*{-1em}
        % \end{figure}

    \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Outlook}

   \begin{itemize}
     \item How good are the dual updates in practice?
    \begin{itemize}
        \item They depend on linearisation around MAP weights,
        \item And performing updates pushes us away from the MAP weights...
        % \item Can they update a dynamics model during episode in model-based RL?
    \end{itemize}
     \item Calculating dual parameters is costly (memory and computation),
    \begin{itemize}
        \item Speed up using KFAC?
    \end{itemize}
     \item Model-based RL
    \begin{itemize}
        \item Consistent function samples along trajectories, i.e. pathwise conditioning.
        \item Update dynamics model during episode using dual parameters?
    \end{itemize}
     % \item No straightforward method for specifying or tuning the prior covariance function.
     % \item It is important to note that the Laplace-GGN
% proach linearizes the network around the MAP solution wâˆ—, resulting in the function-space prior (and340
% consequently the posterior) being only a locally linear approximation of the neural network model
   \end{itemize}

  \end{block}

  \vspace*{1em}

  \nocite{*} % <-- This lists all references that are in the bib file

  \begin{block}{References}
    \vspace*{-.25em}
    \footnotesize{\bibliographystyle{ieeetr}\bibliography{bibliography}}
  \end{block}

\end{column}

\end{columns}

\end{column}

\separatorcolumn

\end{columns}

\end{frame}

\end{document}
