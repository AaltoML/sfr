_target_: src.rl.agents.mppi.init
_convert_: all
transition_model: ${transition_model}
reward_model: ${reward_model}
state_dim: ${state_dim}
action_dim: ${action_dim}
# DDPG config
mlp_dims: [512, 512]
learning_rate: 3e-4
max_ddpg_iterations: 500
# std_schedule: "linear(1.0, 0.1, 50)"
std: 0.1
std_clip: 0.3
# nstep: 1 # TODO always 1 to make it work with SVGP training?
gamma: 0.99
tau: 0.005
# MPPI config
# horizon: 5
horizon: 2
# horizon: 1
num_mppi_iterations: 5
# num_samples: 512
num_samples: 256
# num_samples: 16
mixture_coef: 0.05
num_topk: 32
# num_topk: 64
# num_topk: 4
temperature: 0.5
momentum: 0.1
unc_prop_strategy: "mean"
# unc_prop_strategy: "sample"
# sample_actor: true
sample_actor: true
bootstrap: true
# objective:
#   _target_: src.agents.objectives.greedy
#     actor: ${agent.actor}
#     critic: ${agent.critic}
#     transition_model: ${agent.transition_model}
#     reward_model: {agent.reward_model}
#     horizon: {agent.horizon}
#     # std: ${agent.std}
#     std_clip: ${agent.gamma}
#     gamma: ${agent.gamma}
# General config
device: ${device}
