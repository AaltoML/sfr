{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Environment Setup](#setup)\n",
    "* [Results](#results)\n",
    "    * [S-FSVI](#res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as Colab notebook\n",
    "\n",
    "**Important: Before connecting to a kernel, select a GPU runtime. To do so, open the `Runtime` tab above, click `Change runtime type`, and select `GPU`. Run the setup cell below only after you've done this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pull S-FSVI repository\n",
    "!git clone https://github.com/timrudner/S-FSVI.git\n",
    "# patch required packages\n",
    "!pip install -r ./S-FSVI/colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After successfully running the cell above, you need to restart the runtime. To do so, open the “Runtime” tab above and and click “Restart runtime”. Once the runtime was restarted, run the cell below. There is no need to re-run the installation in the cell above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add the repo to path\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"S-FSVI\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as Jupyter notebook (-->skip ahead to “Results” if you are running this as a Colab notebook<--)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install conda environment `fsvi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!conda env update -f ../environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting:\n",
    "\n",
    " - In case there is an error when installing sklearn: run `pip install Cython==0.29.23` manually and then run the above command again.\n",
    " - In case you have access to a GPU, see instructions [here](https://github.com/google/jax#pip-installation-gpu-cuda) for installing the GPU version of `jaxlib`. This will make the experiment run significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below to install the conda environment as a kernel of the jupyter notebook. Then switch to this kernel using the Jupyter Notebook menu bar by selecting `Kernel`, `Change kernel`, and then selecting `fsvi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python -m ipykernel install --user --name=fsvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting: For further details, see [here](https://medium.com/@nrk25693/how-to-add-your-conda-environment-to-your-jupyter-notebook-in-just-4-steps-abeab8b8d084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# assuming os.getcwd() returns the directory containing this jupyter notebook\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results <a name=\"results\"></a>\n",
    "\n",
    "To read a model checkpoint instead of training the model from scratch, pass load_chkpt=True to the function read_config_and_run .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/sklearn/utils/multiclass.py:13: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: TensorFlow is set to only use CPU.\n",
      "WARNING: TensorFlow is set to only use CPU.\n",
      "Jax is running on gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scratch/timner/applications/anaconda3/envs/fsvi/lib/python3.8/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import sfsvi.exps.utils.load_utils as lutils\n",
    "from notebooks.nb_utils.common import read_config_and_run, show_final_average_accuracy\n",
    "\n",
    "task_sequence = \"cifar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-FSVI <a name=\"res1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading experiments: 100%|███████████████████████████████████████████| 20/20 [00:00<00:00, 1040.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache:\n",
      "Running on clpc156.cs.ox.ac.uk\n",
      "Jax is running on gpu\n",
      "\n",
      "\n",
      "Input arguments:\n",
      " {\n",
      "    \"command\":\"cl\",\n",
      "    \"logroot\":\"ablation\",\n",
      "    \"subdir\":\"reproduce_main_results_2\",\n",
      "    \"save_alt\":true,\n",
      "    \"data_training\":\"continual_learning_cifar\",\n",
      "    \"not_use_val_split\":true,\n",
      "    \"n_permuted_tasks\":10,\n",
      "    \"n_omniglot_tasks\":20,\n",
      "    \"n_valid\":\"same\",\n",
      "    \"fix_shuffle\":false,\n",
      "    \"n_omniglot_coreset_chars\":2,\n",
      "    \"omniglot_randomize_test_split\":false,\n",
      "    \"omniglot_randomize_task_sequence\":false,\n",
      "    \"seed\":0,\n",
      "    \"n_coreset_inputs_per_task\":\"200\",\n",
      "    \"batch_size\":512,\n",
      "    \"no_artifact\":false,\n",
      "    \"data_ood\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"use_val_split\":true,\n",
      "    \"architecture\":\"six_layers\",\n",
      "    \"activation\":\"relu\",\n",
      "    \"prior_mean\":\"0.0\",\n",
      "    \"prior_cov\":\"0.03\",\n",
      "    \"prior_covs\":[\n",
      "        0.0\n",
      "    ],\n",
      "    \"prior_type\":\"bnn_induced\",\n",
      "    \"start_var_opt\":0,\n",
      "    \"learning_rate_var\":0.001,\n",
      "    \"dropout_rate\":0.0,\n",
      "    \"regularization\":0.0,\n",
      "    \"context_points_add_mode\":0,\n",
      "    \"context_point_adjustment\":false,\n",
      "    \"not_use_coreset\":false,\n",
      "    \"context_point_augmentation\":false,\n",
      "    \"plotting\":false,\n",
      "    \"logging\":1,\n",
      "    \"coreset\":\"random\",\n",
      "    \"coreset_entropy_mode\":\"soft_lowest\",\n",
      "    \"coreset_entropy_offset\":\"0.0\",\n",
      "    \"coreset_kl_heuristic\":\"lowest\",\n",
      "    \"coreset_kl_offset\":\"0.0\",\n",
      "    \"coreset_elbo_heuristic\":\"lowest\",\n",
      "    \"coreset_elbo_offset\":\"0.0\",\n",
      "    \"coreset_elbo_n_samples\":5,\n",
      "    \"coreset_n_tasks\":\"not_specified\",\n",
      "    \"coreset_entropy_n_mixed\":1,\n",
      "    \"full_ntk\":false,\n",
      "    \"constant_context_points\":false,\n",
      "    \"epochs_first_task\":\"200\",\n",
      "    \"identity_cov\":false,\n",
      "    \"n_epochs_save_params\":\"not_specified\",\n",
      "    \"n_augment\":\"not_specified\",\n",
      "    \"augment_mode\":\"constant\",\n",
      "    \"learning_rate_first_task\":\"5e-4\",\n",
      "    \"save_first_task\":false,\n",
      "    \"first_task_load_exp_path\":\"not_specified\",\n",
      "    \"only_task_id\":\"not_specified\",\n",
      "    \"loss_type\":1,\n",
      "    \"only_trainable_head\":false,\n",
      "    \"n_context_point_adjust_amount\":\"not_specified\",\n",
      "    \"save_all_params\":false,\n",
      "    \"n_marginals\":1,\n",
      "    \"n_condition\":512,\n",
      "    \"context_point_type\":\"train_pixel_rand_0.5\",\n",
      "    \"context_point_ood_data\":[\n",
      "        \"not_specified\"\n",
      "    ],\n",
      "    \"context_point_ood_data_size\":50000,\n",
      "    \"model_type\":\"fsvi_cnn\",\n",
      "    \"kl_scale\":\"normalized\",\n",
      "    \"feature_map_jacobian\":false,\n",
      "    \"feature_map_jacobian_train_only\":false,\n",
      "    \"feature_map_type\":\"not_specified\",\n",
      "    \"td_prior_scale\":0.0,\n",
      "    \"feature_update\":1,\n",
      "    \"full_cov\":false,\n",
      "    \"n_samples\":5,\n",
      "    \"n_samples_eval\":5,\n",
      "    \"tau\":1.0,\n",
      "    \"noise_std\":1.0,\n",
      "    \"ind_lim\":\"ind_-1_1\",\n",
      "    \"logging_frequency\":10,\n",
      "    \"figsize\":[\n",
      "        \"10\",\n",
      "        \"4\"\n",
      "    ],\n",
      "    \"save\":false,\n",
      "    \"name\":\"\",\n",
      "    \"evaluate\":false,\n",
      "    \"resume_training\":false,\n",
      "    \"no_final_layer_bias\":false,\n",
      "    \"extra_linear_layer\":false,\n",
      "    \"map_initialization\":false,\n",
      "    \"stochastic_linearization\":false,\n",
      "    \"grad_flow_jacobian\":false,\n",
      "    \"stochastic_prior_mean\":\"not_specified\",\n",
      "    \"batch_normalization\":false,\n",
      "    \"batch_normalization_mod\":\"not_specified\",\n",
      "    \"final_layer_variational\":true,\n",
      "    \"kl_sup\":\"not_specified\",\n",
      "    \"kl_sampled\":false,\n",
      "    \"fixed_inner_layers_variational_var\":false,\n",
      "    \"init_logvar\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_lin\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"init_logvar_conv\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"perturbation_param\":0.01,\n",
      "    \"wandb_project\":\"not_specified\",\n",
      "    \"n_context_points\":50,\n",
      "    \"n_context_points_first_task\":\"10\",\n",
      "    \"n_context_points_second_task\":\"200\",\n",
      "    \"context_points_bound\":[\n",
      "        0.0,\n",
      "        0.0\n",
      "    ],\n",
      "    \"use_generative_model\":false,\n",
      "    \"optimizer\":\"adam\",\n",
      "    \"optimizer_var\":\"not_specified\",\n",
      "    \"momentum\":0.0,\n",
      "    \"momentum_var\":0.0,\n",
      "    \"schedule\":\"not_specified\",\n",
      "    \"epochs\":50,\n",
      "    \"learning_rate\":0.0003,\n",
      "    \"context_points\":0,\n",
      "    \"init_logvar_minval\":0.0,\n",
      "    \"init_logvar_maxval\":0.0,\n",
      "    \"init_logvar_conv_minval\":0.0,\n",
      "    \"init_logvar_conv_maxval\":0.0\n",
      "} \n",
      "\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (posterior): False\n",
      "Full NTK computation: False\n",
      "Stochastic linearization (prior): False\n",
      "\n",
      "\n",
      "Learning task 1\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------\n",
      "  epoch    mean acc    t1 acc\n",
      "-------  ----------  --------\n",
      "      0       39.97     39.97\n",
      "      1       47.28     47.28\n",
      "      2       50.59     50.59\n",
      "      3       54.36     54.36\n",
      "      4       54.74     54.74\n",
      "      5       56.92     56.92\n",
      "      6       59.27     59.27\n",
      "      7       59.25     59.25\n",
      "      8       61.54     61.54\n",
      "      9       61.79     61.79\n",
      "     10       63.27     63.27\n",
      "     11       63.83     63.83\n",
      "     12       63.13     63.13\n",
      "     13       64.00     64.00\n",
      "     14       66.68     66.68\n",
      "     15       65.46     65.46\n",
      "     16       66.92     66.92\n",
      "     17       67.51     67.51\n",
      "     18       68.54     68.54\n",
      "     19       68.96     68.96\n",
      "     20       68.86     68.86\n",
      "     21       70.45     70.45\n",
      "     22       69.73     69.73\n",
      "     23       71.11     71.11\n",
      "     24       70.66     70.66\n",
      "     25       70.80     70.80\n",
      "     26       71.57     71.57\n",
      "     27       71.68     71.68\n",
      "     28       72.25     72.25\n",
      "     29       73.04     73.04\n",
      "     30       73.15     73.15\n",
      "     31       73.81     73.81\n",
      "     32       73.24     73.24\n",
      "     33       73.74     73.74\n",
      "     34       74.65     74.65\n",
      "     35       74.74     74.74\n",
      "     36       74.21     74.21\n",
      "     37       74.72     74.72\n",
      "     38       74.34     74.34\n",
      "     39       74.72     74.72\n",
      "     40       73.18     73.18\n",
      "     41       74.66     74.66\n",
      "     42       75.10     75.10\n",
      "     43       75.40     75.40\n",
      "     44       75.42     75.42\n",
      "     45       75.27     75.27\n",
      "     46       76.18     76.18\n",
      "     47       76.28     76.28\n",
      "     48       75.81     75.81\n",
      "     49       76.34     76.34\n",
      "     50       75.82     75.82\n",
      "     51       75.39     75.39\n",
      "     52       76.58     76.58\n",
      "     53       76.40     76.40\n",
      "     54       76.64     76.64\n",
      "     55       77.15     77.15\n",
      "     56       77.52     77.52\n",
      "     57       77.75     77.75\n",
      "     58       75.89     75.89\n",
      "     59       77.55     77.55\n",
      "     60       77.55     77.55\n",
      "     61       77.88     77.88\n",
      "     62       76.06     76.06\n",
      "     63       77.85     77.85\n",
      "     64       77.16     77.16\n",
      "     65       78.07     78.07\n",
      "     66       78.00     78.00\n",
      "     67       78.56     78.56\n",
      "     68       77.93     77.93\n",
      "     69       78.09     78.09\n",
      "     70       78.42     78.42\n",
      "     71       78.25     78.25\n",
      "     72       78.80     78.80\n",
      "     73       77.88     77.88\n",
      "     74       78.43     78.43\n",
      "     75       78.17     78.17\n",
      "     76       77.77     77.77\n",
      "     77       78.53     78.53\n",
      "     78       77.35     77.35\n",
      "     79       78.42     78.42\n",
      "     80       77.53     77.53\n",
      "     81       78.36     78.36\n",
      "     82       78.50     78.50\n",
      "     83       78.43     78.43\n",
      "     84       78.88     78.88\n",
      "     85       78.89     78.89\n",
      "     86       78.43     78.43\n",
      "     87       79.19     79.19\n",
      "     88       79.13     79.13\n",
      "     89       78.85     78.85\n",
      "     90       78.42     78.42\n",
      "     91       79.29     79.29\n",
      "     92       78.61     78.61\n",
      "     93       78.61     78.61\n",
      "     94       78.67     78.67\n",
      "     95       78.76     78.76\n",
      "     96       79.30     79.30\n",
      "     97       78.95     78.95\n",
      "     98       79.53     79.53\n",
      "     99       79.59     79.59\n",
      "    100       78.78     78.78\n",
      "    101       79.70     79.70\n",
      "    102       79.32     79.32\n",
      "    103       79.29     79.29\n",
      "    104       79.04     79.04\n",
      "    105       79.12     79.12\n",
      "    106       78.65     78.65\n",
      "    107       79.85     79.85\n",
      "    108       79.44     79.44\n",
      "    109       79.51     79.51\n",
      "    110       79.51     79.51\n",
      "    111       79.33     79.33\n",
      "    112       79.33     79.33\n",
      "    113       79.84     79.84\n",
      "    114       79.39     79.39\n",
      "    115       80.14     80.14\n",
      "    116       79.87     79.87\n",
      "    117       78.73     78.73\n",
      "    118       79.79     79.79\n",
      "    119       80.15     80.15\n",
      "    120       79.64     79.64\n",
      "    121       79.45     79.45\n",
      "    122       79.59     79.59\n",
      "    123       79.54     79.54\n",
      "    124       80.23     80.23\n",
      "    125       79.90     79.90\n",
      "    126       79.75     79.75\n",
      "    127       79.97     79.97\n",
      "    128       79.89     79.89\n",
      "    129       80.10     80.10\n",
      "    130       79.99     79.99\n",
      "    131       80.42     80.42\n",
      "    132       79.79     79.79\n",
      "    133       79.76     79.76\n",
      "    134       79.71     79.71\n",
      "    135       79.71     79.71\n",
      "    136       80.35     80.35\n",
      "    137       79.67     79.67\n",
      "    138       80.18     80.18\n",
      "    139       79.91     79.91\n",
      "    140       80.28     80.28\n",
      "    141       80.01     80.01\n",
      "    142       80.38     80.38\n",
      "    143       79.88     79.88\n",
      "    144       79.90     79.90\n",
      "    145       80.11     80.11\n",
      "    146       79.79     79.79\n",
      "    147       80.55     80.55\n",
      "    148       80.46     80.46\n",
      "    149       79.45     79.45\n",
      "    150       79.86     79.86\n",
      "    151       80.28     80.28\n",
      "    152       79.89     79.89\n",
      "    153       79.99     79.99\n",
      "    154       80.46     80.46\n",
      "    155       80.72     80.72\n",
      "    156       80.58     80.58\n",
      "    157       80.38     80.38\n",
      "    158       80.50     80.50\n",
      "    159       79.60     79.60\n",
      "    160       80.57     80.57\n",
      "    161       80.49     80.49\n",
      "    162       80.64     80.64\n",
      "    163       80.36     80.36\n",
      "    164       80.52     80.52\n",
      "    165       80.33     80.33\n",
      "    166       80.46     80.46\n",
      "    167       80.75     80.75\n",
      "    168       80.64     80.64\n",
      "    169       80.80     80.80\n",
      "    170       80.52     80.52\n",
      "    171       80.21     80.21\n",
      "    172       80.84     80.84\n",
      "    173       80.24     80.24\n",
      "    174       80.70     80.70\n",
      "    175       80.47     80.47\n",
      "    176       80.56     80.56\n",
      "    177       80.79     80.79\n",
      "    178       80.49     80.49\n",
      "    179       80.14     80.14\n",
      "    180       80.52     80.52\n",
      "    181       80.45     80.45\n",
      "    182       80.60     80.60\n",
      "    183       80.17     80.17\n",
      "    184       80.62     80.62\n",
      "    185       80.79     80.79\n",
      "    186       80.47     80.47\n",
      "    187       79.90     79.90\n",
      "    188       79.87     79.87\n",
      "    189       80.55     80.55\n",
      "    190       80.13     80.13\n",
      "    191       80.78     80.78\n",
      "    192       80.20     80.20\n",
      "    193       80.48     80.48\n",
      "    194       80.68     80.68\n",
      "    195       80.22     80.22\n",
      "    196       80.66     80.66\n",
      "    197       80.51     80.51\n",
      "    198       80.35     80.35\n",
      "    199       80.10     80.10\n",
      "Adding context points to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.8010 \n",
      "Accuracies (test): [0.801]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 2\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc\n",
      "-------  ----------  --------  --------\n",
      "      0       61.09     80.29     41.90\n",
      "      1       64.88     80.45     49.30\n",
      "      2       67.06     80.23     53.90\n",
      "      3       68.78     80.27     57.30\n",
      "      4       70.29     80.28     60.30\n",
      "      5       71.45     80.19     62.70\n",
      "      6       72.65     79.99     65.30\n",
      "      7       73.17     79.93     66.40\n",
      "      8       73.87     79.93     67.80\n",
      "      9       74.56     79.93     69.20\n",
      "     10       75.18     79.96     70.40\n",
      "     11       75.66     79.91     71.40\n",
      "     12       75.61     79.71     71.50\n",
      "     13       76.32     79.74     72.90\n",
      "     14       76.48     79.97     73.00\n",
      "     15       77.00     79.70     74.30\n",
      "     16       77.23     79.77     74.70\n",
      "     17       77.45     79.89     75.00\n",
      "     18       77.75     79.71     75.80\n",
      "     19       77.70     79.91     75.50\n",
      "     20       78.30     80.10     76.50\n",
      "     21       78.00     79.79     76.20\n",
      "     22       78.09     79.68     76.50\n",
      "     23       78.22     79.75     76.70\n",
      "     24       78.43     79.56     77.30\n",
      "     25       78.15     79.60     76.70\n",
      "     26       78.81     79.52     78.10\n",
      "     27       78.89     79.87     77.90\n",
      "     28       78.46     79.71     77.20\n",
      "     29       79.06     79.81     78.30\n",
      "     30       78.64     79.78     77.50\n",
      "     31       78.50     79.71     77.30\n",
      "     32       78.66     79.52     77.80\n",
      "     33       78.81     79.73     77.90\n",
      "     34       78.62     79.64     77.60\n",
      "     35       79.04     79.68     78.40\n",
      "     36       79.18     79.77     78.60\n",
      "     37       79.11     79.71     78.50\n",
      "     38       79.06     79.61     78.50\n",
      "     39       78.93     79.46     78.40\n",
      "     40       79.04     79.48     78.60\n",
      "     41       79.22     79.75     78.70\n",
      "     42       79.50     79.70     79.30\n",
      "     43       79.26     79.62     78.90\n",
      "     44       79.55     79.71     79.40\n",
      "     45       79.29     79.77     78.80\n",
      "     46       79.42     79.53     79.30\n",
      "     47       79.44     79.48     79.40\n",
      "     48       79.69     79.57     79.80\n",
      "     49       79.07     79.44     78.70\n",
      "Adding context points to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7907 \n",
      "Accuracies (test): [0.7944, 0.787]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 3\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc\n",
      "-------  ----------  --------  --------  --------\n",
      "      0       67.48     79.25     78.40     44.80\n",
      "      1       70.48     79.15     78.50     53.80\n",
      "      2       72.23     79.28     78.80     58.60\n",
      "      3       72.15     79.05     77.70     59.70\n",
      "      4       73.01     79.42     78.60     61.00\n",
      "      5       73.69     79.48     79.00     62.60\n",
      "      6       74.25     79.26     79.10     64.40\n",
      "      7       74.32     79.15     78.10     65.70\n",
      "      8       74.64     79.01     78.20     66.70\n",
      "      9       74.72     78.76     77.90     67.50\n",
      "     10       75.18     78.95     77.80     68.80\n",
      "     11       75.98     78.73     78.80     70.40\n",
      "     12       75.68     79.14     77.40     70.50\n",
      "     13       75.59     79.06     77.30     70.40\n",
      "     14       75.91     79.13     78.20     70.40\n",
      "     15       76.39     78.98     77.90     72.30\n",
      "     16       76.54     79.13     77.70     72.80\n",
      "     17       76.66     79.07     77.40     73.50\n",
      "     18       76.74     79.01     78.30     72.90\n",
      "     19       76.39     78.67     77.80     72.70\n",
      "     20       76.31     78.83     76.80     73.30\n",
      "     21       76.65     78.94     77.50     73.50\n",
      "     22       76.47     79.01     76.90     73.50\n",
      "     23       76.72     79.37     77.20     73.60\n",
      "     24       76.86     78.97     77.60     74.00\n",
      "     25       76.58     78.94     77.20     73.60\n",
      "     26       77.16     79.27     78.10     74.10\n",
      "     27       76.82     78.65     77.40     74.40\n",
      "     28       77.11     79.04     77.70     74.60\n",
      "     29       76.63     78.59     77.00     74.30\n",
      "     30       76.88     78.84     77.50     74.30\n",
      "     31       77.23     79.20     78.20     74.30\n",
      "     32       77.11     78.94     77.40     75.00\n",
      "     33       77.53     78.79     78.10     75.70\n",
      "     34       76.97     78.71     77.00     75.20\n",
      "     35       76.51     78.54     77.30     73.70\n",
      "     36       77.57     79.10     78.20     75.40\n",
      "     37       77.22     78.97     78.30     74.40\n",
      "     38       77.67     79.01     78.30     75.70\n",
      "     39       77.57     79.12     77.90     75.70\n",
      "     40       77.27     78.90     77.30     75.60\n",
      "     41       77.01     78.62     76.70     75.70\n",
      "     42       76.98     79.14     77.10     74.70\n",
      "     43       77.44     78.73     78.00     75.60\n",
      "     44       77.13     79.00     77.10     75.30\n",
      "     45       77.25     78.85     77.20     75.70\n",
      "     46       77.39     79.06     77.10     76.00\n",
      "     47       77.43     79.10     77.40     75.80\n",
      "     48       77.28     78.74     76.80     76.30\n",
      "     49       77.66     78.69     77.50     76.80\n",
      "Adding context points to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7766 \n",
      "Accuracies (test): [0.7869, 0.775, 0.768]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 4\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc\n",
      "-------  ----------  --------  --------  --------  --------\n",
      "      0       69.31     78.66     76.90     75.10     46.60\n",
      "      1       72.54     78.34     77.60     75.70     58.50\n",
      "      2       73.96     77.74     78.10     75.80     64.20\n",
      "      3       73.87     77.67     77.50     75.40     64.90\n",
      "      4       74.68     78.02     77.50     75.80     67.40\n",
      "      5       75.24     77.96     77.70     75.80     69.50\n",
      "      6       75.56     78.14     77.80     75.50     70.80\n",
      "      7       75.74     77.96     78.20     75.50     71.30\n",
      "      8       75.89     77.44     77.70     76.30     72.10\n",
      "      9       76.06     77.73     77.90     75.60     73.00\n",
      "     10       75.61     78.04     76.80     75.00     72.60\n",
      "     11       76.14     77.57     77.60     75.20     74.20\n",
      "     12       76.61     77.94     77.80     76.10     74.60\n",
      "     13       76.66     77.44     77.90     75.70     75.60\n",
      "     14       76.45     77.80     77.50     75.30     75.20\n",
      "     15       76.66     77.86     77.50     75.50     75.80\n",
      "     16       76.93     77.83     78.00     75.50     76.40\n",
      "     17       76.89     77.95     77.70     75.40     76.50\n",
      "     18       77.00     77.70     77.30     75.80     77.20\n",
      "     19       77.14     77.75     76.90     76.10     77.80\n",
      "     20       77.23     78.20     77.60     75.40     77.70\n",
      "     21       77.45     77.60     77.40     76.30     78.50\n",
      "     22       77.26     77.75     77.30     75.30     78.70\n",
      "     23       76.88     77.31     76.70     75.60     77.90\n",
      "     24       77.12     77.47     76.70     76.00     78.30\n",
      "     25       77.44     77.86     77.20     75.90     78.80\n",
      "     26       77.49     77.77     76.70     76.10     79.40\n",
      "     27       77.37     77.77     77.00     75.30     79.40\n",
      "     28       77.58     77.40     76.90     76.20     79.80\n",
      "     29       77.40     77.61     76.70     76.20     79.10\n",
      "     30       77.19     77.26     76.60     75.80     79.10\n",
      "     31       77.53     77.41     77.00     76.20     79.50\n",
      "     32       77.64     77.57     77.60     75.60     79.80\n",
      "     33       77.43     77.53     76.70     75.80     79.70\n",
      "     34       77.81     77.73     77.00     76.30     80.20\n",
      "     35       77.41     77.45     76.30     76.00     79.90\n",
      "     36       77.74     77.27     77.20     76.10     80.40\n",
      "     37       77.86     77.64     77.50     76.10     80.20\n",
      "     38       77.50     77.49     76.40     75.60     80.50\n",
      "     39       77.85     77.29     77.20     76.10     80.80\n",
      "     40       77.53     77.12     76.60     76.00     80.40\n",
      "     41       77.74     77.46     76.40     76.20     80.90\n",
      "     42       77.74     77.25     76.20     76.70     80.80\n",
      "     43       77.78     77.62     76.40     76.30     80.80\n",
      "     44       78.17     77.47     77.10     76.90     81.20\n",
      "     45       77.72     77.56     76.00     76.50     80.80\n",
      "     46       77.70     77.30     76.80     75.70     81.00\n",
      "     47       77.76     77.54     76.60     75.90     81.00\n",
      "     48       78.03     77.72     76.90     76.10     81.40\n",
      "     49       77.57     77.50     75.80     75.50     81.50\n",
      "Adding context points to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7757 \n",
      "Accuracies (test): [0.775, 0.758, 0.755, 0.815]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 5\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------\n",
      "      0       70.95     77.43     76.50     76.00     79.90     44.90\n",
      "      1       73.96     77.50     77.70     75.10     80.70     58.80\n",
      "      2       73.73     77.26     76.00     75.30     80.20     59.90\n",
      "      3       74.09     77.27     76.10     74.50     80.20     62.40\n",
      "      4       74.63     77.56     76.20     75.70     80.70     63.00\n",
      "      5       74.88     77.49     75.90     74.80     80.70     65.50\n",
      "      6       74.85     77.37     75.50     75.30     80.30     65.80\n",
      "      7       75.32     77.18     75.60     75.60     80.40     67.80\n",
      "      8       75.32     76.98     75.60     75.70     80.20     68.10\n",
      "      9       76.00     77.30     76.30     75.90     81.00     69.50\n",
      "     10       75.65     76.85     76.40     74.90     80.10     70.00\n",
      "     11       76.23     77.24     76.50     75.90     80.30     71.20\n",
      "     12       75.88     76.90     77.00     74.80     80.10     70.60\n",
      "     13       75.88     76.72     76.20     75.20     79.80     71.50\n",
      "     14       76.36     76.71     76.60     75.70     80.60     72.20\n",
      "     15       76.58     77.22     77.10     75.30     80.20     73.10\n",
      "     16       76.36     76.92     76.70     75.40     80.40     72.40\n",
      "     17       76.67     76.96     76.70     75.40     80.50     73.80\n",
      "     18       76.98     76.92     77.10     76.30     80.40     74.20\n",
      "     19       76.95     77.24     76.20     76.60     80.20     74.50\n",
      "     20       76.52     76.51     76.40     75.20     80.30     74.20\n",
      "     21       77.05     76.76     76.20     76.60     80.60     75.10\n",
      "     22       76.65     76.73     76.20     75.50     80.70     74.10\n",
      "     23       76.76     76.91     75.70     75.60     80.60     75.00\n",
      "     24       76.80     76.40     76.50     74.90     80.40     75.80\n",
      "     25       77.07     76.93     76.30     75.80     80.80     75.50\n",
      "     26       77.27     76.93     77.00     76.00     80.60     75.80\n",
      "     27       77.35     76.74     76.60     76.20     80.50     76.70\n",
      "     28       77.15     76.66     76.20     75.90     80.70     76.30\n",
      "     29       77.45     77.15     76.90     75.90     80.30     77.00\n",
      "     30       77.51     76.85     76.60     76.20     80.70     77.20\n",
      "     31       77.59     76.83     76.80     76.20     80.40     77.70\n",
      "     32       77.31     76.97     76.40     75.90     80.40     76.90\n",
      "     33       77.37     76.63     76.60     75.80     80.50     77.30\n",
      "     34       77.60     77.00     76.50     76.50     80.30     77.70\n",
      "     35       77.50     76.69     76.90     75.70     80.70     77.50\n",
      "     36       77.73     76.64     77.40     76.20     80.10     78.30\n",
      "     37       77.48     76.61     76.40     75.80     80.40     78.20\n",
      "     38       77.49     76.84     77.00     75.40     80.40     77.80\n",
      "     39       77.55     76.94     76.50     75.60     80.90     77.80\n",
      "     40       77.73     76.67     76.70     76.00     80.10     79.20\n",
      "     41       77.81     76.74     77.20     76.00     79.90     79.20\n",
      "     42       77.37     76.23     75.90     75.60     80.40     78.70\n",
      "     43       77.69     76.65     76.80     75.80     80.60     78.60\n",
      "     44       77.60     76.78     75.80     76.50     80.30     78.60\n",
      "     45       77.75     76.67     76.60     76.10     80.10     79.30\n",
      "     46       77.97     76.67     77.50     75.60     80.10     80.00\n",
      "     47       77.53     76.43     75.80     75.70     80.20     79.50\n",
      "     48       77.83     76.54     77.00     76.00     79.90     79.70\n",
      "     49       77.56     76.52     77.10     75.30     80.10     78.80\n",
      "Adding context points to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7756 \n",
      "Accuracies (test): [0.7652, 0.771, 0.753, 0.801, 0.788]\n",
      "---\n",
      "\n",
      "\n",
      "Learning task 6\n",
      "Nomenclature:\n",
      "\tacc: accuracy in %\n",
      "\tt1: the first task\n",
      "\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------\n",
      "  epoch    mean acc    t1 acc    t2 acc    t3 acc    t4 acc    t5 acc    t6 acc\n",
      "-------  ----------  --------  --------  --------  --------  --------  --------\n",
      "      0       74.28     76.69     75.70     74.60     80.30     79.30     59.10\n",
      "      1       75.78     76.21     76.70     74.90     80.40     79.20     67.30\n",
      "      2       75.64     76.66     75.60     74.40     79.90     78.90     68.40\n",
      "      3       76.03     76.30     76.40     75.20     79.80     78.90     69.60\n",
      "      4       76.16     76.48     76.00     75.10     80.10     78.60     70.70\n",
      "      5       76.28     76.29     75.40     74.10     80.40     79.20     72.30\n",
      "      6       76.29     76.45     75.90     74.90     80.20     77.70     72.60\n",
      "      7       76.39     76.25     76.10     74.80     80.10     77.90     73.20\n",
      "      8       76.88     76.67     76.30     75.40     80.30     78.10     74.50\n",
      "      9       76.62     76.64     76.60     74.00     80.20     77.60     74.70\n",
      "     10       76.63     76.16     75.50     74.00     80.60     78.10     75.40\n",
      "     11       76.77     76.41     76.30     73.80     80.40     77.90     75.80\n",
      "     12       77.11     76.55     76.50     74.90     80.40     78.10     76.20\n",
      "     13       77.03     76.46     75.90     75.80     80.20     77.20     76.60\n",
      "     14       77.35     77.00     76.40     74.80     80.60     78.10     77.20\n",
      "  from scipy.sparse.base import spmatrix\n",
      "     15       77.31     76.46     75.90     75.50     80.70     77.40     77.90\n",
      "     16       77.48     76.48     76.90     75.30     80.60     77.70     77.90\n",
      "     17       77.50     76.27     77.50     75.10     80.30     77.80     78.00\n",
      "     18       77.48     76.57     76.60     75.60     80.70     77.00     78.40\n",
      "     19       77.52     76.52     77.00     75.20     80.50     77.50     78.40\n",
      "     20       77.43     76.50     76.40     75.60     80.50     76.90     78.70\n",
      "     21       77.49     76.33     76.70     75.10     80.90     77.20     78.70\n",
      "     22       77.59     76.34     77.30     74.90     80.50     78.20     78.30\n",
      "     23       77.58     76.79     77.40     74.10     80.80     77.40     79.00\n",
      "     24       77.43     76.36     77.50     74.90     80.60     76.70     78.50\n",
      "     25       77.37     76.34     77.10     74.60     80.10     77.10     79.00\n",
      "     26       77.62     76.61     77.70     74.60     80.20     77.40     79.20\n",
      "     27       77.74     76.26     77.10     75.40     80.10     77.90     79.70\n",
      "     28       77.79     76.33     77.60     75.50     80.10     77.20     80.00\n",
      "     29       77.83     76.59     77.20     75.40     80.40     77.70     79.70\n",
      "     30       77.53     76.46     76.90     74.40     80.10     77.40     79.90\n",
      "     31       77.50     76.21     76.40     75.00     79.60     78.20     79.60\n",
      "     32       77.54     76.52     76.50     74.40     80.80     77.40     79.60\n",
      "     33       77.64     76.32     76.90     74.90     80.40     77.60     79.70\n",
      "     34       77.63     76.39     76.40     74.90     80.20     77.60     80.30\n",
      "     35       77.60     76.01     76.10     75.00     80.80     77.70     80.00\n",
      "     36       77.52     75.94     77.10     74.50     80.90     77.30     79.40\n",
      "     37       77.42     76.12     77.00     74.10     80.40     76.70     80.20\n",
      "     38       77.47     76.13     76.60     74.50     80.80     77.20     79.60\n",
      "     39       77.83     76.20     77.30     75.10     80.10     77.40     80.90\n",
      "     40       77.90     76.43     77.00     75.90     80.00     77.60     80.50\n",
      "     41       77.78     76.20     77.50     74.50     80.40     77.60     80.50\n",
      "     42       77.71     76.05     77.30     75.00     80.20     77.70     80.00\n",
      "     43       77.73     76.29     78.10     74.70     80.20     77.00     80.10\n",
      "     44       77.72     76.53     77.10     74.50     79.80     77.30     81.10\n",
      "     45       77.52     76.31     76.90     74.30     80.00     76.80     80.80\n",
      "     46       77.40     76.08     75.90     74.30     80.50     76.90     80.70\n",
      "     47       77.70     76.01     76.80     74.50     80.00     77.80     81.10\n",
      "     48       77.73     76.19     77.10     74.80     80.80     76.90     80.60\n",
      "     49       77.50     76.17     76.10     74.20     80.10     77.60     80.80\n",
      "Adding context points to the coreset randomly\n",
      "For tasks seen so far, \n",
      "---\n",
      "Mean accuracy (test): 0.7750 \n",
      "Accuracies (test): [0.7617, 0.761, 0.742, 0.801, 0.776, 0.808]\n",
      "---\n",
      "\n",
      "\n",
      "------------------- DONE -------------------\n",
      "\n",
      "\n",
      "0.77495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logdir = read_config_and_run(\"fsvi_cifar.pkl\", task_sequence)\n",
    "exp = lutils.read_exp(logdir)\n",
    "show_final_average_accuracy(exp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsvi",
   "language": "python",
   "name": "fsvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
